<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>基于k8s的家用大数据集群设计与实现</title>
      <link href="//household_k8s_bigdata/"/>
      <url>//household_k8s_bigdata/</url>
      
        <content type="html"><![CDATA[<p>使用3台废旧笔记本搭建k8s集群，部署大数据组件，利用路由器进行异地组网，配合wsl作为管理和客户端，<br>实现随时随地，在工作笔记本上以本地访问的体验使用自建家庭大数据平台进行学习、开发、测试。</p><span id="more"></span><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h3><p>最近换了电脑，之前的机器闲置着浪费，又刚好看到了<a href="https://github.com/geekyouth/SZT-bigdata">https://github.com/geekyouth/SZT-bigdata</a> 这个项目，不错不错，让我来可以做得更好。</p><p>索性自己在家里搭个大数据集群来玩。而且女朋友的笔记本也闲置着，再搜刮一台来组个集群，就可以为所欲为了。</p><h3 id="最初目标"><a href="#最初目标" class="headerlink" title="最初目标"></a>最初目标</h3><p>在主力机的wsl里可以提交flink任务，在idea的单元测试里可以配置wsl执行hadoop操作。<br>这样基本就可以满足日常的开发测试需求了。</p><h2 id="物理配置及组网"><a href="#物理配置及组网" class="headerlink" title="物理配置及组网"></a>物理配置及组网</h2><h3 id="机器"><a href="#机器" class="headerlink" title="机器"></a>机器</h3><p>服务器：lian1、lian2、lian3 </p><p>蒲公英家用路由器（带内网异地组网功能）</p><p>客户端：工作用笔记本（win11，带wsl）</p><h3 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h3><p>3台服务器利用网线连接路由器，组成一个局域网。</p><p>在家的时候笔记本直接通过wifi连接到局域网，在外也还可以通过蒲公英的异地组网功能连接。忽略网速的话体验一致。</p><p>注意：也可以通过一些内网穿透的软件达到相同效果，不推荐专门去买路由器。</p><h2 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h2><h3 id="大数据平台选型"><a href="#大数据平台选型" class="headerlink" title="大数据平台选型"></a>大数据平台选型</h3><p>本来只是出于实验目的，想直接用cdh，方便管理一些。</p><p>CDH6.3.3之后没有免费社区版了。我之前还部署了一套6.3.2的。<br>但是从2021年2月开始下载之前的版本也要订阅了，很麻烦。<br>算了，人家就是不想让你用免费的，而且免费的组件版本还老，不用也罢。</p><p>然后我就开始物色其他的，hdp也早被cdh收编了，mapr本来就是主打商用的。</p><p>国内的华为阿里腾讯都是云服务，不会提供这种社区版。</p><p>倒是有看到UCloud出了一个USDP的大数据平台免费版。有点意思。<br>然后看了一遍部署教程和操作文档。嗯。。。再看一眼组件版本，嗯。。。</p><p>算了，还是用我自己的基于k8s的方法部署吧</p><h3 id="k8s部署管理平台选型"><a href="#k8s部署管理平台选型" class="headerlink" title="k8s部署管理平台选型"></a>k8s部署管理平台选型</h3><p>kubernetes相关部署管理工具较多，我是不推荐自己手动一步步部署的</p><p>这里我用的是： kubeoperator</p><p>使用wsl-docker作为kubeoperator的管理端，将k8s部署到3台机器上<br>平时需要用到管理功能的时候再启动win11上的docker就行，日常运行不需要</p><p>按公网教程一步步操作即可，这里有个坑：<br>主机的ip只能填ip不能填hostname，虽然填的时候不会报错，但是在生成k8s证书的时候会失败！</p><h2 id="资源分配"><a href="#资源分配" class="headerlink" title="资源分配"></a>资源分配</h2><p>3台机器本来就是闲置机，唯一配置好一点的前主力机的32g内存也被我拆到现在的主力机上了。<br>机器配置都是4核，但内存加起来才32g，可以说是非常垃圾，但是受限于经济能力，没办法。<br>不过一想到云服务器1c2g也被我跑了那么多服务，12c32g好像也还不错了，至少比在自己笔记本上开3台虚拟机强。</p><p>经常看到群里出于学习要搭大数据的，怕自己配置不够。<br>够不够其实看需求，只是做功能测试、验证的，没多大数据量，耗不了多少资源。<br>再说，就算数据量大，只要控制好，反正最后都是在yarn上面跑，无非就是慢一点，总是能跑完，挂不了。<br>更何况我还是放在kubernetes上的，就算一个挂了也影响不了其他。这点比cdh脚本部署强。</p><p>下面来看各个组件的安排</p><p>物理机上直接启用的有： nfs<br>docker直接启用的有： docker镜像仓库（docker-registry、docker-registry-web-server）、mysql、 ldap（openldap、ldapadmin）<br>kubernetes管理的有：</p><h3 id="1-kerberos"><a href="#1-kerberos" class="headerlink" title="1. kerberos"></a>1. kerberos</h3><p>用于大数据组件的认证，支持1主多从，这里给1主1从，毕竟是最基础的组件，就象征性地高可用吧<br>配置的话1c1g就够了，这里的资源值是指limit而非request，即最高能申请到的内存，实际用到的会低很多</p><h3 id="2-zookeeper"><a href="#2-zookeeper" class="headerlink" title="2. zookeeper"></a>2. zookeeper</h3><p>分布式协调，一般是其他组件实现高可用的基础。至少3台，1c1g。</p><h3 id="3-elasticsearch"><a href="#3-elasticsearch" class="headerlink" title="3. elasticsearch"></a>3. elasticsearch</h3><p>搜索引擎，这个是给ranger存储审计日志用的。支持多个，但1个就够了，1c1g。</p><h3 id="4-ranger"><a href="#4-ranger" class="headerlink" title="4. ranger"></a>4. ranger</h3><p>用于大数据权限管理，主要跑两个程序：<br>ranger-admin提供ranger核心服务和web管理功能<br>ranger-usersync负责从ldap同步用户账户信息</p><h3 id="5-hdfs"><a href="#5-hdfs" class="headerlink" title="5. hdfs"></a>5. hdfs</h3><p>大数据最基本的存储组件，因为是高可用部署，得区分好几种角色（这里用的时hadoop最新版本3.3.1的架构配置）。</p><ul><li>zkfc-format:用于第一次运行时格式化hdfs在zookeeper上的存储，一次性运行，故配置不重要</li><li>jn:journalnode，日志节点，hdfs内部的高可用机制的实现，至少3台，1c1g</li><li>nn-active：active-namenode，默认状态为active的名称节点，即默认主节点，1个，在第一次启动时负责做初始化操作，1c2g</li><li>nn-standby:standby-namenode，默认状态为standby的名称节点，即默认从节点，支持多个，参与主节点选举以实现高可用。这里也给1个，1c2g</li><li>nn-observer:observer-namenode，状态为observer的名称节点，即观察者节点，负责读写分离的读的部分，不参与主节点选举。这里直接不给，省点资源。</li><li>dn:datanode，数据节点，至少3个，1c2g。</li></ul><p>虽然把nn-observer给省略了，但实际上hadoop2的时候namenode也只支持两个，所以其实现在这样从设计上来说也很不错了。<br>一般给1c1g是因为我知道它跑不到1g，1c2g是因为内部进程jvm参数容易配置些。</p><h3 id="6-yarn"><a href="#6-yarn" class="headerlink" title="6. yarn"></a>6. yarn</h3><p>大数据最基本的资源调度组件</p><ul><li>rm:resource-manager,资源管理器，支持任意多个实现高可用，这里给2个，1c2g</li><li>nm：node-manager,节点管理器，支持任意多个横向拓展，这里给1个，3c8g。</li></ul><p>注意，nm的3c8g里面2c6g是给yarn平台调度用的，1g是给nm进程本身的，其他的预留给容器本身的。<br>所以2c6g就是这个大数据平台的计算任务能调度的最多资源了…</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>到yarn就已经把大数据最最基础的部分搭建好了。<br>其他组件按需启动就行，像hive、hbase这些有主从多节点高可用结构的我暂时没用到，又耗资源，就先不管<br>像flink、spark这种支持把任务放到yarn上面跑的，本身留个一个历史日志服务器节点就够了，甚至不需要。<br>另外用得比较多就是kafka，一般也是至少3台，1台也可以，先不管</p><h3 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h3><table><thead><tr><th>hostname</th><th>cpu核数</th><th>内存（g）</th><th>组件</th><th>limit资源总和</th></tr></thead><tbody><tr><td>lian1</td><td>4</td><td>16</td><td>kerberos-master、zookeeper、es、ranger、hdfs-jn、hdfs-nn-active、hdfs-dn、yarn-rm、yarn-nm</td><td>12c19g</td></tr><tr><td>lian2</td><td>4</td><td>12</td><td>kerberos-slave、zookeeper、hdfs-jn、hdfs-nn-standby、hdfs-dn、yarn-rm</td><td>6c9g</td></tr><tr><td>lian3</td><td>4</td><td>4</td><td>zookeeper、hdfs-jn、hdfs-dn</td><td>3c3g</td></tr></tbody></table><p>可以看到limit的资源总和是超过实际资源的，但是只是我为了方便分配和设置而已<br>大部分进程cpu用得都是很低的，内存不跑任务的时候也不多<br>毕竟条件所限，这样子也不错了<br>接下来就是实际测试了</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产力 </tag>
            
            <tag> 大数据 </tag>
            
            <tag> WSL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>个人博客系统设计(支持hexo和halo同步)</title>
      <link href="//my_blog_system_design/"/>
      <url>//my_blog_system_design/</url>
      
        <content type="html"><![CDATA[<p>本文介绍我自己的博客系统是如何设计的，以及使用自己开发的软件达到hexo（git pages: <a href="https://linshenkx.github.io/">https://linshenkx.github.io</a> ）文章自动同步到halo( <a href="http://linshenkx.cn/">http://linshenkx.cn</a> )的效果。<br>实现一次编写、两套博客系统并存、多个网址访问的效果。</p><span id="more"></span><h2 id="一-总览"><a href="#一-总览" class="headerlink" title="一 总览"></a>一 总览</h2><h3 id="达到效果"><a href="#达到效果" class="headerlink" title="达到效果"></a>达到效果</h3><table><thead><tr><th>个人博客网址</th><th>介绍</th><th>对应git仓库/管理界面</th></tr></thead><tbody><tr><td><a href="https://linshenkx.gitee.io/">https://linshenkx.gitee.io</a></td><td>hexo next gitee pages</td><td><a href="https://gitee.com/linshenkx/linshenkx">https://gitee.com/linshenkx/linshenkx</a></td></tr><tr><td><a href="https://linshenkx.github.io/">https://linshenkx.github.io</a></td><td>hexo next github pages</td><td><a href="https://github.com/linshenkx/linshenkx.github.io">https://github.com/linshenkx/linshenkx.github.io</a></td></tr><tr><td><a href="https://linshen.netlify.app/">https://linshen.netlify.app</a></td><td>netlify加速，文章同步自blog源码仓库</td><td><a href="https://app.netlify.com/teams/linshenkx">https://app.netlify.com/teams/linshenkx</a></td></tr><tr><td><a href="http://linshenkx.cn/">http://linshenkx.cn</a></td><td>halo个人网站，文章同步自blog源码仓库</td><td><a href="http://linshenkx.cn/admin/index.html#/dashboard">http://linshenkx.cn/admin/index.html#/dashboard</a></td></tr></tbody></table><p>blog博客源码仓库（核心，私有）：<a href="https://github.com/linshenkx/blog">https://github.com/linshenkx/blog</a></p><h3 id="博客发布流程"><a href="#博客发布流程" class="headerlink" title="博客发布流程"></a>博客发布流程</h3><ol><li>编写博客<br>在blog工程下写博客，工程为标准hexo，博客为markdown文件放在source/_posts目录下，使用多层级分类存放</li><li>发布到git pages<br>完成博客的增删改后，在工程目录下执行<code>hexo clean &amp;&amp; hexo d -g</code>部署到git pages。<br>这里我配置了同时发布到github和gitee，需要注意的是，gitee的git pages需要手动去触发更新才能生效。</li><li>提交并推送工程<br>提交并推送blog工程的修改。<br>netlify将自动获取blog工程，并执行hexo部署脚本（效果和git pages一样，只是用netlify访问据说会快一点）<br>自开发的hexo_halo同步软件也会检测到blog工程更新，根据更新情况将变化同步到halo博客系统中。</li></ol><h2 id="二-设计思路"><a href="#二-设计思路" class="headerlink" title="二 设计思路"></a>二 设计思路</h2><h3 id="1-起因"><a href="#1-起因" class="headerlink" title="1 起因"></a>1 起因</h3><p>本来我一直是在使用csdn的，但是网页端写作确实不方便，而且还可能受网络情况限制。<br>所以我后面一般都是用印象笔记做记录，在印象笔记写好再看心情整理到csdn上去。<br>但是悄不注意的，在21年初csdn改版，同时也改变了排名和引流规则。<br>之前一个星期2500到3000的访问量现在只剩1500到2000了。</p><p>嗯，不可忍。换。</p><h3 id="2-调研"><a href="#2-调研" class="headerlink" title="2 调研"></a>2 调研</h3><p>市面上的博客系统可安装对Git Pages的支持（即是否支持生成静态网站）分为两大类：</p><p>一是以hexo为代表的静态网站生成器：如hexo、hugo、jekyll，较成熟，有较多第三方主题和插件，可与git pages搭配使用，也可自行部署。</p><p>二是以halo为代表的五花八门的个人博客系统，功能更加强大，自由度更高，通常带后台管理，但不支持git pages，需自行部署。</p><h3 id="3-分析"><a href="#3-分析" class="headerlink" title="3 分析"></a>3 分析</h3><p>个人博客的话使用git pages比较稳定，网址固定，可以永久使用，而且可以通过搭配不同的git服务商来保证访问速度。<br>但是git pages的缺点也很明显，是静态网站，虽然可以搭配第三方插件增强，但说到底还是个静态网站。</p><p>而如果自己买服务器，买域名，用第三方个人博客系统，就可以玩得比较花里胡哨了，但谁知道会用多久呢。<br>服务器、域名都要自己负责，三五年之后还能不能访问就比较难说了。<br>但是年轻人嘛，总还是花里胡哨点才香。</p><p>那我就全都要。</p><p>git pages作为专业性较强的个人网站可以永久访问，<br>然后再弄个服务器放个博客系统自己玩。</p><h3 id="4-选型"><a href="#4-选型" class="headerlink" title="4 选型"></a>4 选型</h3><p>静态网站生成器选的是hexo，传统一点，支持的插件和主题比较多。<br>hugo虽然也不错，但似乎国内用的不多，支持可能还不够完善。</p><p>然后hexo的主题用的最经典的next，比较成熟，功能也很完善<br>虽然整体比较严肃压抑，但可以自己加个live2d增添点活力，<br>作为一个展示专业性的博客网站这样也就够了</p><p>自定义博客系统的话我选的是halo，最主要原因是它是java写的，利于二次开发（事实上后面用着也确实有问题，还提交了一个issue）<br>而且功能比较强大，生态比较完善，虽然第三方主题少且基本都没更新，但是…实在是找不出其他一个能打的了<br>另外halo支持导入markdown，且功能基本都通过rest接口放开，适合开发者使用</p><h2 id="三-设计实现"><a href="#三-设计实现" class="headerlink" title="三 设计实现"></a>三 设计实现</h2><h3 id="1-hexo"><a href="#1-hexo" class="headerlink" title="1 hexo"></a>1 hexo</h3><p>hexo本身只是静态网站生成器，你可以把hexo项目本身发布成为git pages项目，<br>像github、gitee这些会识别出这是一个hexo项目，然后进行编译，得到静态资源供外部访问。<br>这也是最简单的用法。</p><p>但是不推荐。</p><p>因为git pages项目一般都要求是public的（且名称固定，一个git账号只有一个git pages仓库），<br>hexo项目包含你的博客markdown源文件和其他的个人信息。<br>我们只是想把必要的生成后的静态网页放出去而已，至于项目的配置信息和markdown源文件应该藏起来。</p><p>所以需要使用 hexo-deployer-git 插件进行git pages的部署。<br>即放到git公开的文件只有生成后的网页文件而已，git只是把你生成后的index.html进行直接展示，不会再去编译了<br>（需要在source目录下添加.nojekyll文件表明为静态网页，无须编译）</p><p>而项目本身为了更好地进行管理和记录，还是要发布到git上面的，作为一个普通的私有仓库，名称可以任意（如 blog）</p><p>这样，每次要增删改完文章只需要执行<code>hexo clean &amp;&amp; hexo d -g</code>即可发布到git仓库上<br>注意，不同git服务商git pages规则不一样。<br>比方说我gitee和github的用户名都是linshenkx<br>但是gitee要求的仓库名是linshenkx，而github的仓库名就必须是linshenkx.github.io了<br>而github的git pages仓库在接收到推送后就自动（编译）部署<br>gitee则需要手动触发更新</p><p>截至到这一步是大多数人的做法，即git上两个仓库并存，一（或多）个git pages公有仓库做展示，一个blog仓库存放博客源码<br>注意：如果git pages仓库允许私有，则可以使用一个仓库多个分支来实现相同效果。<br>但还是推荐使用两个仓库，因为这样更通用，设计上也更合理。</p><p>工程总体结构如下，为普通hexo工程：<br><img src="https://lian-gallery.oss-cn-guangzhou.aliyuncs.com/img/1631718173(1).png"><br>博客源码目录结构如下，为多层级结构：<br><img src="https://lian-gallery.oss-cn-guangzhou.aliyuncs.com/img/1631718305(1).png"></p><h3 id="2-halo"><a href="#2-halo" class="headerlink" title="2 halo"></a>2 halo</h3><p>halo的使用看官方文档一般就够了，这里需要补充的是其代理配置。<br>因为halo的在线下载更新主题功能通常需要连接到github，我习惯通过代理访问<br>这里提供一下配置方法<br>即在容器启动时添加JVM参数即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -d --name halo --network host -e JVM_OPTS=&quot;-Dhttp.proxyHost=127.0.0.1 -Dhttp.proxyPort=7890 -Dhttps.proxyHost=127.0.0.1 -Dhttps.proxyPort=7890&quot; -v /opt/halo/workspace:/root/.halo --restart=always halohub/halo</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="3-同步"><a href="#3-同步" class="headerlink" title="3 同步"></a>3 同步</h3><p>这才是重点</p><h4 id="1-同步的方向"><a href="#1-同步的方向" class="headerlink" title="1 同步的方向"></a>1 同步的方向</h4><p>即在哪里写文章，同步到哪里</p><p>我还是习惯用idea写markdown文档而不是在网页上。<br>所以确定是流向为 hexo-&gt;halo</p><h4 id="2-技术支撑"><a href="#2-技术支撑" class="headerlink" title="2 技术支撑"></a>2 技术支撑</h4><p>halo支持导入markdown文件，所以主要问题为hexo的markdown博客源码文件的获取<br>hexo文章存储路径为 source/_posts ，有多层级文件夹，可以简单地理解成文件IO操作获取文章内容。<br>但关键是存储在git上，这里可以用JGit进行操作。<br>同时，JGit支持获取两次commit之间的文件变化情况。<br>即可以捕获到文章的增删改操作，而不用每次都全量地同步。</p><h4 id="3-成果"><a href="#3-成果" class="headerlink" title="3 成果"></a>3 成果</h4><p>又处理了一些细节问题，最终还是自己做了个haloSyncServer同步程序，<br>封装成docker，放服务器上跑，实现同步。<br>待整理后开源。</p>]]></content>
      
      
      <categories>
          
          <category> 程序员杂记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产力 </tag>
            
            <tag> 博客 </tag>
            
            <tag> hexo </tag>
            
            <tag> halo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>clash代理工具docker版使用笔记</title>
      <link href="//clash_docker/"/>
      <url>//clash_docker/</url>
      
        <content type="html"><![CDATA[<p>clash docker镜像使用笔记，以及常用代理配置方法</p><span id="more"></span><h3 id="1-执行命令"><a href="#1-执行命令" class="headerlink" title="1 执行命令"></a>1 执行命令</h3><p>需自行提供config.yaml 文件，这里推荐直接使用host网络，可以直接使用127.0.0.1 7890作为代理</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --name clash --network host --restart=always  -v /opt/clash/config.yaml:/root/.config/clash/config.yaml -d dreamacro/clash </span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-常规安装"><a href="#2-常规安装" class="headerlink" title="2 常规安装"></a>2 常规安装</h3><p>如果没有安装docker的话，可以使用常规方法启动</p><ol><li>下载<br><a href="https://github.com/Dreamacro/clash/releases">https://github.com/Dreamacro/clash/releases</a></li><li>配置<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gzip -d clash-linux-amd64-v1.5.0.gz</span><br><span class="line">mkdir /opt/clash</span><br><span class="line">mv clash-linux-amd64-v1.5.0 /opt/clash/clash</span><br><span class="line">echo &quot;/opt/clash/clash &gt;/opt/clash/start.out 2&gt;&amp;1  &amp;&quot; &gt;&gt; /opt/clash/start.sh</span><br><span class="line">chmod -R +x  /opt/clash/</span><br><span class="line"></span><br></pre></td></tr></table></figure>把配置文件放到/root/.config/clash 下取代默认的config.yaml</li><li>启动<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh /opt/clash/start.sh </span><br></pre></td></tr></table></figure></li><li>查看clash启动日志<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail -n 100 -f /opt/clash/start.out</span><br></pre></td></tr></table></figure></li></ol><h3 id="2-linux系统设置代理"><a href="#2-linux系统设置代理" class="headerlink" title="2 linux系统设置代理"></a>2 linux系统设置代理</h3><p>在 /etc/profile 添加all_proxy配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &#x27;all_proxy=&quot;http://127.0.0.1:7890&quot;&#x27;&gt;&gt; /etc/profile</span><br></pre></td></tr></table></figure><h3 id="3-java程序设置代理"><a href="#3-java程序设置代理" class="headerlink" title="3 java程序设置代理"></a>3 java程序设置代理</h3><p>测试时，在通过代码方式添加：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(<span class="string">&quot;proxyHost&quot;</span>, <span class="string">&quot;127.0.0.1&quot;</span>);</span><br><span class="line">System.setProperty(<span class="string">&quot;proxyPort&quot;</span>, <span class="string">&quot;7890&quot;</span>);</span><br></pre></td></tr></table></figure><p>发布时，建议通过jvm参数配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-DsocksProxyHost=127.0.0.1 -DsocksProxyPort=7890</span><br><span class="line">-Dhttp.proxyHost=127.0.0.1 -Dhttp.proxyPort=7890</span><br><span class="line">-Dhttps.proxyHost=127.0.0.1 -Dhttps.proxyPort=7890</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如下，为使用系统代理方式启动halo，这样就可以在halo的管理界面使用在线下载、更新主题功能了，否则github连接过慢主题基本无法在线下载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -d --name halo --network host -e JVM_OPTS=&quot;-Dhttp.proxyHost=127.0.0.1 -Dhttp.proxyPort=7890 -Dhttps.proxyHost=127.0.0.1 -Dhttps.proxyPort=7890&quot; -v /opt/halo/workspace:/root/.halo --restart=always halohub/halo</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 程序员杂记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产力 </tag>
            
            <tag> clash </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ranger2.1编译笔记</title>
      <link href="//ranger-compile/"/>
      <url>//ranger-compile/</url>
      
        <content type="html"><![CDATA[<p>本文记录了 ranger2.1版本的编译过程，与遇到的一些bug的解决方法。</p><span id="more"></span><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><h3 id="1-外部工具依赖"><a href="#1-外部工具依赖" class="headerlink" title="1 外部工具依赖"></a>1 外部工具依赖</h3><p>需要linux环境，保证python命令可用，如果只装了python3，推荐</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt install -y python-is-python3</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>另外还需要安装gcc</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt install gcc -y</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-排除kylin模块"><a href="#2-排除kylin模块" class="headerlink" title="2 排除kylin模块"></a>2 排除kylin模块</h3><p>kylin2.6.4依赖calcite-linq4j-1.16.0-kylin-r2<br>这个jar包在外部maven仓库是找不到的，导致build失败，参考：<br><a href="https://issues.apache.org/jira/browse/RANGER-2994">https://issues.apache.org/jira/browse/RANGER-2994</a><br><a href="https://issues.apache.org/jira/browse/RANGER-2999">https://issues.apache.org/jira/browse/RANGER-2999</a></p><p>因为是kylin本身的依赖问题，不是ranger的锅，这里谴责一下kylin然后对其选择性放弃<br>放弃方法：在pom.xml将其所有的module注释掉，如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--                &lt;module&gt;plugin-kylin&lt;/module&gt;--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--                &lt;module&gt;ranger-kylin-plugin-shim&lt;/module&gt;--&gt;</span></span><br></pre></td></tr></table></figure><h3 id="3-ranger-examples-distro错误"><a href="#3-ranger-examples-distro错误" class="headerlink" title="3 ranger-examples-distro错误"></a>3 ranger-examples-distro错误</h3><p>参考：<a href="https://github.com/apache/ranger/pull/71">https://github.com/apache/ranger/pull/71</a><br>根据issue，修改 sampleapp.xml 和 plugin-sampleapp.xml</p><h3 id="4-ranger-client"><a href="#4-ranger-client" class="headerlink" title="4 ranger client"></a>4 ranger client</h3><p>官方的rangerClient有bug，新版本已修复，但还没发布，需要自行编译<br>步骤：</p><ol><li>参考官方仓库 ranger/intg/src/main/java 工程，可直接搬运过来，原来的目录要先删掉</li><li>将 ranger/intg/pom.xml 中 ranger-plugins-common 的版本从${project.version} 改为2.1.0（即公网仓库能找到的最新版本）</li></ol><h2 id="执行编译"><a href="#执行编译" class="headerlink" title="执行编译"></a>执行编译</h2><h3 id="1-编译各个组件-tar-gz包"><a href="#1-编译各个组件-tar-gz包" class="headerlink" title="1 编译各个组件 tar.gz包"></a>1 编译各个组件 tar.gz包</h3><p>在ranger工程下执行，执行成功可以在target下看到包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mvn clean compile package assembly:assembly -DskipTests -DskipJSTests -Dpmd.skip=true</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>日志参考：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary for ranger 2.1.1-SNAPSHOT:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] ranger ............................................. SUCCESS [02:31 min]</span><br><span class="line">[INFO] Jdbc SQL Connector ................................. SUCCESS [ 12.635 s]</span><br><span class="line">[INFO] Credential Support ................................. SUCCESS [ 14.241 s]</span><br><span class="line">[INFO] Audit Component .................................... SUCCESS [ 40.494 s]</span><br><span class="line">[INFO] ranger-plugin-classloader .......................... SUCCESS [  8.902 s]</span><br><span class="line">[INFO] Common library for Plugins ......................... SUCCESS [02:43 min]</span><br><span class="line">[INFO] ranger-intg ........................................ SUCCESS [ 15.121 s]</span><br><span class="line">[INFO] Installer Support Component ........................ SUCCESS [  4.590 s]</span><br><span class="line">[INFO] Credential Builder ................................. SUCCESS [  7.686 s]</span><br><span class="line">[INFO] Embedded Web Server Invoker ........................ SUCCESS [ 12.830 s]</span><br><span class="line">[INFO] Key Management Service ............................. SUCCESS [ 52.264 s]</span><br><span class="line">[INFO] HBase Security Plugin Shim ......................... SUCCESS [ 15.598 s]</span><br><span class="line">[INFO] HBase Security Plugin .............................. SUCCESS [ 32.827 s]</span><br><span class="line">[INFO] Hdfs Security Plugin ............................... SUCCESS [ 34.502 s]</span><br><span class="line">[INFO] Hive Security Plugin ............................... SUCCESS [ 35.109 s]</span><br><span class="line">[INFO] Knox Security Plugin Shim .......................... SUCCESS [ 11.906 s]</span><br><span class="line">[INFO] Knox Security Plugin ............................... SUCCESS [ 49.254 s]</span><br><span class="line">[INFO] Storm Security Plugin .............................. SUCCESS [ 23.171 s]</span><br><span class="line">[INFO] YARN Security Plugin ............................... SUCCESS [ 16.011 s]</span><br><span class="line">[INFO] Ozone Security Plugin .............................. SUCCESS [ 16.418 s]</span><br><span class="line">[INFO] Ranger Util ........................................ SUCCESS [01:43 min]</span><br><span class="line">[INFO] Unix Authentication Client ......................... SUCCESS [  8.165 s]</span><br><span class="line">[INFO] Security Admin Web Application ..................... SUCCESS [21:33 min]</span><br><span class="line">[INFO] KAFKA Security Plugin .............................. SUCCESS [ 21.139 s]</span><br><span class="line">[INFO] SOLR Security Plugin ............................... SUCCESS [ 16.487 s]</span><br><span class="line">[INFO] NiFi Security Plugin ............................... SUCCESS [ 18.495 s]</span><br><span class="line">[INFO] NiFi Registry Security Plugin ...................... SUCCESS [ 15.196 s]</span><br><span class="line">[INFO] Kudu Security Plugin ............................... SUCCESS [  9.105 s]</span><br><span class="line">[INFO] Unix User Group Synchronizer ....................... SUCCESS [ 47.248 s]</span><br><span class="line">[INFO] Ldap Config Check Tool ............................. SUCCESS [  8.631 s]</span><br><span class="line">[INFO] Unix Authentication Service ........................ SUCCESS [  9.789 s]</span><br><span class="line">[INFO] Unix Native Authenticator .......................... SUCCESS [  4.242 s]</span><br><span class="line">[INFO] KMS Security Plugin ................................ SUCCESS [ 34.582 s]</span><br><span class="line">[INFO] Tag Synchronizer ................................... SUCCESS [ 28.172 s]</span><br><span class="line">[INFO] Hdfs Security Plugin Shim .......................... SUCCESS [  7.847 s]</span><br><span class="line">[INFO] Hive Security Plugin Shim .......................... SUCCESS [ 13.040 s]</span><br><span class="line">[INFO] YARN Security Plugin Shim .......................... SUCCESS [ 10.582 s]</span><br><span class="line">[INFO] OZONE Security Plugin Shim ......................... SUCCESS [ 16.250 s]</span><br><span class="line">[INFO] Storm Security Plugin shim ......................... SUCCESS [ 10.807 s]</span><br><span class="line">[INFO] KAFKA Security Plugin Shim ......................... SUCCESS [  8.435 s]</span><br><span class="line">[INFO] SOLR Security Plugin Shim .......................... SUCCESS [ 10.037 s]</span><br><span class="line">[INFO] Atlas Security Plugin Shim ......................... SUCCESS [  9.938 s]</span><br><span class="line">[INFO] KMS Security Plugin Shim ........................... SUCCESS [ 13.735 s]</span><br><span class="line">[INFO] ranger-examples .................................... SUCCESS [  2.577 s]</span><br><span class="line">[INFO] Ranger Examples - Conditions and ContextEnrichers .. SUCCESS [ 13.963 s]</span><br><span class="line">[INFO] Ranger Examples - SampleApp ........................ SUCCESS [ 14.751 s]</span><br><span class="line">[INFO] Ranger Examples - Ranger Plugin for SampleApp ...... SUCCESS [  9.067 s]</span><br><span class="line">[INFO] sample-client ...................................... SUCCESS [ 21.965 s]</span><br><span class="line">[INFO] Apache Ranger Examples Distribution ................ SUCCESS [ 17.195 s]</span><br><span class="line">[INFO] Ranger Tools ....................................... SUCCESS [ 32.274 s]</span><br><span class="line">[INFO] Atlas Security Plugin .............................. SUCCESS [ 15.610 s]</span><br><span class="line">[INFO] SchemaRegistry Security Plugin ..................... SUCCESS [ 33.643 s]</span><br><span class="line">[INFO] Sqoop Security Plugin .............................. SUCCESS [ 32.979 s]</span><br><span class="line">[INFO] Sqoop Security Plugin Shim ......................... SUCCESS [  8.851 s]</span><br><span class="line">[INFO] Presto Security Plugin ............................. SUCCESS [ 36.558 s]</span><br><span class="line">[INFO] Presto Security Plugin Shim ........................ SUCCESS [01:06 min]</span><br><span class="line">[INFO] Elasticsearch Security Plugin Shim ................. SUCCESS [ 14.989 s]</span><br><span class="line">[INFO] Elasticsearch Security Plugin ...................... SUCCESS [ 13.180 s]</span><br><span class="line">[INFO] Apache Ranger Distribution ......................... SUCCESS [38:46 min]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time:  01:24 h</span><br><span class="line">[INFO] Finished at: 2021-05-27T15:47:58+08:00</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><h3 id="2-编译-ranger-client-jar包"><a href="#2-编译-ranger-client-jar包" class="headerlink" title="2 编译 ranger_client jar包"></a>2 编译 ranger_client jar包</h3><p>在ranger/intg下执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean compile package install -DskipTests</span><br></pre></td></tr></table></figure><p>可以在 ranger/intg/target 看到生成后的jar包</p>]]></content>
      
      
      <categories>
          
          <category> ranger </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WSL2配置与结合IDEA2021使用体验（及wsl-gui踩坑）</title>
      <link href="//wsl2_idea2021/"/>
      <url>//wsl2_idea2021/</url>
      
        <content type="html"><![CDATA[<p>最近把IDEA更新到了2021，发现新版本增强了对WSL2的支持。之前的WSL2很鸡肋，吹很厉害，但真要开发又很不方便。 本来还想等着windows增强WSL2对gui的支持，然后在WSL2里装个IDEA。<br>倒是IDEA动作快一点，先做了支持。不错，值得捣鼓一番。之前为了gui把wsl环境弄得很混乱，直接重装，随便记录一下操作。<br>2021.11更新：wsl-gui环境配置及idea使用</p><span id="more"></span><h3 id="一-配置WSL"><a href="#一-配置WSL" class="headerlink" title="一 配置WSL"></a>一 配置WSL</h3><p>按惯例，先给出官方文档地址，基础性操作参考这个： <a href="https://docs.microsoft.com/zh-cn/windows/wsl/">https://docs.microsoft.com/zh-cn/windows/wsl/</a></p><h4 id="1-开启wsl"><a href="#1-开启wsl" class="headerlink" title="1 开启wsl"></a>1 开启wsl</h4><p>参考：<a href="https://docs.microsoft.com/zh-cn/windows/wsl/install-win10">https://docs.microsoft.com/zh-cn/windows/wsl/install-win10</a></p><h5 id="1-安装「适用于-Linux-的-Windows-子系统」和「虚拟机平台」这两个可选组件"><a href="#1-安装「适用于-Linux-的-Windows-子系统」和「虚拟机平台」这两个可选组件" class="headerlink" title="1 安装「适用于 Linux 的 Windows 子系统」和「虚拟机平台」这两个可选组件"></a>1 安装「适用于 Linux 的 Windows 子系统」和「虚拟机平台」这两个可选组件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart</span><br><span class="line">dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart</span><br></pre></td></tr></table></figure><h5 id="2-重启"><a href="#2-重启" class="headerlink" title="2 重启"></a>2 重启</h5><h5 id="3-配置默认使用wsl2"><a href="#3-配置默认使用wsl2" class="headerlink" title="3 配置默认使用wsl2"></a>3 配置默认使用wsl2</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --set-default-version 2</span><br></pre></td></tr></table></figure><h4 id="2-安装windows-terminal"><a href="#2-安装windows-terminal" class="headerlink" title="2 安装windows terminal"></a>2 安装windows terminal</h4><p>在应用商店下载安装，自行美化即可，新版本已可直接通过选项配置，不需要直接操作配置文件</p><h3 id="二-配置ubuntu"><a href="#二-配置ubuntu" class="headerlink" title="二 配置ubuntu"></a>二 配置ubuntu</h3><h4 id="1-安装ubuntu"><a href="#1-安装ubuntu" class="headerlink" title="1 安装ubuntu"></a>1 安装ubuntu</h4><p>在应用商店搜索下载ubuntu，当前版本为：20.04.2，下载完打开图标进行安装<br>安装过程会提示输入一个默认用户和对应密码，注意不能用root（已存在）</p><h4 id="2-修改默认用户为root"><a href="#2-修改默认用户为root" class="headerlink" title="2 修改默认用户为root"></a>2 修改默认用户为root</h4><h5 id="1-查看当前wsl列表："><a href="#1-查看当前wsl列表：" class="headerlink" title="1 查看当前wsl列表："></a>1 查看当前wsl列表：</h5><p>   之所以要这一步是因为如果装了多个版本的ubuntu，其标识可能不一样<br>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl -l </span><br></pre></td></tr></table></figure></p><h5 id="2-修改对应wsl默认用户"><a href="#2-修改对应wsl默认用户" class="headerlink" title="2 修改对应wsl默认用户"></a>2 修改对应wsl默认用户</h5>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ubuntu config --default-user root</span><br></pre></td></tr></table></figure><h4 id="3-将linux根目录映射到win10磁盘"><a href="#3-将linux根目录映射到win10磁盘" class="headerlink" title="3 将linux根目录映射到win10磁盘"></a>3 将linux根目录映射到win10磁盘</h4><p>在win10文件地址栏输入 \wsl$ ，可以看到已安装的wsl文件系统，如 Ubuntu<br>右键-映射网络驱动器，选择要映射的盘符即可。<br>另外注意，可以用<code>explorer.exe . </code>命令使用windows文件资源管理器打开当前路径</p><h4 id="4-配置-terminal-打开默认路径"><a href="#4-配置-terminal-打开默认路径" class="headerlink" title="4 配置 terminal 打开默认路径"></a>4 配置 terminal 打开默认路径</h4><p>在 terminal 配置文件中，找到Ubuntu配置项，添加如下配置即可<br>startingDirectory” : “//wsl$/Ubuntu-20.04/home”</p><h4 id="5-更新镜像源"><a href="#5-更新镜像源" class="headerlink" title="5 更新镜像源"></a>5 更新镜像源</h4><h5 id="1-查看具体版本"><a href="#1-查看具体版本" class="headerlink" title="1 查看具体版本"></a>1 查看具体版本</h5>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/lsb-release</span><br></pre></td></tr></table></figure><h5 id="2-到-https-mirrors-tuna-tsinghua-edu-cn-help-ubuntu-复制最新的镜像"><a href="#2-到-https-mirrors-tuna-tsinghua-edu-cn-help-ubuntu-复制最新的镜像" class="headerlink" title="2 到 https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ 复制最新的镜像"></a>2 到 <a href="https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/">https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/</a> 复制最新的镜像</h5><p>   阿里的支持傻瓜式操作，但经常打不开，就算了<br>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">备份：</span></span><br><span class="line">mv /etc/apt/sources.list /etc/apt/sources.list.backup</span><br><span class="line"><span class="meta">#</span><span class="bash">编辑：</span></span><br><span class="line">vi /etc/apt/sources.list</span><br></pre></td></tr></table></figure></p><h5 id="3-更新"><a href="#3-更新" class="headerlink" title="3 更新"></a>3 更新</h5>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get update -y &amp;&amp; apt-get upgrade -y</span><br></pre></td></tr></table></figure><h4 id="6-配置代理"><a href="#6-配置代理" class="headerlink" title="6 配置代理"></a>6 配置代理</h4><p>如果代理软件是装在windows系统的，参考第四部分的 获取宿主机ip 部分配置my.win就行了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo &#x27;export ALL_PROXY=&quot;http://$host_ip:7890&quot;&#x27;&gt;&gt;/etc/profile</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="7-禁止添加Windows-PATH"><a href="#7-禁止添加Windows-PATH" class="headerlink" title="7 禁止添加Windows-PATH"></a>7 禁止添加Windows-PATH</h4><blockquote><p>此部分内容过期，可不配置</p></blockquote><p>为了避免开发环境冲突，需关闭Windows-PATH<br>wsl.conf配置参考：<a href="https://docs.microsoft.com/zh-cn/windows/wsl/wsl-config">https://docs.microsoft.com/zh-cn/windows/wsl/wsl-config</a><br>新建 /etc/wsl.conf，添加以下内容：<br>    <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 不加载Windows中的PATH内容</span></span><br><span class="line">[interop]</span><br><span class="line">appendWindowsPath = false</span><br></pre></td></tr></table></figure></p><h3 id="三-IDEA开发和总结"><a href="#三-IDEA开发和总结" class="headerlink" title="三 IDEA开发和总结"></a>三 IDEA开发和总结</h3><p>经过测试，常规SpringBoot工程，在Windows目录下的，可以使用WSL环境进行debug、部署<br>对于纯maven工程，支持也还好</p><blockquote><p>此部分过期：<br>但对于纯gradle工程（非spring），则无法完成gradle初始化，无论工程是否是在WSL中都不行<br>设置JDK是WSL，但将Gradle路径指定为WSL却一直不行，说路径有问题，只能使用默认的Gradle。<br>根据issue的说法，建议将Windows的gradle path设置为WSL的路径，不过我没有试，我觉得这单纯就是官方的bug。<br>官方issue：<a href="https://youtrack.jetbrains.com/issues/IDEA?q=wsl">https://youtrack.jetbrains.com/issues/IDEA?q=wsl</a><br>另外，对于WSL下工程的git，idea似乎也无法识别</p></blockquote><p>总之，IDEA（2021.1）的WSL2支持尚不成熟，但是也已经初步支持，还可以自动下载、选择WSL环境的JDK<br>可以作为日常开发的辅助支持，但目前仍不建议全面迁移过去</p><p>对我来说，WSL的开发应用场景主要在于：<br>大数据功能开发：之前简单的hdfs操作都需要在windows中配置hadoop相关的环境变量，而且各种版本或兼容性问题，很麻烦，以后这些直接丢wsl里面<br>大数据组件编译：很多组件并不是纯Java代码，可能还需要一些脚本，这些有平台依赖性的，很可能因为windows而导致编译失败<br>脚本开发：在windows新建的sh脚本在拿去linux系统之前都要先转换一下格式，以后直接作为wsl系统下文件新建，就不用考虑这些乱七八糟的问题</p><p>最终目标是以后把所有的工程都迁移到wsl下，开发相关全部使用wsl环境</p><p>2021年11月更新：<br>IDEA2021.2.3的wsl工程的bug已经较少，git、gradle支持相对正常，可以满足一般场景下的开发需求。<br>但仍不能在工程里读取到wsl的环境变量（能读取到windows的），所以我选择放弃。<br>直接在WSLg里装jetbrains toolbox，然后启动idea。<br>除去一些小bug，已经和原生linux体验差不多了，用来做大数据开发很舒服。<br>配置好环境就可以直接本地进行大数据操作，不再需要远程开发了，之前被这东西折磨得不行。<br>性能的话比windows idea的wsl开发要快很多，前提是你内存够。<br>wsl比较吃内存，当然开发的标配是32g，一般还是够用的。</p><p>目前遇到的问题有：</p><ol><li>全屏偏移<br>在全屏状态下，无法进行部分拖拽操作，可能与这个issue有关<br><a href="https://github.com/microsoft/wslg/issues/502">https://github.com/microsoft/wslg/issues/502</a></li><li>卡死<br>通常是在调出新的窗口的时候<br>需要手动wsl –shutdown，再启动，但可能造成idea文件未保存，修改丢失的情况<br>在windows上不管怎么造基本都不会出现idea文件修改丢失的情况</li></ol><h3 id="四-其他"><a href="#四-其他" class="headerlink" title="四 其他"></a>四 其他</h3><h4 id="1-wsl2获取宿主机ip"><a href="#1-wsl2获取宿主机ip" class="headerlink" title="1 wsl2获取宿主机ip"></a>1 wsl2获取宿主机ip</h4><p>windows可以直接访问wsl的应用，使用localhost即可，但是不能使用127.0.0.1<br>wsl可以通过以下操作获取windows的ip并设置host，以后直接提供my.win访问，一般用于访问windows的代理服务<br>注意需要使用/tmp/hosts作为中转，否则/etc/hosts会被清空</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">echo &#x27;host_ip=$(cat /etc/resolv.conf |grep &quot;nameserver&quot; |cut -f 2 -d &quot; &quot;)&#x27;&gt;&gt;/etc/profile</span><br><span class="line">echo &#x27;sed -e &quot;/my.win/d&quot;  /etc/hosts  &gt; /tmp/hosts&#x27;&gt;&gt;/etc/profile</span><br><span class="line">echo &#x27;cat /tmp/hosts &gt; /etc/hosts&gt;&gt;/etc/profile</span><br><span class="line">echo &#x27;echo &quot;$host_ip my.win&quot; &gt;&gt;/etc/hosts&#x27;&gt;&gt;/etc/profile</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2-局域网访问wsl2"><a href="#2-局域网访问wsl2" class="headerlink" title="2 局域网访问wsl2"></a>2 局域网访问wsl2</h4><p>在powershell 管理员模式下执行以下命令，局域网其他机器只需要连接宿主机的28090端口即可访问wsl2内19909端口的内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">netsh interface portproxy add v4tov4 listenport=28090 listenaddress=0.0.0.0 connectport=19909 connectaddress=127.0.0.1</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="3-手动添加jetbrains-toolbox软件的快捷方式到开始菜单"><a href="#3-手动添加jetbrains-toolbox软件的快捷方式到开始菜单" class="headerlink" title="3 手动添加jetbrains toolbox软件的快捷方式到开始菜单"></a>3 手动添加jetbrains toolbox软件的快捷方式到开始菜单</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.local/share/applications</span><br><span class="line">sudo cp *.desktop /usr/share/applications</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>参考链接：<a href="https://juejin.cn/post/6966630345915498526">https://juejin.cn/post/6966630345915498526</a></p><h4 id="3-中文环境及输入法"><a href="#3-中文环境及输入法" class="headerlink" title="3 中文环境及输入法"></a>3 中文环境及输入法</h4><p>这个教程比较多，而且写得都不一样，但大同小异，需要注意的是<br>输入法没有切换处理很可能是快捷键被windows系统的覆盖了，<br>需要手动调出来输入法的配置界面，修改快捷键<br>比方说fcix的是fcitx-config-gtk3命令<br>然后我的配置是这样的，就是shift键，保持和windows使用习惯统一<br><img src="https://lian-gallery.oss-cn-guangzhou.aliyuncs.com/img/1636899123(1).png"></p><p>这里的教程可以参考：<a href="https://monkeywie.cn/2021/09/26/wsl2-gui-idea-config/">https://monkeywie.cn/2021/09/26/wsl2-gui-idea-config/</a><br>我的配置和里面的不一样， 也没有遇到idea切不出输入法的问题<br>如果有的话参考文章里的方法或者看：<a href="https://github.com/microsoft/wslg/issues/278">https://github.com/microsoft/wslg/issues/278</a></p>]]></content>
      
      
      <categories>
          
          <category> 程序员杂记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生产力 </tag>
            
            <tag> WSL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据通用计算平台(支持flink、spark等)(1)系统调研及设计</title>
      <link href="//bigdata_compute_platform/"/>
      <url>//bigdata_compute_platform/</url>
      
        <content type="html"><![CDATA[<p>项目源于对flink_sql流计算任务的实际使用需求，最初目标是设计一个系统可以在线提交sql生成flink流式计算任务，并进行监控监测。 后延申至支持在线jar包提交的方式，同时支持批式计算任务。并以模块化开发的思路，引入对spark的支持。</p><span id="more"></span><h3 id="一-简介"><a href="#一-简介" class="headerlink" title="一 简介"></a>一 简介</h3><h4 id="1-系统介绍"><a href="#1-系统介绍" class="headerlink" title="1 系统介绍"></a>1 系统介绍</h4><p>本系统基于多种不同的底层计算框架，如flink、spark，提供可视化web界面，实现大数据任务的在线发布、管理、监控等功能。<br>支持多种任务类型，如sql提交、jar包提交等，屏蔽底层部署细节与参数配置，使开发人员可以专注于业务本身，提高工作效率<br>平台本身依托于现有大数据计算框架，不会引入其他依赖，不会对现有开发造成入侵，可放心使用。</p><h4 id="2-设计目标"><a href="#2-设计目标" class="headerlink" title="2 设计目标"></a>2 设计目标</h4><ol><li>支持多种计算框架：flink、spark等</li><li>支持多种计算任务：批、流</li><li>支持多种任务类型：sql、jar包</li><li>支持功能：发布、停止、删除、监控等</li><li>零入侵：纯客户端工具，不需要调整业务代码，不需要修改平台配置</li></ol><h4 id="3-RoadMap"><a href="#3-RoadMap" class="headerlink" title="3 RoadMap"></a>3 RoadMap</h4><ol><li>支持更多的提交方式，如脚本提交</li><li>支持更多的计算框架，如storm</li><li>支持更多的底层运行平台，如kubernetes、standalone等</li></ol><h3 id="二-设计调研"><a href="#二-设计调研" class="headerlink" title="二 设计调研"></a>二 设计调研</h3><h4 id="0-基础方向"><a href="#0-基础方向" class="headerlink" title="0 基础方向"></a>0 基础方向</h4><h5 id="1-底层运行平台的选择"><a href="#1-底层运行平台的选择" class="headerlink" title="1. 底层运行平台的选择"></a>1. 底层运行平台的选择</h5><p>不管是flink还是spark，都只是计算框架，需要真正的运行平台作为支持，主流的有yarn、kubernetes、standalone等</p><ol><li>local模式基本只适用于测试，这里不作考虑   </li><li>其中standalone即独立集群模式建设维护成本高，且重复建设易造成资源浪费（如flink一套、spark一套）。<br>不过standalone往往可以实现更多的特性，如spark的spark.master.rest.enabled配置项就只支持standalone模式，<br>而且大部分时候开发者只会用其中一个平台，并不存在重复建设的情况。<br>这里出于开发成本考虑，先不作实现，后续考虑支持。</li><li>各个计算框架基本都出了对k8s的支持，但文档来看还并不成熟，实际生产环境使用还较少，先观望，后续考虑支持。</li><li>Mesos我不熟<br>根据排除法，就只剩下yarn作为底层计算框架了。<br>优点是成熟、稳定，经过大规模检验，基本生产环境都是用yarn，而且不同的计算框架都可以共用一套yarn计算平台。</li></ol><h5 id="2-提交部署方式的选择"><a href="#2-提交部署方式的选择" class="headerlink" title="2. 提交部署方式的选择"></a>2. 提交部署方式的选择</h5><ol><li>在确定底层运行平台使用yarn之后，提交部署方式主要就剩下cluster、client两种了（此处使用spark的说法）<br>两者的区别主要在于启动程序的解析运行在客户端还是在集群上进行。<br>作为平台型工具，自然是选择cluster，减小客户端压力。client更适合个人调试用，可以直接看到输出。</li><li>对应flink的说法即为 per-job方式（客户端）和application方式（集群），即使用application方式。<br>另外需注意，application方式得在flink高版本（1.11+）才有。</li><li>另外，flink还有个session模式，简单理解为上面的是一任务一集群，这个是多任务一集群，适合轻量级使用，自带web端jar包提交，方便快捷。<br>但不建议生产环境使用，很容易一个任务崩就导致集群崩溃。</li></ol><h5 id="3-sql任务的实现"><a href="#3-sql任务的实现" class="headerlink" title="3. sql任务的实现"></a>3. sql任务的实现</h5><p>将sql保存为文件，使用约定好的jar程序进行解析，即将sql任务转化为jar任务。其他类型的任务同理。</p><h4 id="1-flink"><a href="#1-flink" class="headerlink" title="1 flink"></a>1 flink</h4><p>flink计算平台网上有很多，其中质量比较好且更新时间比较近的主要有：Flink-SQL流计算平台管理系统（ <a href="https://github.com/zhp8341/flink-streaming-platform-web">https://github.com/zhp8341/flink-streaming-platform-web</a> ）<br>本人最开始就是受了这个系统启发，最初的代码和思路也有部分参考，比较推荐。<br>优点是功能完整，且更新快。不过sql方面定制化程度有点高，不够通用，存在一定程度的入侵，且功能对于我来说有点花哨，支持flink本地模式、yarn-per模式、STANDALONE模式，最新的application模式倒没有。</p><h5 id="代码提交or接口提交or命令行提交？"><a href="#代码提交or接口提交or命令行提交？" class="headerlink" title="代码提交or接口提交or命令行提交？"></a>代码提交or接口提交or命令行提交？</h5><ol><li>代码提交？<br>yarn-application方式官方不支持，网上有的一般是session模式的，还有一些自己看源码挖出来提交代码的。<br>这里不推荐的去挖代码，官方没有放出来的东西可能换个版本就不一样了，如非必要，这种开发思路并不可取。</li><li>接口提交？<br>flink的web界面确实有jar包提交功能，但首先要把flink跑起来，即session模式，这里不使用。</li><li>命令行提交<br>默认的提交方法，万能的。</li></ol><h5 id="监控实现？"><a href="#监控实现？" class="headerlink" title="监控实现？"></a>监控实现？</h5><ol><li>提交yarn的时候指定tag，根据tag进行搜索，获取任务的appId，进行任务追踪</li><li>yarn层根据appId和yarn的rest接口跟踪任务在yarn上的状态</li><li>flink层根据appId可以组成出flink的web监控页面，同时flink提供了web界面所需的各项数据的rest接口，可以二次开发</li></ol><h5 id="任务管理？"><a href="#任务管理？" class="headerlink" title="任务管理？"></a>任务管理？</h5><p>flink任务的停止可以直接调用其rest接口<br>savepoint信息也可以提供rest接口获取，savepoint的执行通过rest执行好像有问题，这里还是先用命令行操作</p><h4 id="2-spark"><a href="#2-spark" class="headerlink" title="2 spark"></a>2 spark</h4><p>spark作为一个<del>过时</del>成熟稳重的计算框架，虽然不支持流批一体，但是相关的第三方框架更加成熟。</p><h5 id="代码提交or接口提交or命令行提交？-1"><a href="#代码提交or接口提交or命令行提交？-1" class="headerlink" title="代码提交or接口提交or命令行提交？"></a>代码提交or接口提交or命令行提交？</h5><p>我们还是先从官网看起。大体情况和flink类似。<br>没有官方的代码提交接口，都是各自挖掘出来的，而且写法还都不一样，这里不推荐。<br>接口提交有个spark.master.rest.enabled配置项，但只支持standalone模式<br>所以还是走命令行提交</p><h5 id="监控实现？-1"><a href="#监控实现？-1" class="headerlink" title="监控实现？"></a>监控实现？</h5><p>同flink，yarn层api+spark自身api</p><h5 id="任务管理？-1"><a href="#任务管理？-1" class="headerlink" title="任务管理？"></a>任务管理？</h5><p>主要就是想找个spark原生的rest api来kill它。<br>网上有一些spark隐藏rest api合集之类的东西，我本身是不想用的，毕竟你都知道是隐藏了，说明官方还不想给你用，你又不知道什么时间哪个版本就会变了<br>不过我也很好奇为什么官方不提供，很多人跟我有同样的想法，还跑去问了。<a href="https://issues.apache.org/jira/browse/SPARK-12528">https://issues.apache.org/jira/browse/SPARK-12528</a><br>standalone已经可以通过spark.master.rest.enabled来用了，但yarn还不行。讨论的人有提到可以用Livy或spark-jobserver。</p><ol><li>spark-jobserver的社区比较活跃，更新比较快，功能也很强大，但是需要继承指定类，造成极强的入侵性，这是无法容忍的，这里不做考虑。<br>按我的理解，你都能继承自己的类了，那你做出什么增强的功能都不稀奇，因为你本质上已经不是工具型产品，而是一个基于spark二次开发的平台，还不支持spark原生jar包。</li><li>Livy毕竟是Apache孵化项目，虽然还没毕业，但使用还是没什么问题的。<br>接口很简单，使用也很方便。主要就是可以通过livy的rest接口实现任务的发布、停止、状态获取、日志获取。<br>其实livy的存在已经和本项目对spark的支持重复了…但是本着轮子要自己造的思想，我们还是来<del>挑挑刺</del>研究一下livy的优缺点<br>嗯…文档不清晰：虽然参数都是对官方的封装，但是缺少必要的描述说明，比方说没有说jar包的路径默认是hdfs的（当然这不是主要问题…）<br>其实没什么大问题，还是推荐使用的，任务提交我们已经通过脚本方式实现了，这里就没必要再引入livy了，毕竟是个单独的web服务器，也挺重的。</li></ol><p>既然如此就还是走yarn的api去kill吧。</p><h3 id="三-设计思路"><a href="#三-设计思路" class="headerlink" title="三 设计思路"></a>三 设计思路</h3><p>项目以分为三个类型工程</p><ol><li>core ：核心处理工程。实现核心逻辑，对外提供统一的计算任务管理接口，不同计算平台任务依赖不同模块工程进行实现</li><li>module : 计算平台处理工程。对接不同的底层计算平台，如flink和spark</li><li>plugin : 独立插件工程。本质为自定义功能jar包，封装特定任务，按约定进行解析处理。如flink-sql的jar包封装。</li></ol><h4 id="core工程"><a href="#core工程" class="headerlink" title="core工程"></a>core工程</h4><p>core是计算平台的核心。对不同类型的任务进行封装，对外提供统一的api。并在数据库中记录、跟踪任务信息。<br>从功能设计来看主要类有：</p><ul><li>controller：对外接口类<br>对外提供start、stop、delete、getLog、getStatus等api<br>start需传入任务配置json，根据任务类型的不同交由不同的module工程的JobServer去解析处理。<br>start、stop、delete 也需调用不同的JobServer进行实现。<br>getLog、getStatus本质上只是读取数据库，而数据库的信息维护和同步也依赖各个module工程。</li><li>jobInfo: 任务信息类<br>对不同任务的统一封装，拥有以下属性<br>deployMode 任务类型：FLINK_SQL、FLINK_JAR、SPARK_SQL、SPARK_JAR等<br>runConfig 运行时配置，json，不同类型任务不同<br>runInfo 运行时信息，json，不同类型任务不同，如flink任务包含yarn运行信息和flink的jobId、savepoint信息等<br>runLog 任务运行日志,text，用以记录任务的基本操作记录以及状态更改记录等（不包含业务日志，不属于监控范围）<br>status 任务运行状态，基于yarn的job status进行抽象。各个类型任务可以有自身的状态标识，但必须实现特定接口以转化为顶层标准status。</li><li>JobServer: 任务处理核心抽象接口<br>定义了 start、stop、delete、checkAllJobStatus、checkJobStatus(String jobInfoId)等接口并由各个模块自行实现</li><li>SchedulerTask：定时任务类<br>定时调用各个模块的checkStatus接口，用以检测任务真实运行状态，维护数据库任务信息真实性有效性，防止已停任务仍在运行</li></ul><p>另外我还把命令行执行模块、yarn功能管理模块放到了这里。<br>命令行执行要包含日志记录、异步处理、异常处理、多命令同会话执行，还是比较麻烦的，踩了一些坑，以后再开一篇分享一下。<br>本来的设想就是把yarn作为基础运行平台，所以对yarn的操作也统一到了这里。</p><h4 id="module工程"><a href="#module工程" class="headerlink" title="module工程"></a>module工程</h4><p>一个deployMode对应一个JobServer实现类，相同类型的JobServer可共用一个module工程，如FLINK_SQL和FLINK_JAR<br>各个module工程以其JobServer实现类为核心，拥有自己的runConfig、runInfo、status实体类<br>除此之外，根据需求，还可以有自身的定时任务类，如Flink可定时执行savepoint操作</p><h4 id="plugin工程"><a href="#plugin工程" class="headerlink" title="plugin工程"></a>plugin工程</h4><p>plugin工程本质上为特殊任务类型的支持实现。<br>这里以FLINK_SQL工程为例。</p><p>Flink原生的sql只支持shell执行，且需在session模式中运行，<br>使用上也不算十分方便，需要先在配置文件中写好各种连接配置，下载好各种connector的jar包。<br>作为测试用倒还可以，语句可以直接修改，不用重新打包，但性能、可靠性都存疑，本质上难以在生产环境中应用。<br>而Jar包执行是绝对通用的，sql的功能基本都可以通过flink的executeSql来执行，理论上对sql脚本进行解析，一句句转成用executeSql来调用即可。<br>不过也有一些坑，比分说hive-catalog的相关配置，我觉得flink关于hive的设计是比较别扭的，hive一方面是作为元数据仓库使用，一方面又可以作为数据源连接，另外flink还支持hive作为sql方言使用。<br>而hive的连接也相对复杂一些，需要先准备hive-conf配置文件，其jar包的位置也应该是放到flink的classpath下，而不是用户的fat-jar。注意，这一点和connector是不一样的。<br>对于元数据仓库catalog、方言dialect这些特殊sql语句，都需要识别出来调用专门代码实现，而不能直接使用executeSql</p><p>sql脚本应该通过文件作为参数传递给Jar包进行读取，其他类型的任务同理，无非是把sql脚本换成scala脚本、shell脚本之类的<br>具体做法是将sql写为本地临时文件，通过flink的yarn.ship-files参数分发到Jar包运行容器里面，sql文件路径作为Jar包启动参数即可。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive使用bulkLoad批量导入数据到hbase</title>
      <link href="//hive_bulkLoad_hbase/"/>
      <url>//hive_bulkLoad_hbase/</url>
      
        <content type="html"><![CDATA[<p>本文主要参考了hbase和hive官方文档的说明，并结合cdh和hdp的一些教程以及个人在生产中的实践进行记录。主要内容有hbase bulkload的原理以及对应hive的操作步骤，最后基于cdh进行完整实验提供参考实例。<br>不过整个操作确实很复杂繁琐，不是很建议使用。现在有挺多使用Spark Bulkload，下次有机会尝试一下。<br>之前是遇到一个需求，源表在hbase上，需要重新生成rowkey并提取部分字段形成新表。功能很简单，就是行对行映射过去，但是效率太低耗时太长，用bulkload确实解决了当时的麻烦。<br>写这篇文章是对自己操作的复盘，同时也梳理一下知识。<br>实验环境为：CDH6.3.2，对应的各个组件版本为：hadoop3.0.0，hbase2.1.0，hive2.1.1</p><span id="more"></span><h2 id="一-hbase-bulk-loading批量加载-原理"><a href="#一-hbase-bulk-loading批量加载-原理" class="headerlink" title="一 hbase bulk loading批量加载 原理"></a>一 hbase bulk loading批量加载 原理</h2><p>这里推荐阅读hbase 最新官方文档：<a href="http://hbase.apache.org/2.3/book.html#arch.bulk.load">http://hbase.apache.org/2.3/book.html#arch.bulk.load</a><br>注意官方文档提供了2.2版本和2.3版本，这里原理讲解使用的是最新的，具体操作需结合根据实际使用版本来，有一些细节可能存在差异，例如2.2和2.3版本最终bulkload使用的jar包名称不一样。<br>这里简单地机翻一下：</p><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h3><p>HBase包括几种将数据加载到表中的方法。 最直接的方法是使用MapReduce任务中的TableOutputFormat类，或使用常规客户端API。 但是，这些方法并不总是最有效的方法。<br>批量加载功能使用MapReduce任务以HBase的内部数据格式输出表数据，然后将生成的StoreFiles直接加载到正在运行的集群中。 与通过HBase API进行加载相比，使用批量加载将占用更少的CPU和网络资源。</p><h3 id="2-批量加载架构"><a href="#2-批量加载架构" class="headerlink" title="2 批量加载架构"></a>2 批量加载架构</h3><p>HBase批量加载过程包括两个主要步骤。</p><h4 id="1-通过MapReduce作业准备数据"><a href="#1-通过MapReduce作业准备数据" class="headerlink" title="1. 通过MapReduce作业准备数据"></a>1. 通过MapReduce作业准备数据</h4><p>批量加载的第一步是使用HFileOutputFormat2从MapReduce任务生成HBase数据文件(存储文件)。这种输出格式以HBase的内部存储格式写出数据，以便稍后可以有效地将它们加载到集群中。<br>为了有效地运行，必须配置HFileOutputFormat2，使每个输出的HFile都适用于单个region。为此，那些 输出结果将被批量加载到HBase中 的MapReduce任务，使用 Hadoop 的 TotalOrderPartitioner 类将map输出划分为 key 空间的不相交范围，该范围与表中 region 的 key 范围相对应。</p><p>HFileOutputFormat2包括一个便捷函数configureIncrementalLoad()，该函数根据表的当前区域边界自动设置TotalOrderPartitioner。</p><h4 id="2-完成数据加载"><a href="#2-完成数据加载" class="headerlink" title="2. 完成数据加载"></a>2. 完成数据加载</h4><p>在准备好要导入的数据之后，通过使用具有 “importtsv.bulk.output” 选项的 importtsv 工具，或通过使用 HFileOutputFormat 的 MapReduce 作业，可以使用 completebulkload 工具将数据导入到正在运行的集群中。 此命令行工具遍历准备好的数据文件，并为每个文件确定文件所属的region。 然后，它将连接上 HFile 对应的合适的 RegionServer ，将其移入其存储目录并将数据提供给客户端。</p><p>如果region边界在批量加载准备过程中或在准备和完成步骤之间发生了更改，则completebulkload程序将自动将数据文件分割为与新边界对应的部分。这个过程效率不高，所以用户应该尽量减少准备批量加载和将其导入集群之间的延迟，特别是当其他客户机通过其他方式同时加载数据时。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">hadoop jar hbase-mapreduce-VERSION.jar completebulkload [-c /path/to/hbase/config/hbase-site.xml] /user/todd/myoutput mytable</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果类路径中没有提供相应的hbase参数，那么-c config-file选项可以用来指定包含相应hbase参数的文件(例如hbase-site.xml)(另外，如果zookeeper不是由hbase管理的，那么类路径必须包含包含zookeeper配置文件的目录)。</p><h2 id="二-hive-操作步骤"><a href="#二-hive-操作步骤" class="headerlink" title="二 hive 操作步骤"></a>二 hive 操作步骤</h2><p>这里主要参考hive的文档：<a href="https://cwiki.apache.org/confluence/display/Hive/HBaseBulkLoad">https://cwiki.apache.org/confluence/display/Hive/HBaseBulkLoad</a></p><h3 id="1-确定目标hbase表的结构"><a href="#1-确定目标hbase表的结构" class="headerlink" title="1. 确定目标hbase表的结构"></a>1. 确定目标hbase表的结构</h3><p>这里需要明确最终导入的hbase表的几个限制条件：</p><ol><li>目标表必须是新建的（即不能导入到已经存在的表）</li><li>目标表只能有一个列族</li><li>目标表不能是稀疏的（即每一行数据的结构必须一致）</li></ol><h3 id="2-添加必要的jar包"><a href="#2-添加必要的jar包" class="headerlink" title="2. 添加必要的jar包"></a>2. 添加必要的jar包</h3><blockquote><p>实测这一步可跳过，不如自己在hive shell上 add jar 。个人感觉hive.aux.jars.path 这个配置比较鸡肋。又不支持直接指定文件夹，发现少了jar包后还得再去编辑文件，需要的jar包一多xml看起来就很恶心。</p></blockquote><ol><li>将hive操作所需的jar包上传到hdfs<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfs -put /usr/lib/hive/lib/hbase-VERSION.jar /user/hive/hbase-VERSION.jar</span><br><span class="line">hadoop dfs -put /usr/lib/hive/lib/hive-hbase-handler-VERSION.jar /user/hive/hive-hbase-handler-VERSION.jar</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>将其路径添加进 hive-site.xml<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.aux.jars.path&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/user/hive/hbase-VERSION.jar,/user/hive/hive-hbase-handler-VERSION.jar&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h3 id="3-确定分区范围"><a href="#3-确定分区范围" class="headerlink" title="3. 确定分区范围"></a>3. 确定分区范围</h3><p>为了最后bulk load的时候可以使用多个reducer加载到多个region，我们需要根据rowkey预先划分好数据的分区范围。这个范围将以文件的形式提供给下一步的中间hive表。这一步的目的就是要得到这个分区文件。</p><h4 id="1-创建分区信息表"><a href="#1-创建分区信息表" class="headerlink" title="1. 创建分区信息表"></a>1. 创建分区信息表</h4><p>分区文件以 org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat outputformat格式存在于hdfs上。我们可以创建一个分区信息表，指定好该表的存储位置，这样该位置上的文件即为我们要的分区信息文件。<br>分区信息表的创建如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create external table hb_range_keys(transaction_id_range_start string)</span><br><span class="line">row format serde</span><br><span class="line">&#x27;org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe&#x27;</span><br><span class="line">stored as</span><br><span class="line">inputformat</span><br><span class="line">&#x27;org.apache.hadoop.mapred.TextInputFormat&#x27;</span><br><span class="line">outputformat</span><br><span class="line">&#x27;org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat&#x27;</span><br><span class="line">location &#x27;/tmp/hb_range_keys&#x27;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2-插入分区边界"><a href="#2-插入分区边界" class="headerlink" title="2. 插入分区边界"></a>2. 插入分区边界</h4><p>这一步需要插入rowkey的边界。例如，插入数据为 005，则最终生成的hbase的region有两个：下边界<del>005、005</del>上边界。<br>边界的选择很重要，良好的预分区设计可以减少hbase再平衡的压力，在大数据量下很有用。<br>因为我们的源表已经存在于hive中了，而目标hbase表的rowkey又是符合字典排序的。所以我们完全可以利用hive的抽样功能先生成hbase的rowkey序列并排好序，再以一定的间隔进行取样，这样就可以得到分布比较合理的边界值了。<br>抽样的实现参考如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">-- 添加必要的 jar 包 </span><br><span class="line"></span><br><span class="line">add jar lib/hive-contrib-0.7.0.jar;</span><br><span class="line"></span><br><span class="line">--设置reduce任务数为1</span><br><span class="line"></span><br><span class="line">set mapred.reduce.tasks=1;</span><br><span class="line"></span><br><span class="line">--创建临时函数 row_sequence，用于让结果带上排序编号</span><br><span class="line"></span><br><span class="line">create temporary function row_sequence as</span><br><span class="line">&#x27;org.apache.hadoop.hive.contrib.udf.UDFRowSequence&#x27;;</span><br><span class="line"></span><br><span class="line">--插入抽样结果到 分区信息表</span><br><span class="line"></span><br><span class="line">insert overwrite table hb_range_keys -- 步骤（3）</span><br><span class="line">    select transaction_id from   -- 步骤（2）</span><br><span class="line">        (select transaction_id     -- 步骤（1）</span><br><span class="line">            from transactions</span><br><span class="line">        tablesample(bucket 1 out of 10000 on transaction_id) s</span><br><span class="line">        order by transaction_id</span><br><span class="line">        limit 10000000</span><br><span class="line">        ) x</span><br><span class="line">    where (row_sequence() % 910000)=0</span><br><span class="line">    order by transaction_id</span><br><span class="line">    limit 11;</span><br><span class="line">    </span><br><span class="line">-- region数量=count(1)+1</span><br><span class="line"> </span><br><span class="line">select count(1) from hb_range_keys ;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这里 transactions 是源表，transaction_id 是导入到 hbase 中的 rowkey （实际使用可能需要生成，这里暂用 transaction_id 字段代替）<br>步骤（1）：根据transaction_id字段将数据分到 1万 个桶里，取里面的第一个桶。理论上约等于按 万分之一 进行抽样，抽样后的结果限制最多为 1千万 条。<br>步骤（2）：对步骤（1）的结果按照 rowkey 排序，每 91万 条 取 一条 作为最终结果，最多有 11 条（1000/91&lt;=11）。<br>步骤（3）：将步骤（2）的结果插入到分区信息表</p><p>综上，假设源表的数据量在1千亿以上，经过两次抽样后最终最多被划分成了12个region。</p><blockquote><p>注意：桶抽样得到的数量不可靠，在极端情况下步骤（1）抽样的结果可能较少，所以需要自己确定分区信息表的记录行数，后面会用到。</p></blockquote><h4 id="3-得到分区文件"><a href="#3-得到分区文件" class="headerlink" title="3. 得到分区文件"></a>3. 得到分区文件</h4><p>在hive shell下执行改命令，即可得到分区文件 /tmp/hb_range_key_list</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dfs -cp /tmp/hb_range_keys/* /tmp/hb_range_key_list;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4-生成中间hive表"><a href="#4-生成中间hive表" class="headerlink" title="4. 生成中间hive表"></a>4. 生成中间hive表</h3><p>该表用于生成符合 bulk load 格式要求的hfile文件。最终该表的数据都将移动到hbase中。</p><h4 id="1-准备中间表存储位置"><a href="#1-准备中间表存储位置" class="headerlink" title="1. 准备中间表存储位置"></a>1. 准备中间表存储位置</h4><p>因为中间表的占用的空间一般要比hbase的还大（数据是相同的，但hbase一般会压缩得比较好）<br>所以中间表的存储位置要特别注意。<br>如果存储位置不存在，hive会自动创建。如果存储位置已存在，需确保其内容为空。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dfs -rmr /tmp/hbsort;</span><br><span class="line">dfs -mkdir /tmp/hbsort;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2-创建中间表"><a href="#2-创建中间表" class="headerlink" title="2. 创建中间表"></a>2. 创建中间表</h4><p>mapred.reduce.tasks=region数量=分区信息表记录数+1<br>total.order.partitioner.path即为分区文件路径<br>注意：hfile.family.path=中间表存储路径/列族名</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">set hive.execution.engine=mr;</span><br><span class="line">set mapred.reduce.tasks=12;</span><br><span class="line">set hive.mapred.partitioner=org.apache.hadoop.mapred.lib.TotalOrderPartitioner;</span><br><span class="line">set total.order.partitioner.path=/tmp/hb_range_key_list;</span><br><span class="line">set hfile.compression=gz;</span><br><span class="line"> </span><br><span class="line">create table hbsort(transaction_id string, user_name string, amount double, ...)</span><br><span class="line">stored as</span><br><span class="line">INPUTFORMAT &#x27;org.apache.hadoop.mapred.TextInputFormat&#x27;</span><br><span class="line">OUTPUTFORMAT &#x27;org.apache.hadoop.hive.hbase.HiveHFileOutputFormat&#x27;</span><br><span class="line">TBLPROPERTIES (&#x27;hfile.family.path&#x27; = &#x27;/tmp/hbsort/cf&#x27;);</span><br><span class="line"> </span><br></pre></td></tr></table></figure><h4 id="3-导入数据"><a href="#3-导入数据" class="headerlink" title="3. 导入数据"></a>3. 导入数据</h4><p>注意这里使用 cluster by 后加 排序字段（rowkey）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> hbsort</span><br><span class="line"><span class="keyword">select</span> transaction_id, user_name, amount, ...</span><br><span class="line"><span class="keyword">from</span> transactions</span><br><span class="line">cluster <span class="keyword">by</span> transaction_id;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="5-完成bulk-load"><a href="#5-完成bulk-load" class="headerlink" title="5. 完成bulk load"></a>5. 完成bulk load</h3><p>这一步已经和hive没什么关系了，以上面 bulk load 为准。hive的文档有些过时了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hbase-VERSION.jar completebulkload [-c /path/to/hbase/config/hbase-site.xml] /tmp/hbsort transactions</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="三-实验"><a href="#三-实验" class="headerlink" title="三 实验"></a>三 实验</h2><h3 id="0-测试环境及数据源准备"><a href="#0-测试环境及数据源准备" class="headerlink" title="0 测试环境及数据源准备"></a>0 测试环境及数据源准备</h3><h4 id="1-环境"><a href="#1-环境" class="headerlink" title="1. 环境"></a>1. 环境</h4><p>实验环境为：CDH6.3.2 parcel安装版本，对应的各个组件版本为：hadoop3.0.0，hbase2.1.0，hive2.1.1</p><p>注意，不同方式安装的组件位置不同，jar包位置也不一样<br>parcels 安装的 默认在 /opt/cloudera/parcels/CDH/lib/组件名<br>package 安装的 默认在 /usr/lib/组件名<br>开源hadoop的就随意了<br>下面的以 parcels 为准</p><h4 id="2-权限相关"><a href="#2-权限相关" class="headerlink" title="2. 权限相关"></a>2. 权限相关</h4><p>请自行解决。<br>我这里直接把dfs的权限认证给关闭了。生产环境我用的是kerberos，主要需要确保使用的用户对hive、hbase目录有相应的读写权限。</p><h4 id="3-实验用例"><a href="#3-实验用例" class="headerlink" title="3. 实验用例"></a>3. 实验用例</h4><p>参考 hdp的文档： <a href="https://docs.cloudera.com/HDPDocuments/HDP1/HDP-1.3.10/bk_user-guide/content/user-guide-hbase-import-1.html">https://docs.cloudera.com/HDPDocuments/HDP1/HDP-1.3.10/bk_user-guide/content/user-guide-hbase-import-1.html</a><br>我下载的数据是：<br><a href="https://dumps.wikimedia.org/other/pagecounts-raw/2008/2008-08/pagecounts-20080801-010000.gz">https://dumps.wikimedia.org/other/pagecounts-raw/2008/2008-08/pagecounts-20080801-010000.gz</a></p><h3 id="1-创建源表"><a href="#1-创建源表" class="headerlink" title="1 创建源表"></a>1 创建源表</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载测试数据</span></span><br><span class="line">wget https://dumps.wikimedia.org/other/pagecounts-raw/2008/2008-08/pagecounts-20080801-010000.gz</span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压</span></span><br><span class="line">gzip pagecounts-20080801-010000.gz -d </span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建源表存储路径</span></span><br><span class="line">hadoop fs -mkdir /tmp/wikistats</span><br><span class="line"><span class="meta">#</span><span class="bash"> 上传测试数据</span></span><br><span class="line">hadoop fs -put pagecounts-20080801-010000 /tmp/wikistats/</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>进入 hive shell 创建表<br>这里我们创建了 pagecounts 表作为源表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> pagecounts (projectcode STRING, pagename STRING, pageviews STRING, bytes STRING)</span><br><span class="line"><span class="type">ROW</span> FORMAT</span><br><span class="line">  DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27; &#x27;</span></span><br><span class="line">  LINES TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">STORED <span class="keyword">AS</span> TEXTFILE</span><br><span class="line">LOCATION <span class="string">&#x27;/tmp/wikistats&#x27;</span>;</span><br></pre></td></tr></table></figure><p>我们可以select一下看一下数据的样子<br><img src="https://img-blog.csdnimg.cn/20201120005936525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>最好再count一下看一下数据量大小，这里可以看到是 3039029<br><img src="https://img-blog.csdnimg.cn/20201120010036509.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="2-创建rowkey视图"><a href="#2-创建rowkey视图" class="headerlink" title="2 创建rowkey视图"></a>2 创建rowkey视图</h3><p>创建视图，用于获取自定义的rowkey，以后对源表的操作的转为对此视图的操作<br>注意必须保证生成rowkey没有重复，不然在最后一步写hfile的时候会报错：Added a key not lexically larger than previous。hbase的api写rowkey重复倒没什么问题，相当于覆盖，但这里是直接写hfile，必须保证rowkey不重复。<br>实测这里给的rowkey生成方法是会导致重复的。</p><p>hql如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> pgc (rowkey, pageviews, bytes) <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> concat_ws(<span class="string">&#x27;/&#x27;</span>,</span><br><span class="line">         projectcode,</span><br><span class="line">         concat_ws(<span class="string">&#x27;/&#x27;</span>,</span><br><span class="line">           pagename,</span><br><span class="line">           regexp_extract(INPUT__FILE__NAME, <span class="string">&#x27;pagecounts-(\\d&#123;8&#125;-\\d&#123;6&#125;)\\..*$&#x27;</span>, <span class="number">1</span>))),</span><br><span class="line">       pageviews, bytes</span><br><span class="line"><span class="keyword">FROM</span> pagecounts;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020112001012265.png#pic_center" alt="在这里插入图片描述"><br>检测重复：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> rowkey,<span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> pgc <span class="keyword">group</span> <span class="keyword">by</span> rowkey <span class="keyword">having</span> <span class="built_in">count</span>(<span class="number">1</span>)<span class="operator">&gt;</span><span class="number">1</span>;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201123011902531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>既然如此，我们还是直接创建一个rowkey视图表出来，然后把重复的和源表都删掉吧。</p><blockquote><p>注意hive的distinct是无法识别字段级的重复的，而group by也没办法用在这里，所以我用的是 not in。</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建 pgct表 取代 源表pagecounts和视图pgc</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> pgct   </span><br><span class="line"><span class="type">ROW</span> FORMAT</span><br><span class="line">  DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27; &#x27;</span></span><br><span class="line">  LINES TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">STORED <span class="keyword">AS</span> TEXTFILE</span><br><span class="line">LOCATION <span class="string">&#x27;/tmp/wikistats_pgct&#x27;</span></span><br><span class="line"> <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> pgc <span class="keyword">where</span> rowkey <span class="keyword">not</span> <span class="keyword">in</span></span><br><span class="line">( <span class="keyword">select</span> rowkey <span class="keyword">from</span> pgc <span class="keyword">group</span> <span class="keyword">by</span> rowkey <span class="keyword">having</span> <span class="built_in">count</span>(<span class="number">1</span>)<span class="operator">&gt;</span><span class="number">1</span>)</span><br><span class="line">;</span><br><span class="line"><span class="comment">-- 查看内容格式是否正确</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> pgct limit <span class="number">1</span>;</span><br><span class="line"><span class="comment">-- 再次确认是否有重复的rowkey</span></span><br><span class="line"><span class="keyword">select</span> rowkey,<span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> pgct <span class="keyword">group</span> <span class="keyword">by</span> rowkey <span class="keyword">having</span> <span class="built_in">count</span>(<span class="number">1</span>)<span class="operator">&gt;</span><span class="number">1</span>;</span><br><span class="line"><span class="comment">-- 删除源表</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> pagecounts;</span><br><span class="line"><span class="comment">-- 删除视图</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> pgc;</span><br></pre></td></tr></table></figure><p>如果视图pgc没有重复的rowkey就不用那么麻烦再生成pgct表了</p><h3 id="3-创建分区信息表"><a href="#3-创建分区信息表" class="headerlink" title="3 创建分区信息表"></a>3 创建分区信息表</h3><p>创建的分区信息表名为：hbase_splits</p><blockquote><p>原教程的列名为 partition ，会导致异常，我这里改为了 key。异常信息为：FAILED: ParseException line 1:49 cannot recognize input near ‘partition’ ‘STRING’ ‘)’ in column name or primary key or foreign key</p></blockquote><blockquote><p>hbase_splits_tmp的作用：<br>用于获取分区信息的行数<br>其他的教程都没有写到需要count获取分区信息表的行数，但如果没获取就没办法正确设置下一步的mapred.reduce.tasks。<br>hbase_splits表由于存储格式和inputformat的关系，存在一些bug，没办法select出内容，甚至连count都不行。所以我这里用了hbase_splits_tmp进行记录，内容先写到hbase_splits_tmp，count完再把hbase_splits_tmp的内容写到hbase_splits上。</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> hbase_splits(key STRING) </span><br><span class="line"><span class="type">ROW</span> FORMAT</span><br><span class="line">  SERDE <span class="string">&#x27;org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe&#x27;</span></span><br><span class="line">STORED <span class="keyword">AS</span></span><br><span class="line"> INPUTFORMAT <span class="string">&#x27;org.apache.hadoop.mapred.TextInputFormat&#x27;</span></span><br><span class="line"> OUTPUTFORMAT <span class="string">&#x27;org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat&#x27;</span></span><br><span class="line">LOCATION <span class="string">&#x27;/tmp/hbase_splits_out&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> hbase_splits_tmp(key STRING) </span><br><span class="line"><span class="type">ROW</span> FORMAT</span><br><span class="line">  DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27; &#x27;</span></span><br><span class="line">  LINES TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">STORED <span class="keyword">AS</span> TEXTFILE</span><br><span class="line">LOCATION <span class="string">&#x27;/tmp/hbase_splits_tmp&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4-获取分区信息"><a href="#4-获取分区信息" class="headerlink" title="4 获取分区信息"></a>4 获取分区信息</h3><p>我们源表的数据在300万条以上（教程里的数据是在400w条左右，这里为了偷懒，就不改了）<br>这里设计为每1w条抽出一条，得到400条以内的结果，再每100条取1，最终不超过4条rowkey作为边界。</p><p>这里为了使用 row_seq 需要添加 hive-contrib 包</p><p>这里是对视图进行操作，视图里已经规定了rowkey的设计方法了<br>最后，要根据存储位置把文件cp成一个文件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ADD</span> JAR <span class="operator">/</span>opt<span class="operator">/</span>cloudera<span class="operator">/</span>parcels<span class="operator">/</span>CDH<span class="operator">/</span>lib<span class="operator">/</span>hive<span class="operator">/</span>contrib<span class="operator">/</span>hive<span class="operator">-</span>contrib<span class="number">-2.1</span><span class="number">.1</span><span class="operator">-</span>cdh6<span class="number">.3</span><span class="number">.2</span>.jar;</span><br><span class="line"><span class="keyword">SET</span> mapred.reduce.tasks<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">FUNCTION</span> row_seq <span class="keyword">AS</span> <span class="string">&#x27;org.apache.hadoop.hive.contrib.udf.UDFRowSequence&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 获取分区信息</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> hbase_splits_tmp</span><br><span class="line"><span class="keyword">SELECT</span> rowkey <span class="keyword">FROM</span></span><br><span class="line">  (<span class="keyword">SELECT</span> rowkey, row_seq() <span class="keyword">AS</span> seq <span class="keyword">FROM</span> pgct</span><br><span class="line">   <span class="keyword">TABLESAMPLE</span>(BUCKET <span class="number">1</span> <span class="keyword">OUT</span> <span class="keyword">OF</span> <span class="number">10000</span> <span class="keyword">ON</span> rowkey) s</span><br><span class="line">   <span class="keyword">ORDER</span> <span class="keyword">BY</span> rowkey</span><br><span class="line">   LIMIT <span class="number">400</span>) x</span><br><span class="line"><span class="keyword">WHERE</span> (seq <span class="operator">%</span> <span class="number">100</span>) <span class="operator">=</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> rowkey</span><br><span class="line">LIMIT <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看内容 及 获取count数</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hbase_splits_tmp;</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> hbase_splits_tmp;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 内容写到hbase_splits_tmp</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> hbase_splits_tmp</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> hbase_splits_tmp;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 删除临时工具表</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> hbase_splits_tmp;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 得到分区信息文件</span></span><br><span class="line">dfs <span class="operator">-</span>cp <span class="operator">/</span>tmp<span class="operator">/</span>hbase_splits_out<span class="comment">/* /tmp/hbase_splits;</span></span><br><span class="line"><span class="comment"></span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201123142921465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="5-创建hfile表并插入数据"><a href="#5-创建hfile表并插入数据" class="headerlink" title="5 创建hfile表并插入数据"></a>5 创建hfile表并插入数据</h3><h4 id="1-创建hfile表"><a href="#1-创建hfile表" class="headerlink" title="1 创建hfile表"></a>1 创建hfile表</h4><p>这个表用来生成hfile，这里我们使用的列族为 w</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE hbase_hfiles(rowkey STRING, pageviews STRING, bytes STRING)</span><br><span class="line">STORED AS</span><br><span class="line">  INPUTFORMAT &#x27;org.apache.hadoop.mapred.TextInputFormat&#x27;</span><br><span class="line">  OUTPUTFORMAT &#x27;org.apache.hadoop.hive.hbase.HiveHFileOutputFormat&#x27;</span><br><span class="line">TBLPROPERTIES(&#x27;hfile.family.path&#x27; = &#x27;/tmp/hbase_hfiles/w&#x27;);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2-插入数据"><a href="#2-插入数据" class="headerlink" title="2 插入数据"></a>2 插入数据</h4><p>total.order.partitioner.path 用于指定使用的分区排序文件的位置，已经过时，使用 mapreduce.totalorderpartitioner.path 代替。<br>注意，我这里把hive 的lib下所有带有hbase的包都添加进去了。理论上应该是完整的了，不过实测还是会提示缺少类：.ClassNotFoundException: org.apache.hadoop.hbase.shaded.protobuf.generated.HFileProtos$FileInfoProto。<br>这个类是在hbase-protocol-shaded.jar包里的，需要到hbase目录里才能找到，也给添加进去，就可以了。<br>mapred.reduce.tasks数目是前方预估的regions数。</p><p>最后一步select出来的数据必须保证无重复的rowkey，实测加distinct 也没用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-client.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-common.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-hadoop2-compat.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-hadoop-compat.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-http.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-mapreduce.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-metrics-api.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-metrics.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-procedure.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-protocol.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-replication.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-server.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-shaded-miscellaneous.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-shaded-netty.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-shaded-protobuf.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hbase-zookeeper.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/tephra-hbase-compat-1.0-0.6.0.jar;</span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hbase-handler.jar;</span><br><span class="line"></span><br><span class="line">ADD JAR /opt/cloudera/parcels/CDH/lib/hbase/hbase-protocol-shaded.jar;</span><br><span class="line"></span><br><span class="line">SET mapred.reduce.tasks=3;</span><br><span class="line">SET hive.mapred.partitioner=org.apache.hadoop.mapred.lib.TotalOrderPartitioner;</span><br><span class="line">SET mapreduce.totalorderpartitioner.path=/tmp/hbase_splits;</span><br><span class="line"></span><br><span class="line">-- generate hfiles using the splits ranges</span><br><span class="line">INSERT OVERWRITE TABLE hbase_hfiles</span><br><span class="line">SELECT distinct rowkey, pageviews, bytes FROM pgct</span><br><span class="line">CLUSTER BY rowkey;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6-完成bulk-load"><a href="#6-完成bulk-load" class="headerlink" title="6 完成bulk load"></a>6 完成bulk load</h3><h4 id="1-创建目标hbase表"><a href="#1-创建目标hbase表" class="headerlink" title="1. 创建目标hbase表"></a>1. 创建目标hbase表</h4><p>在hbase shell下执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create &#x27;page_count&#x27;, &#123;  NAME =&gt;&#x27;w&#x27;,COMPRESSION =&gt; &#x27;SNAPPY&#x27; &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201123142510273.png#pic_center" alt="在这里插入图片描述"></p><h4 id="2-bulkload"><a href="#2-bulkload" class="headerlink" title="2. bulkload"></a>2. bulkload</h4><p>bulkload有两种写法，这里推荐使用下面的第二种，不需要设置classpath，如果是使用 hadoop jar需要将 hbase classpath 添加到 hadoop classpath</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hadoop jar hbase-mapreduce.jar completebulkload /tmp/hbase_hfiles page_count</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /tmp/hbase_hfiles page_count</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201123142428299.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="7-总结"><a href="#7-总结" class="headerlink" title="7 总结"></a>7 总结</h3><p>不同的版本会有不同的问题。我自己使用的官方开源版本的hive获取分区信息的时候可以直接count，这里用的比较老，似乎就有问题。<br>另外需要注意可能会各种缺少class，所以建议直接add所有hbase相关的包。</p><h2 id="四-参考文章"><a href="#四-参考文章" class="headerlink" title="四 参考文章"></a>四 参考文章</h2><ul><li>hbase官方教程：<br><a href="http://hbase.apache.org/2.2/book.html#arch.bulk.load">http://hbase.apache.org/2.2/book.html#arch.bulk.load</a></li><li>hive 官方教程：<br><a href="https://cwiki.apache.org/confluence/display/Hive/HBaseBulkLoad">https://cwiki.apache.org/confluence/display/Hive/HBaseBulkLoad</a></li><li>hortonworks教程：<br><a href="https://docs.cloudera.com/HDPDocuments/HDP1/HDP-1.3.10/bk_user-guide/content/user-guide-hbase-import-1.html">https://docs.cloudera.com/HDPDocuments/HDP1/HDP-1.3.10/bk_user-guide/content/user-guide-hbase-import-1.html</a></li><li>有赞HBase Bulkload 实践探讨：<a href="https://tech.youzan.com/hbase-bulkloadshi-practice/">https://tech.youzan.com/hbase-bulkloadshi-practice/</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
            <tag> hive </tag>
            
            <tag> bulkload </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive编写udf实践记录</title>
      <link href="//hive_udf/"/>
      <url>//hive_udf/</url>
      
        <content type="html"><![CDATA[<p>官方教程：<a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a><br>简单使用查看上面官方的文档即可。这里记录一下我使用的实践和一点注意事项。</p><span id="more"></span><h2 id="一-编写udf"><a href="#一-编写udf" class="headerlink" title="一 编写udf"></a>一 编写udf</h2><p>这里的需求是写一个udf，用于将经纬度转换成geohash。参数有 经纬度和geohash的精度。</p><h3 id="gradle配置"><a href="#gradle配置" class="headerlink" title="gradle配置"></a>gradle配置</h3><p>gradle 部分配置如下：</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">dependencies</span> &#123;</span><br><span class="line">    <span class="keyword">compile</span> <span class="keyword">group</span>: <span class="string">&#x27;ch.hsr&#x27;</span>, name: <span class="string">&#x27;geohash&#x27;</span>, version: <span class="string">&#x27;1.3.0&#x27;</span></span><br><span class="line">    compileOnly <span class="keyword">group</span>: <span class="string">&#x27;org.apache.hive&#x27;</span>, name: <span class="string">&#x27;hive-exec&#x27;</span>, version: <span class="string">&#x27;2.3.6&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">task</span> fatJar(type: Jar) &#123;</span><br><span class="line">    <span class="comment">//baseName = project.name</span></span><br><span class="line">    baseName = <span class="string">&#x27;hiveFunction&#x27;</span></span><br><span class="line">    <span class="keyword">project</span>.<span class="keyword">fileTree</span>(<span class="string">&quot;$buildDir/fatjar/libs&quot;</span>).forEach &#123; jarFile -&gt;</span><br><span class="line">        <span class="keyword">from</span> zipTree(jarFile )</span><br><span class="line">    &#125;</span><br><span class="line">    with jar</span><br><span class="line">    <span class="keyword">destinationDir</span> = <span class="keyword">file</span>(<span class="string">&quot;$buildDir/fatjar&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">task</span> copyToLib(type: <span class="keyword">Copy</span>) &#123;</span><br><span class="line">    <span class="keyword">into</span> <span class="string">&quot;$buildDir/fatjar/libs&quot;</span></span><br><span class="line">    <span class="keyword">from</span> <span class="keyword">configurations</span>.<span class="keyword">runtime</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="UDF类"><a href="#UDF类" class="headerlink" title="UDF类"></a>UDF类</h3><p>GeoHashUDF.java 代码如下：<br>这里需要注意的就是注释的写法，以及异常的处理。<br>因为在hive执行的过程中，如果udf抛一个异常出来，有可能会导致整个hive sql执行的失败。所以，对于业务异常，应该避免向外抛出。<br>以本geohash的udf为例，如果给定的经纬度不合法，则可返回用0填充的默认字符串。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ch.hsr.geohash.GeoHash;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Description(</span></span><br><span class="line"><span class="meta">        name = &quot;geohash&quot;,</span></span><br><span class="line"><span class="meta">        value = &quot;_FUNC_(double lat,double lon,int precison) - Returns geohash&quot;,</span></span><br><span class="line"><span class="meta">        extended = &quot;Example:\n  &gt; SELECT _FUNC_(\&#x27;20.1,111.2,6\&#x27;) FROM src LIMIT 1;\n  \&#x27;hello:Facebook\&#x27;&quot;</span></span><br><span class="line"><span class="meta">)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GeoHashUDF</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Map&lt;Integer,String&gt; defaultValueMap = <span class="keyword">new</span> HashMap&lt;&gt; ();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        StringBuilder sb = <span class="keyword">new</span> StringBuilder(<span class="string">&quot;&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">20</span>; i++)&#123;</span><br><span class="line">            defaultValueMap.put(i,sb.toString());</span><br><span class="line">            sb.append(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span> <span class="params">(Double lat,Double lon,Integer precision)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> GeoHash.geoHashStringWithCharacterPrecision(lat, lon, precision);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">return</span> defaultValueMap.get(precision);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="keyword">new</span> GeoHashUDF().evaluate(<span class="number">20.1</span>,<span class="number">111.2</span>,<span class="number">6</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="打jar包"><a href="#打jar包" class="headerlink" title="打jar包"></a>打jar包</h3><p>先执行gradle copyToLib，再执行 gradle  fatJar<br>得到 hiveFunction.jar</p><h2 id="二-创建function"><a href="#二-创建function" class="headerlink" title="二 创建function"></a>二 创建function</h2><h3 id="1-创建临时function"><a href="#1-创建临时function" class="headerlink" title="1 创建临时function"></a>1 创建临时function</h3><ol><li>先将生成的jar包添加到hive的lib目录下<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\cp -f hiveFunction.jar $HIVE_HOME/lib/</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>再在hive shell里面执行以下命令即可，注意要使用全称类名<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create temporary function geohash as &#x27;GeoHashUDF&#x27;;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>删除<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop temporary function if exists geohash;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-创建永久function"><a href="#2-创建永久function" class="headerlink" title="2 创建永久function"></a>2 创建永久function</h3></li><li>上传jar包到hdfs上<figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm /my/hive/libs/hiveFunction.jar</span><br><span class="line">hadoop fs -put hiveFunction.jar /my/hive/libs/hiveFunction.jar</span><br></pre></td></tr></table></figure></li><li>创建永久函数<figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create function geohash as &#x27;com.yaoyun.anyonedev.hive.function.GeoHashUDF&#x27; using jar &#x27;hdfs:///my/hive/libs/hiveFunction.jar&#x27;;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>删除<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop function if exists geohash;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH客户端环境搭建</title>
      <link href="//cdh_client_env_deploy/"/>
      <url>//cdh_client_env_deploy/</url>
      
        <content type="html"><![CDATA[<p>最近遇到一个需求：要使用azkaban对接客户的CDH集群，CDH用的是oozie，azkaban只能部署在我们客户端的机器上，所以需要在客户机上手动搭建CDH的hadoop环境。操作很简单，过程比较麻烦，这里记录一下。</p><span id="more"></span><h2 id="一-获取所需CDH-rpm包"><a href="#一-获取所需CDH-rpm包" class="headerlink" title="一 获取所需CDH rpm包"></a>一 获取所需CDH rpm包</h2><h3 id="1-搭建本地CDH-package仓库"><a href="#1-搭建本地CDH-package仓库" class="headerlink" title="1. 搭建本地CDH package仓库"></a>1. 搭建本地CDH package仓库</h3><p>说明：CDH有两种本地仓库配置方式，package和parcel。官方是推荐使用package，我推荐两种都配置好，CDH还是用parcel。package方式可用于获取rpm包。<br><a href="https://docs.cloudera.com/documentation/enterprise/6/latest/topics/cm_ig_create_local_package_repo.html">https://docs.cloudera.com/documentation/enterprise/6/latest/topics/cm_ig_create_local_package_repo.html</a><br>yum包的下载建议使用idm的站点抓取功能，不要用wget下载</p><h3 id="2-使用-yumdownloader-获取rpm包"><a href="#2-使用-yumdownloader-获取rpm包" class="headerlink" title="2. 使用 yumdownloader 获取rpm包"></a>2. 使用 yumdownloader 获取rpm包</h3><p>这一步需要在docker容器环境下进行，这样环境很纯粹，获取到到rpm包更完整，参考：<a href="https://blog.csdn.net/alinyua/article/details/108071365">https://blog.csdn.net/alinyua/article/details/108071365</a></p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> yum install yum-utils -y</span></span><br><span class="line">mkdir cdh-packages &amp;&amp; cd cdh-packages </span><br><span class="line">yumdownloader --resolve  hadoop-client hbase hive-hbase spark-core</span><br><span class="line">cd .. &amp;&amp; tar -zcf cdh-packages.tar.gz cdh-packages</span><br></pre></td></tr></table></figure><h2 id="二-安装CDH-rpm包"><a href="#二-安装CDH-rpm包" class="headerlink" title="二 安装CDH rpm包"></a>二 安装CDH rpm包</h2><p>安装时虽然不会有依赖缺少，但可能由于系统版本关系部分系统自带软件版本不一致，我是参考错误提示直接忽略掉，后面也没有遇到问题。</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf cdh-packages.tar.gz</span><br><span class="line">yum -y localinstall cdh-packages/*.rpm</span><br></pre></td></tr></table></figure><h2 id="三-配置CDH-环境"><a href="#三-配置CDH-环境" class="headerlink" title="三 配置CDH 环境"></a>三 配置CDH 环境</h2><h3 id="1-配置环境变量"><a href="#1-配置环境变量" class="headerlink" title="1. 配置环境变量"></a>1. 配置环境变量</h3><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;export HADOOP_HOME=/usr/lib/hadoop&quot;&gt;&gt; /etc/profile</span><br><span class="line">echo &quot;export HBASE_HOME=/usr/lib/hbase&quot;&gt;&gt; /etc/profile</span><br><span class="line">echo &quot;export HIVE_HOME=/usr/lib/hive&quot;&gt;&gt; /etc/profile</span><br><span class="line">echo &quot;export SPARK_HOME=/usr/lib/spark&quot;&gt;&gt; /etc/profile</span><br><span class="line">echo &quot;export HADOOP_CONF_DIR=/etc/hadoop/conf/&quot;&gt;&gt; /etc/profile</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h3 id="2-添加配置文件"><a href="#2-添加配置文件" class="headerlink" title="2. 添加配置文件"></a>2. 添加配置文件</h3><p>在CDH管理界面下载各组件的客户端配置，添加到对应文件夹下<br>各组件配置文件位置如下：<br>hadoop/yarn: /etc/hadoop/conf/<br>hbase: /etc/hbase/conf<br>hive: /etc/hive/conf</p><h2 id="四-验证"><a href="#四-验证" class="headerlink" title="四 验证"></a>四 验证</h2><p>输入以下命令对各个组件进行验证</p><h3 id="1-hdfs"><a href="#1-hdfs" class="headerlink" title="1. hdfs"></a>1. hdfs</h3><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls /</span><br></pre></td></tr></table></figure><h3 id="2-yarn"><a href="#2-yarn" class="headerlink" title="2. yarn"></a>2. yarn</h3><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn node -list</span><br></pre></td></tr></table></figure><h3 id="3-hbase"><a href="#3-hbase" class="headerlink" title="3. hbase"></a>3. hbase</h3><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase shell</span><br><span class="line"></span><br><span class="line">status</span><br></pre></td></tr></table></figure><h3 id="4-hive"><a href="#4-hive" class="headerlink" title="4. hive"></a>4. hive</h3><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive shell</span><br><span class="line"></span><br><span class="line">show tables;</span><br></pre></td></tr></table></figure><h3 id="5-spark"><a href="#5-spark" class="headerlink" title="5. spark"></a>5. spark</h3><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 能正确进入即可</span></span><br><span class="line">spark-shell</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>如果报错：java.lang.NoSuchMethodError: jline.console.completer.CandidateListCompletionHandler.setPrintSpaceAfterFullCompletion(Z)V<br>根据：<a href="https://issues.apache.org/jira/browse/SPARK-25783">https://issues.apache.org/jira/browse/SPARK-25783</a><br>可下载 jline-2.14.3.jar 放到 $SPARK_HOME/jars 下<br>下载地址：<a href="https://repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar">https://repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH部署笔记</title>
      <link href="//cdh_server_env_deploy/"/>
      <url>//cdh_server_env_deploy/</url>
      
        <content type="html"><![CDATA[<p>本文为个人安装CDH时记录，不具普适性，仅供参考。建议对比官方文档阅读。</p><span id="more"></span><h2 id="一-依赖检查"><a href="#一-依赖检查" class="headerlink" title="一 依赖检查"></a>一 依赖检查</h2><p>以centos7为例</p><h3 id="1-软件"><a href="#1-软件" class="headerlink" title="1. 软件"></a>1. 软件</h3><p><a href="https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_os_requirements.html">https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_os_requirements.html</a></p><h4 id="0-常用软件安装"><a href="#0-常用软件安装" class="headerlink" title="0. 常用软件安装"></a>0. 常用软件安装</h4><p>此步骤非必须</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y bind-utils</span><br></pre></td></tr></table></figure><h4 id="1-python"><a href="#1-python" class="headerlink" title="1. python"></a>1. python</h4><p>使用2.7版本或以上版本，但不支持3<br>centos7 中默认已包含，如果有多版本需设置PYSPARK_PYTHON和<br>PYSPARK_DRIVER_PYTHON环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看版本</span></span><br><span class="line">python --version</span><br></pre></td></tr></table></figure><h4 id="2-Perl"><a href="#2-Perl" class="headerlink" title="2. Perl"></a>2. Perl</h4><p>一般已安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看版本</span></span><br><span class="line">perl -v</span><br></pre></td></tr></table></figure><h4 id="3-python-psycopg2"><a href="#3-python-psycopg2" class="headerlink" title="3. python-psycopg2"></a>3. python-psycopg2</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 要先安装epel-release，，这个包包含了 EPEL 源的 gpg 密钥和软件源信息，该软件包会自动配置yum的软件仓库</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 否则下一步会提示：没有可用软件包 pip-python</span></span><br><span class="line">yum -y install epel-release</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装pip</span></span><br><span class="line">yum -y install python-pip</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用pip 安装 psycopg2</span></span><br><span class="line">pip install psycopg2==2.7.5 --ignore-installed</span><br></pre></td></tr></table></figure><h3 id="2-网络"><a href="#2-网络" class="headerlink" title="2. 网络"></a>2. 网络</h3><h4 id="禁用ipv6"><a href="#禁用ipv6" class="headerlink" title="禁用ipv6"></a>禁用ipv6</h4><p><a href="https://www.jianshu.com/p/225d040d0b66">https://www.jianshu.com/p/225d040d0b66</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv6.conf.all.disable_ipv6=1</span><br><span class="line">sysctl -w net.ipv6.conf.default.disable_ipv6=1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reboot</span><br><span class="line">netstat -lnpt</span><br></pre></td></tr></table></figure><h4 id="配置hostname"><a href="#配置hostname" class="headerlink" title="配置hostname"></a>配置hostname</h4><p><a href="https://docs.cloudera.com/documentation/enterprise/6/latest/topics/configure_network_names.html">https://docs.cloudera.com/documentation/enterprise/6/latest/topics/configure_network_names.html</a><br>用全称域名，如：paas-201.adp.com 而不是 paas-201</p><ol><li><p>配置hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo hostnamectl set-hostname foo-1.example.com</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>编辑/ets/hosts(集群统一)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.1.1.1  foo-1.example.com  foo-1</span><br><span class="line">2.2.2.2  foo-2.example.com  foo-2</span><br><span class="line">3.3.3.3  foo-3.example.com  foo-3</span><br><span class="line">4.4.4.4  foo-4.example.com  foo-4</span><br></pre></td></tr></table></figure></li><li><p>编辑/etc/sysconfig/network</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;HOSTNAME=$HOSTNAME&quot; &gt;&gt;/etc/sysconfig/network</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 保存规则</span></span><br><span class="line">sudo iptables-save &gt; ~/firewall.rules</span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭</span></span><br><span class="line">sudo systemctl disable firewalld</span><br><span class="line">sudo systemctl stop firewalld</span><br></pre></td></tr></table></figure><h3 id="3-关闭SELinux"><a href="#3-关闭SELinux" class="headerlink" title="3. 关闭SELinux"></a>3. 关闭SELinux</h3><p>查看是否已经关闭</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">getenforce</span><br></pre></td></tr></table></figure><p>关闭方法：<br>修改/etc/selinux/config<br>SELINUX=permissive<br>并执行 setenforce 0  立即生效</p><h3 id="4-启用ntp"><a href="#4-启用ntp" class="headerlink" title="4. 启用ntp"></a>4. 启用ntp</h3><p>安装后用以下命令进行验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl status chronyd.service </span><br><span class="line">chronyc sources -v </span><br><span class="line">chronyc sourcestats -v</span><br></pre></td></tr></table></figure><h2 id="二-安装"><a href="#二-安装" class="headerlink" title="二 安装"></a>二 安装</h2><h3 id="一-配置本地仓库"><a href="#一-配置本地仓库" class="headerlink" title="一. 配置本地仓库"></a>一. 配置本地仓库</h3><p><a href="https://docs.cloudera.com/documentation/enterprise/6/latest/topics/cm_ig_create_local_package_repo.html">https://docs.cloudera.com/documentation/enterprise/6/latest/topics/cm_ig_create_local_package_repo.html</a></p><h4 id="1-配置web服务器"><a href="#1-配置web服务器" class="headerlink" title="1. 配置web服务器"></a>1. 配置web服务器</h4></li><li><p>安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install httpd</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>需保证/var/www磁盘空间足够，建议挂载，这里假设 /home/www 有足够的空间</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mv /var/www /var/www2</span><br><span class="line">mkdir /home/www</span><br><span class="line">ln -s /home/www /var/www</span><br><span class="line">mv /var/www2/* /var/www/</span><br><span class="line">rm -rf /var/www2</span><br><span class="line">ls /home/www</span><br></pre></td></tr></table></figure></li><li><p>启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start httpd</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2-下载文件-clouderaManager和cdh）"><a href="#2-下载文件-clouderaManager和cdh）" class="headerlink" title="2. 下载文件(clouderaManager和cdh）"></a>2. 下载文件(clouderaManager和cdh）</h4></li><li><p>clouderaManager<br>下载：<a href="https://archive.cloudera.com/cm6/6.3.1/repo-as-tarball/cm6.3.1-redhat7.tar.gz">https://archive.cloudera.com/cm6/6.3.1/repo-as-tarball/cm6.3.1-redhat7.tar.gz</a></p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /var/www/html/cloudera-repos/cm6</span><br><span class="line">tar xvfz cm6.3.1-redhat7.tar.gz -C /var/www/html/cloudera-repos/cm6 --strip-components=1</span><br><span class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cm6</span><br></pre></td></tr></table></figure><ol start="2"><li>cdh</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /var/www/html/cloudera-repos</span><br><span class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cdh6/6.3.2/redhat7/ -P /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/gplextras6/6.3.2/redhat7/ -P /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cdh6</span><br><span class="line">sudo chmod -R ugo+rX /var/www/html/cloudera-repos/gplextras6</span><br></pre></td></tr></table></figure><p>访问 http://<host>/cloudera-repos/ ，应该可以看到下载的文件</p><h4 id="3-配置使用本地存储库"><a href="#3-配置使用本地存储库" class="headerlink" title="3. 配置使用本地存储库"></a>3. 配置使用本地存储库</h4><p>创建/etc/yum.repos.d/cloudera-repo.repo<br>内容如下：</p><p>[cloudera-repo]<br>name=cloudera-repo<br>baseurl=<a href="http://paas-241/cloudera-repos/cm6">http://paas-241/cloudera-repos/cm6</a><br>enabled=1<br>gpgcheck=0</p><h3 id="三-安装CDH"><a href="#三-安装CDH" class="headerlink" title="三. 安装CDH"></a>三. 安装CDH</h3><h4 id="1-安装java"><a href="#1-安装java" class="headerlink" title="1. 安装java"></a>1. 安装java</h4> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y oracle-j2sdk1.8</span><br></pre></td></tr></table></figure><h4 id="2-安装CM"><a href="#2-安装CM" class="headerlink" title="2. 安装CM"></a>2. 安装CM</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server</span><br></pre></td></tr></table></figure><h4 id="3-安装数据库"><a href="#3-安装数据库" class="headerlink" title="3. 安装数据库"></a>3. 安装数据库</h4><p>假设使用mysql且已安装</p><p>在cm服务器上：</p><h5 id="1-安装jdbc驱动"><a href="#1-安装jdbc驱动" class="headerlink" title="1. 安装jdbc驱动"></a>1. 安装jdbc驱动</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gz</span><br><span class="line"></span><br><span class="line">tar zxvf mysql-connector-java-5.1.46.tar.gz</span><br><span class="line"></span><br><span class="line">sudo mkdir -p /usr/share/java/</span><br><span class="line"></span><br><span class="line">cd mysql-connector-java-5.1.46</span><br><span class="line"></span><br><span class="line">sudo cp mysql-connector-java-5.1.46-bin.jar  /usr/share/java/mysql-connector-java.jar</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2-创建数据库并授权"><a href="#2-创建数据库并授权" class="headerlink" title="2. 创建数据库并授权"></a>2. 创建数据库并授权</h5><p>登录mysql进行配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -h 192.168.100.200 -p</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>执行以下命令：假设数据库为cdh 授权用户为root，密码为anyonedev<br> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE cdh <span class="keyword">DEFAULT</span> <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">DEFAULT</span> <span class="keyword">COLLATE</span> utf8_general_ci;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">ALL</span> <span class="keyword">ON</span> cdh.<span class="operator">*</span> <span class="keyword">TO</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">&#x27;anyonedev&#x27;</span>;</span><br><span class="line"><span class="keyword">SHOW</span> DATABASES;</span><br><span class="line"><span class="keyword">SHOW</span> GRANTS <span class="keyword">FOR</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h5 id="3-执行创建脚本"><a href="#3-执行创建脚本" class="headerlink" title="3. 执行创建脚本"></a>3. 执行创建脚本</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">/opt/cloudera/cm/schema/scm_prepare_database.sh -h 192.168.100.200 mysql cdh root anyonedev</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="四-启动"><a href="#四-启动" class="headerlink" title="四 启动"></a>四 启动</h4> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl enable cloudera-scm-server</span><br><span class="line">sudo systemctl start cloudera-scm-server</span><br><span class="line">sudo tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>直到看到：<br>INFO WebServerImpl:com.cloudera.server.cmf.WebServerImpl: Started Jetty server.<br>则说明启动完成，打开<br><a href="http://paas-241:7180/">http://paas-241:7180</a> 即可 admin/admin</p><p>注意配置自定义仓库：<br><a href="http://192.168.100.241/cloudera-repos/cdh6/6.3.2/redhat7/yum/">http://192.168.100.241/cloudera-repos/cdh6/6.3.2/redhat7/yum/</a><br>方法是数据表</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>oracle_logminer学习和实践笔记</title>
      <link href="//oracle_logminer/"/>
      <url>//oracle_logminer/</url>
      
        <content type="html"><![CDATA[<p>Oracle LogMiner是Oracle公司从产品8i以后提供的一个实际非常有用的分析工具，使用该工具可以轻松获得Oracle 在线/归档日志文件中的具体内容，特别是该工具可以分析出所有对于数据库操作的DML和DDL语句。该工具特别适用于调试、审计或者回退某个特定的事务。</p><span id="more"></span><h2 id="一-搭建环境"><a href="#一-搭建环境" class="headerlink" title="一 搭建环境"></a>一 搭建环境</h2><h3 id="1-使用docker提供oracle运行环境"><a href="#1-使用docker提供oracle运行环境" class="headerlink" title="1 使用docker提供oracle运行环境"></a>1 使用docker提供oracle运行环境</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name my-oracle --restart=always -p 49161:1521 -e ORACLE_ALLOW_REMOTE=true wnameless/oracle-xe-11g-r2</span><br></pre></td></tr></table></figure><p>连接配置如下：</p><blockquote><p>hostname: localhost<br>port: 49161<br>sid: xe<br>username: system  管理员：sys as sysdba<br>password: oracle</p></blockquote><p>以下操作需在sql plus环境下执行</p><blockquote><p>navicat 配置sql plus方法如下：<br>到 <a href="https://www.oracle.com/database/technologies/instant-client/winx64-64-downloads.html">https://www.oracle.com/database/technologies/instant-client/winx64-64-downloads.html</a> 下载对应版本的 Instant Client Package - Basic 基础安装包 和 Instant Client Package - SQL*Plus，解压到同一目录，在navicat - 选项 - 环境 里配置该目录即可</p></blockquote><p>不过还是建议直接到容器里面执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入容器</span></span><br><span class="line">docker exec -it my-oracle bash</span><br><span class="line"><span class="meta">#</span><span class="bash"> 以系统管理员登录sqlplus</span></span><br><span class="line">sqlplus sys/oracle as sysdba</span><br></pre></td></tr></table></figure><p>也可以创建一个系统管理员，如下，其中”CONNECT”, “DBA”, “RESOURCE”是角色，SYSDBA 是服务器权限，不能同时grant</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> &quot;LOGMINER01&quot; IDENTIFIED <span class="keyword">BY</span> &quot;123456&quot;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">GRANT</span> &quot;CONNECT&quot;, &quot;DBA&quot;, &quot;RESOURCE&quot; <span class="keyword">TO</span> &quot;LOGMINER01&quot;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">GRANT</span> SYSDBA  <span class="keyword">TO</span> &quot;LOGMINER01&quot;;</span><br></pre></td></tr></table></figure><h3 id="2-加入功能包"><a href="#2-加入功能包" class="headerlink" title="2 加入功能包"></a>2 加入功能包</h3><p>通常在安装数据库后就已经安装了Logminer，要查看数据库是否安装了LogMiner，只需查看数据库中是否已经有了dbms_logmnr和dbms_logmnr_d这2个package，如果有了，则已经安装，如果没有，执行下面两个脚本即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">@/u01/app/oracle/product/11.2.0/xe/rdbms/admin/dbmslm.sql</span><br><span class="line">@/u01/app/oracle/product/11.2.0/xe/rdbms/admin/dbmslmd.sql</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如果环境不一样，不知道对应的文件位置，建议直接 find / -name ‘dbmslm.sql’<br><img src="https://img-blog.csdnimg.cn/20200917224107575.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="加入功能包"></p><h3 id="3-启用补充日志"><a href="#3-启用补充日志" class="headerlink" title="3 启用补充日志"></a>3 启用补充日志</h3><p>补充日志有几种类型，SUPPLEMENTAL LOG DATA是最小补充日志，这里推荐使用SUPPLEMENTAL LOG DATA (ALL) COLUMNS，当数据行更新时会记录完整的字段数据（而非只有更新的字段数据）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 最小补充日志</span></span><br><span class="line"><span class="comment">-- ALTER DATABASE ADD SUPPLEMENTAL LOG DATA;</span></span><br><span class="line"><span class="keyword">ALTER</span> DATABASE <span class="keyword">ADD</span> SUPPLEMENTAL LOG DATA (<span class="keyword">ALL</span>) COLUMNS;</span><br><span class="line"><span class="keyword">SELECT</span> SUPPLEMENTAL_LOG_DATA_MIN <span class="keyword">FROM</span> V$DATABASE;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200917225120914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="启用补充日志"></p><h3 id="4-开启archivelog-mode"><a href="#4-开启archivelog-mode" class="headerlink" title="4 开启archivelog mode"></a>4 开启archivelog mode</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SHUTDOWN IMMEDIATE</span><br><span class="line">STARTUP MOUNT</span><br><span class="line">ALTER DATABASE ARCHIVELOG;</span><br><span class="line">ALTER DATABASE OPEN;</span><br><span class="line">archive log list;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200917225424282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="开启archivelog mode"></p><h3 id="5-创建日常维护账号"><a href="#5-创建日常维护账号" class="headerlink" title="5 创建日常维护账号"></a>5 创建日常维护账号</h3><p>需要注意，最后的 ALTER system 权限过大，如果不需要使用alter system switch logfile语句的，可以不授权。<br>alter system switch  logfile 的具体说明参考：<a href="https://blog.csdn.net/fred_yang2013/article/details/45171283">https://blog.csdn.net/fred_yang2013/article/details/45171283</a></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建user1用户</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> &quot;USER1&quot; IDENTIFIED <span class="keyword">BY</span> &quot;123456&quot;;</span><br><span class="line"><span class="comment">-- 授予 连接 权限</span></span><br><span class="line"><span class="keyword">GRANT</span> &quot;CONNECT&quot;,&quot;RESOURCE&quot; <span class="keyword">TO</span> &quot;USER1&quot;;</span><br><span class="line"><span class="comment">-- 授予 使用LogMiner PL / SQL程序包 权限</span></span><br><span class="line"><span class="keyword">GRANT</span> EXECUTE_CATALOG_ROLE <span class="keyword">TO</span> &quot;USER1&quot;;</span><br><span class="line"><span class="comment">-- 授予 查询V$ARCHIVED_LOG视图 权限</span></span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span>V_$ARCHIVED_LOG <span class="keyword">TO</span> &quot;USER1&quot;;</span><br><span class="line"><span class="comment">-- 授予 使用DBMS_LOGMNR_D程序 权限（用于生成字典文件）</span></span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">execute</span> <span class="keyword">ON</span> DBMS_LOGMNR_D <span class="keyword">TO</span> &quot;USER1&quot;;</span><br><span class="line"><span class="comment">-- 授予 使用DBMS_LOGMNR程序 权限 （用于生成分析会话）</span></span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">execute</span> <span class="keyword">ON</span> DBMS_LOGMNR <span class="keyword">TO</span> &quot;USER1&quot;;</span><br><span class="line"><span class="comment">-- 授予 查询V$LOGMNR_CONTENTS TO视图 权限（必须拥有SELECT ANY TRANSACTION权限才能查询此视图）</span></span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ANY</span> TRANSACTION <span class="keyword">TO</span> USER1;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span>V_$LOGMNR_CONTENTS <span class="keyword">TO</span> &quot;USER1&quot;;</span><br><span class="line"><span class="comment">-- 授予 执行alter system switch logfile生成日志文件 权限</span></span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">ALTER</span> <span class="keyword">system</span> <span class="keyword">TO</span> &quot;USER1&quot;;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="二-常用命令"><a href="#二-常用命令" class="headerlink" title="二 常用命令"></a>二 常用命令</h2><h3 id="1-查看日志文件"><a href="#1-查看日志文件" class="headerlink" title="1 查看日志文件"></a>1 查看日志文件</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> V$ARCHIVED_LOG ;</span><br></pre></td></tr></table></figure><h3 id="2-创建字典文件"><a href="#2-创建字典文件" class="headerlink" title="2 创建字典文件"></a>2 创建字典文件</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">    DBMS_LOGMNR_D.BUILD(OPTIONS<span class="operator">=</span><span class="operator">&gt;</span> DBMS_LOGMNR_D.STORE_IN_REDO_LOGS);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="3-创建归档日志"><a href="#3-创建归档日志" class="headerlink" title="3 创建归档日志"></a>3 创建归档日志</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">system</span> switch logfile</span><br></pre></td></tr></table></figure><h2 id="三-使用示例"><a href="#三-使用示例" class="headerlink" title="三 使用示例"></a>三 使用示例</h2><pre><code>  0. 先执行归档操作  1. 根据给定范围，获取要分析的文件列表  2. 先生成字典，再根据1，选定最近的字典.字典可能有多文件，需找出最后一个   2.0 生成字典文件   2.1 根据1的文件找出1之前最近的一个字典结束文件   2.2 根据2.1文件找出2.1之前最近的一个字典开始文件  3. 全部加入要分析的文件列表中  4. 执行分析</code></pre><h3 id="0-归档"><a href="#0-归档" class="headerlink" title="0 归档"></a>0 归档</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">system</span> switch logfile</span><br></pre></td></tr></table></figure><h3 id="1-获取要分析的文件列表"><a href="#1-获取要分析的文件列表" class="headerlink" title="1 获取要分析的文件列表"></a>1 获取要分析的文件列表</h3><p>如下，可以根据scn序号范围或者时间范围获取归档文件列表。<br>这里可以限制待会要分析的文件数量，减少服务器工作量。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> recid , stamp , name , sequence# <span class="keyword">as</span> sequence, next_change# <span class="keyword">as</span> nextChange,next_time <span class="keyword">as</span> nextTime,completion_time <span class="keyword">as</span> completionTime ,dictionary_begin <span class="keyword">as</span> dictionaryBegin,dictionary_end <span class="keyword">as</span> dictionaryEnd</span><br><span class="line"><span class="keyword">FROM</span> V$ARCHIVED_LOG </span><br><span class="line"><span class="keyword">WHERE</span> NEXT_CHANGE# <span class="operator">&gt;=</span> $&#123;beginScn?c&#125; <span class="keyword">AND</span></span><br><span class="line">FIRST_CHANGE# <span class="operator">&lt;=</span> $&#123;endScn?c&#125; <span class="keyword">AND</span></span><br><span class="line">NEXT_TIME <span class="operator">&gt;=</span> to_date(<span class="string">&#x27;$&#123;beginTime&#125;&#x27;</span>,<span class="string">&#x27;YYYYMMDDHH24MISS&#x27;</span>) <span class="keyword">AND</span></span><br><span class="line">FIRST_TIME <span class="operator">&lt;=</span> to_date(<span class="string">&#x27;$&#123;endTime&#125;&#x27;</span>,<span class="string">&#x27;YYYYMMDDHH24MISS&#x27;</span>) <span class="keyword">AND</span></span><br><span class="line"><span class="number">1</span> <span class="operator">=</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="2-选择字典文件"><a href="#2-选择字典文件" class="headerlink" title="2 选择字典文件"></a>2 选择字典文件</h3><p>这一步参考：<a href="https://docs.oracle.com/en/database/oracle/oracle-database/12.2/sutil/oracle-logminer-utility.html#GUID-90944343-46BB-4BD5-A0C6-7A4B79D9BEF0">https://docs.oracle.com/en/database/oracle/oracle-database/12.2/sutil/oracle-logminer-utility.html#GUID-90944343-46BB-4BD5-A0C6-7A4B79D9BEF0</a></p><h4 id="2-0-生成字典文件"><a href="#2-0-生成字典文件" class="headerlink" title="2.0. 生成字典文件"></a>2.0. 生成字典文件</h4><p>这一步非必要，个人习惯，每次多生成一份字典文件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">    DBMS_LOGMNR_D.BUILD(OPTIONS<span class="operator">=</span><span class="operator">&gt;</span> DBMS_LOGMNR_D.STORE_IN_REDO_LOGS);</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure><h4 id="2-1-根据1的文件找出1之前最近的一个字典结束文件"><a href="#2-1-根据1的文件找出1之前最近的一个字典结束文件" class="headerlink" title="2.1 根据1的文件找出1之前最近的一个字典结束文件"></a>2.1 根据1的文件找出1之前最近的一个字典结束文件</h4><p>将${minSequence?c}替换成第1步获取到的分析文件列表中最小的的sequence</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> recid , stamp , name , sequence# sequence, next_change# <span class="keyword">as</span> nextChange,next_time <span class="keyword">as</span> nextTime,completion_time <span class="keyword">as</span> completionTime ,dictionary_begin <span class="keyword">as</span> dictionaryBegin,dictionary_end <span class="keyword">as</span> dictionaryEnd</span><br><span class="line"><span class="keyword">FROM</span> V$ARCHIVED_LOG</span><br><span class="line"><span class="keyword">WHERE</span> SEQUENCE# <span class="operator">=</span> (</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">MAX</span> (SEQUENCE#) <span class="keyword">FROM</span> V$ARCHIVED_LOG</span><br><span class="line"><span class="keyword">WHERE</span> DICTIONARY_END <span class="operator">=</span> <span class="string">&#x27;YES&#x27;</span></span><br><span class="line"><span class="keyword">and</span> SEQUENCE# <span class="operator">&lt;=</span> $&#123;minSequence?c&#125; )</span><br></pre></td></tr></table></figure><p>注意，如果这一步没有符合条件的数据，说明分析的范围之前没有字典文件生成，这里我的处理方法是直接把子查询改成查最远的一个字典文件：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">MIN</span> (SEQUENCE#) <span class="keyword">FROM</span> V$ARCHIVED_LOG </span><br><span class="line"><span class="keyword">WHERE</span> DICTIONARY_END <span class="operator">=</span> <span class="string">&#x27;YES&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="2-2-根据2-1文件找出2-1之前最近的一个字典开始文件"><a href="#2-2-根据2-1文件找出2-1之前最近的一个字典开始文件" class="headerlink" title="2.2 根据2.1文件找出2.1之前最近的一个字典开始文件"></a>2.2 根据2.1文件找出2.1之前最近的一个字典开始文件</h4><p>将${minSequence?c}替换成2.1找出的那个字典结束文件的序号</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> recid , stamp , name , sequence# sequence, next_change# <span class="keyword">as</span> nextChange,next_time <span class="keyword">as</span> nextTime,completion_time <span class="keyword">as</span> completionTime ,dictionary_begin <span class="keyword">as</span> dictionaryBegin,dictionary_end <span class="keyword">as</span> dictionaryEnd</span><br><span class="line"><span class="keyword">FROM</span> V$ARCHIVED_LOG</span><br><span class="line"><span class="keyword">WHERE</span> SEQUENCE# <span class="operator">=</span> (</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">MAX</span> (SEQUENCE#) <span class="keyword">FROM</span> V$ARCHIVED_LOG</span><br><span class="line"><span class="keyword">WHERE</span> DICTIONARY_BEGIN <span class="operator">=</span> <span class="string">&#x27;YES&#x27;</span></span><br><span class="line"><span class="keyword">and</span> SEQUENCE# <span class="operator">&lt;=</span> $&#123;minSequence?c&#125; )</span><br></pre></td></tr></table></figure><h3 id="3-全部加入要分析的文件列表中"><a href="#3-全部加入要分析的文件列表中" class="headerlink" title="3. 全部加入要分析的文件列表中"></a>3. 全部加入要分析的文件列表中</h3><p>把1、2找到的文件都丢进去分析列表中<br>这里用freemarker语法表达如下</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line"><span class="operator">&lt;</span>#list logFileNameSet <span class="keyword">as</span> logFile<span class="operator">&gt;</span></span><br><span class="line">   DBMS_LOGMNR.ADD_LOGFILE(LOGFILENAME <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;$&#123;logFile&#125;&#x27;</span><span class="operator">&lt;</span>#if logFile_index<span class="operator">=</span><span class="operator">=</span><span class="number">0</span><span class="operator">&gt;</span>,OPTIONS <span class="operator">=</span><span class="operator">&gt;</span> DBMS_LOGMNR.NEW<span class="operator">&lt;</span><span class="operator">/</span>#if<span class="operator">&gt;</span>);</span><br><span class="line"><span class="operator">&lt;</span><span class="operator">/</span>#list<span class="operator">&gt;</span></span><br><span class="line"><span class="keyword">END</span>;</span><br></pre></td></tr></table></figure><h3 id="4-启动logminer"><a href="#4-启动logminer" class="headerlink" title="4. 启动logminer"></a>4. 启动logminer</h3><p>直接开始即可</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">DBMS_LOGMNR.START_LOGMNR ( OPTIONS <span class="operator">=</span><span class="operator">&gt;</span> DBMS_LOGMNR.DICT_FROM_REDO_LOGS );</span><br><span class="line"><span class="keyword">END</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="5-从视图中查询"><a href="#5-从视图中查询" class="headerlink" title="5. 从视图中查询"></a>5. 从视图中查询</h3><p>如下，从v$logmnr_contents 中查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> v$logmnr_contents <span class="keyword">where</span> TABLE_NAME<span class="operator">=</span>? <span class="keyword">order</span> <span class="keyword">by</span> SCN <span class="keyword">asc</span></span><br></pre></td></tr></table></figure><h3 id="6-退出logminer"><a href="#6-退出logminer" class="headerlink" title="6. 退出logminer"></a>6. 退出logminer</h3><p>每次分析只对当前会话有效，会话关闭后自动退出logminer。建议手动关闭</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">DBMS_LOGMNR.END_LOGMNR();</span><br><span class="line"><span class="keyword">END</span>;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> oracle </tag>
            
            <tag> logminer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes应用中使用TLS(SSL)证书的两种方法及实践</title>
      <link href="//kubernetes-tls-ssl-certificates/"/>
      <url>//kubernetes-tls-ssl-certificates/</url>
      
        <content type="html"><![CDATA[<p>在k8s应用注入自签发的TLS/SSL证书有两种思路：1.使用certificates.k8s.io API 进行签发。2. 直接利用自己的CA证书进行签发。一般推荐第二种方法，本文记录了两种方法的完整实践并最后将其转换为JAVA的使用格式。</p><span id="more"></span><h2 id="零-前言"><a href="#零-前言" class="headerlink" title="零 前言"></a>零 前言</h2><h3 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h3><p>阅读顺序参照下图</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[ peer-config.json] --&gt; C[ ca.pem]</span><br><span class="line">C</span><br><span class="line">C --&gt; D( k8s API)</span><br><span class="line">C --&gt; E( 直接签发)</span><br></pre></td></tr></table></figure><h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><p>直接签发方式是传统方式，只不过是使用了secret技术进行传递而已（换成nfs也可以），本身与k8s没有关系<br>certificates.k8s.io API 方式一般用于kubernetes内部组件，一般不建议与业务系统耦合。</p><h2 id="一-创建证书签名请求配置（peer-config-json）"><a href="#一-创建证书签名请求配置（peer-config-json）" class="headerlink" title="一 创建证书签名请求配置（peer-config.json）"></a>一 创建证书签名请求配置（peer-config.json）</h2><p>这个东西用于确定最终签发的目标证书的内容，示例如下：<br>这个配置生成的证书可用于linshenkx.com做https用（建议hosts把可能用到的都写上，特别是一个证书给多个节点用的情况，否则以后用的时候会有证书hostname不匹配的警告）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;hosts&quot;:</span> [</span><br><span class="line">    <span class="string">&quot;linshenkx.com&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;CN&quot;:</span> <span class="string">&quot;linshenkx&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;key&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;algo&quot;:</span> <span class="string">&quot;ecdsa&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;size&quot;:</span> <span class="number">256</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="二-获取CA证书（ca-pem）"><a href="#二-获取CA证书（ca-pem）" class="headerlink" title="二 获取CA证书（ca.pem）"></a>二 获取CA证书（ca.pem）</h2><h3 id="1-准备好openssl工具"><a href="#1-准备好openssl工具" class="headerlink" title="1. 准备好openssl工具"></a>1. 准备好openssl工具</h3><p>下载及使用参考：<a href="https://kubernetes.io/zh/docs/concepts/cluster-administration/certificates/#cfssl">https://kubernetes.io/zh/docs/concepts/cluster-administration/certificates/#cfssl</a><br>如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o cfssl</span><br><span class="line">curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o cfssljson</span><br><span class="line">curl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o cfssl-certinfo</span><br><span class="line">mv cfssl* /usr/local/bin/</span><br><span class="line">chmod +x /usr/local/bin/cfssl*</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-创建-CA证书签名申请-配置文件（ca-csr-config-json）"><a href="#2-创建-CA证书签名申请-配置文件（ca-csr-config-json）" class="headerlink" title="2. 创建 CA证书签名申请 配置文件（ca-csr-config.json）"></a>2. 创建 CA证书签名申请 配置文件（ca-csr-config.json）</h3><p>这个东西即代表CA本身的概念，可用于生成CA的公钥密钥。<br>使用 <code>cfssl print-defaults csr &gt; ca-csr-config.json</code> 可获取默认配置如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;CN&quot;:</span> <span class="string">&quot;example.net&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;hosts&quot;:</span> [</span><br><span class="line">        <span class="string">&quot;example.net&quot;</span>,</span><br><span class="line">        <span class="string">&quot;www.example.net&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;key&quot;:</span> &#123;</span><br><span class="line">        <span class="attr">&quot;algo&quot;:</span> <span class="string">&quot;ecdsa&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;size&quot;:</span> <span class="number">256</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;names&quot;:</span> [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">&quot;C&quot;:</span> <span class="string">&quot;US&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;L&quot;:</span> <span class="string">&quot;CA&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;ST&quot;:</span> <span class="string">&quot;San Francisco&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>根据需求去配置，参考如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;CN&quot;:</span> <span class="string">&quot;LinShen Self Signed Ca&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;key&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;algo&quot;:</span> <span class="string">&quot;rsa&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;size&quot;:</span> <span class="number">2048</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;names&quot;:</span> [&#123;</span><br><span class="line">    <span class="attr">&quot;C&quot;:</span> <span class="string">&quot;CN&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;ST&quot;:</span> <span class="string">&quot;GD&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;L&quot;:</span> <span class="string">&quot;GZ&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;O&quot;:</span> <span class="string">&quot;LinShen&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;OU&quot;:</span> <span class="string">&quot;LS&quot;</span></span><br><span class="line">  &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-生成-CA密钥及证书"><a href="#3-生成-CA密钥及证书" class="headerlink" title="3. 生成 CA密钥及证书"></a>3. 生成 CA密钥及证书</h3><p>有了CA的CSR配置就可以生成CA密钥及证书了<br>生成ca.pem(CA证书，含公钥)、ca.csr（步骤2对应的csr，没什么用了）、ca-key.pem(CA私钥,需妥善保管)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cfssl gencert -initca ca-csr-config.json | cfssljson -bare ca -</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200826234715284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="生成 CA密钥及证书"></p><p>使用思路1转一，思路2转二</p><h2 id="三-certificates-k8s-io-API"><a href="#三-certificates-k8s-io-API" class="headerlink" title="三 certificates.k8s.io API"></a>三 certificates.k8s.io API</h2><h3 id="签发"><a href="#签发" class="headerlink" title="签发"></a>签发</h3><p>主要参考：<a href="https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster">https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster</a></p><h4 id="1-配置kubernetes使用自定义的CA根证书"><a href="#1-配置kubernetes使用自定义的CA根证书" class="headerlink" title="1 配置kubernetes使用自定义的CA根证书"></a>1 配置kubernetes使用自定义的CA根证书</h4><p>kube-controller-manager已经有了一个默认的实现了，如果想使用上文中我们创建的CA证书作为集群的根证书，则可如下：<br>Kubernetes 提供了一个 certificates.k8s.io API，可以使用配置的 CA 根证书来签发用户证书。该 API 由 kube-controller-manager 实现，其签发证书使用的根证书在下面的命令行中进行配置。我们希望 Kubernetes 采用集群根 CA 来签发用户证书，因此在 kube-controller-manager 的命令行参数中将相关参数配置为了集群根 CA。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/bin/kube-controller-manager \\</span><br><span class="line">--cluster-signing-cert-file=ca.pem             # 用于签发证书的 CA 根证书</span><br><span class="line">--cluster-signing-key-file=ca-key.pem          # 用于签发证书的 CA 根证书的私钥  </span><br><span class="line">--experimental-cluster-signing-duration=438000h # 所签署的证书的有效期时长。(默认值：默认值：8760h0m0s 即一年)</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 更多参数参考：https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-controller-manager/</span></span></span><br></pre></td></tr></table></figure><h4 id="2-创建证书签名请求"><a href="#2-创建证书签名请求" class="headerlink" title="2 创建证书签名请求"></a>2 创建证书签名请求</h4><p>根据流程一的配置，运行以下命令生成私钥和证书签名请求（或CSR）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cfssl genkey ./peer-config.json | cfssljson -bare peer</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>生成文件有 peer-key.pem（私钥，自己先留着）及peer.csr（证书签名请求，用于下一步申请证书）<br><img src="https://img-blog.csdnimg.cn/20200827210639951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="创建证书签名请求"></p><h4 id="3-创建证书签名请求对象并发送到-Kubernetes-API"><a href="#3-创建证书签名请求对象并发送到-Kubernetes-API" class="headerlink" title="3 创建证书签名请求对象并发送到 Kubernetes API"></a>3 创建证书签名请求对象并发送到 Kubernetes API</h4><p>使用以下命令创建 CSR yaml 文件，并发送到 API server：<br>下面命令的作用为 将上一步peer.csr的内容用base64封装在一个CertificateSigningRequest对象里面。<br>其中usages详细选项参照：<a href="https://godoc.org/k8s.io/api/certificates/v1beta1#KeyUsage">https://godoc.org/k8s.io/api/certificates/v1beta1#KeyUsage</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;<span class="string">EOF | kubectl create -f -</span></span><br><span class="line"><span class="string">apiVersion: certificates.k8s.io/v1beta1</span></span><br><span class="line"><span class="string">kind: CertificateSigningRequest</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: peer</span></span><br><span class="line"><span class="string">spec:</span></span><br><span class="line"><span class="string">  groups:</span></span><br><span class="line"><span class="string">  - system:authenticated</span></span><br><span class="line"><span class="string">  request: $(cat peer.csr | base64 | tr -d &#x27;\n&#x27;)</span></span><br><span class="line"><span class="string">  usages:</span></span><br><span class="line"><span class="string">  - digital signature</span></span><br><span class="line"><span class="string">  - key encipherment</span></span><br><span class="line"><span class="string">  - server auth</span></span><br><span class="line"><span class="string">  - client auth</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>执行完毕会提醒：<code> certificatesigningrequest.certificates.k8s.io/peer created </code></p><h4 id="4-同意证书签名请求"><a href="#4-同意证书签名请求" class="headerlink" title="4. 同意证书签名请求"></a>4. 同意证书签名请求</h4><p>这一步应该等待集群管理员来做的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl certificate approve peer</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>执行完毕会提醒： <code>certificatesigningrequest.certificates.k8s.io/peer approved</code></p><h4 id="5-导出证书"><a href="#5-导出证书" class="headerlink" title="5. 导出证书"></a>5. 导出证书</h4><p>通过后就可以导出成pem证书了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get csr peer -o jsonpath=<span class="string">&#x27;&#123;.status.certificate&#125;&#x27;</span> \</span><br><span class="line">| base64 -d &gt; peer-cert.pem</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>其实来到这一步就行了，证书已经签发完毕并且拿到手了，可以看到peer相关的有4个文件：<br>peer-config.json 、peer.csr 用于请求证书签名，可删除<br>peer-key.pem为私钥，peer-cert.pem为被签发的证书（含公钥）<br><img src="https://img-blog.csdnimg.cn/20200827212050426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="导出证书"></p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>在kubernetes内使用需将证书文件peer-cert.pem和私钥peer-key.pem封装成secret传递进去</p><h4 id="1-创建secret"><a href="#1-创建secret" class="headerlink" title="1 创建secret"></a>1 创建secret</h4><p>命令参考自：<a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands">https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands</a><br><code>$ kubectl create tls NAME --cert=path/to/cert/file --key=path/to/key/file [--dry-run=server|client|none]</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create secret tls peer --cert peer-cert.pem --key peer-key.pem</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到，secret里面的私钥及证书为tls.key，tls.crt<br><img src="https://img-blog.csdnimg.cn/20200827220952946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="创建secret"></p><h4 id="2-挂载secret"><a href="#2-挂载secret" class="headerlink" title="2 挂载secret"></a>2 挂载secret</h4><p>以下命令将secret peer挂载到/tmp/tls_secret下，而CA证书会自动加载到pod的/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tls-secret</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/tmp/tls_secret</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line"> </span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tls-secret</span></span><br><span class="line">    <span class="attr">secret:</span></span><br><span class="line">      <span class="attr">secretName:</span> <span class="string">peer</span></span><br></pre></td></tr></table></figure><p>综上，pod里有签发证书（tls.crt）及其私钥(tls.key)，和CA根证书(ca.crt)，已可用于业务系统的tls认证。</p><h2 id="四-直接签发"><a href="#四-直接签发" class="headerlink" title="四 直接签发"></a>四 直接签发</h2><p>测试的时候记得把方法一产生的peer相关文件移除掉，留下peer-config.json就好</p><h3 id="签发-1"><a href="#签发-1" class="headerlink" title="签发"></a>签发</h3><h4 id="1-创建-CA证书-配置文件（ca-config-json）"><a href="#1-创建-CA证书-配置文件（ca-config-json）" class="headerlink" title="1 创建 CA证书 配置文件（ca-config.json）"></a>1 创建 CA证书 配置文件（ca-config.json）</h4><p>这个东西用于以后签发CA证书时的配置（比分说证书有效期/用途 之类）<br>同样，使用<code>cfssl print-defaults config &gt; ca-config.json</code>可获取默认配置，如下<br>如果签发证书的时候不指定profile，则证书有效期只有一周</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;signing&quot;:</span> &#123;</span><br><span class="line">        <span class="attr">&quot;default&quot;:</span> &#123;</span><br><span class="line">            <span class="attr">&quot;expiry&quot;:</span> <span class="string">&quot;168h&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;profiles&quot;:</span> &#123;</span><br><span class="line">            <span class="attr">&quot;www&quot;:</span> &#123;</span><br><span class="line">                <span class="attr">&quot;expiry&quot;:</span> <span class="string">&quot;8760h&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;usages&quot;:</span> [</span><br><span class="line">                    <span class="string">&quot;signing&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;key encipherment&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;server auth&quot;</span></span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;client&quot;:</span> &#123;</span><br><span class="line">                <span class="attr">&quot;expiry&quot;:</span> <span class="string">&quot;8760h&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;usages&quot;:</span> [</span><br><span class="line">                    <span class="string">&quot;signing&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;key encipherment&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;client auth&quot;</span></span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里我的配置参考如下：<br>按服务端、客户端及双向认证进行分类，且有效期为50年（按365天算）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;signing&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;default&quot;:</span> &#123;</span><br><span class="line">      <span class="attr">&quot;expiry&quot;:</span> <span class="string">&quot;438000h&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;profiles&quot;:</span> &#123;</span><br><span class="line">      <span class="attr">&quot;server&quot;:</span> &#123;</span><br><span class="line">        <span class="attr">&quot;expiry&quot;:</span> <span class="string">&quot;438000h&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;usages&quot;:</span> [</span><br><span class="line">          <span class="string">&quot;signing&quot;</span>,</span><br><span class="line">          <span class="string">&quot;key encipherment&quot;</span>,</span><br><span class="line">          <span class="string">&quot;server auth&quot;</span></span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;client&quot;:</span> &#123;</span><br><span class="line">        <span class="attr">&quot;expiry&quot;:</span> <span class="string">&quot;438000h&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;usages&quot;:</span> [</span><br><span class="line">          <span class="string">&quot;signing&quot;</span>,</span><br><span class="line">          <span class="string">&quot;key encipherment&quot;</span>,</span><br><span class="line">          <span class="string">&quot;client auth&quot;</span></span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;peer&quot;:</span> &#123;</span><br><span class="line">        <span class="attr">&quot;expiry&quot;:</span> <span class="string">&quot;438000h&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;usages&quot;:</span> [</span><br><span class="line">          <span class="string">&quot;signing&quot;</span>,</span><br><span class="line">          <span class="string">&quot;key encipherment&quot;</span>,</span><br><span class="line">          <span class="string">&quot;server auth&quot;</span>,</span><br><span class="line">          <span class="string">&quot;client auth&quot;</span></span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-生成证书和私钥"><a href="#2-生成证书和私钥" class="headerlink" title="2 生成证书和私钥"></a>2 生成证书和私钥</h4><p>这里总共用到4个文件，意思为<br>使用ca公密钥（即代表ca本身）根据ca-config.json(即证书签名的配置)以其中peer的profile配置为peer-config.json代表的对象签发名为peer的证书</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer peer-config.json | cfssljson -bare peer</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到多出了3个文件：<br>peer.csr : 证书签名请求，没用了<br>peer.pem : 证书（公钥）<br>peer-key.pem : 私钥<br>至此证书即签发完毕了<br><img src="https://img-blog.csdnimg.cn/20200827223420525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="生成证书及私钥"></p><h3 id="使用-1"><a href="#使用-1" class="headerlink" title="使用"></a>使用</h3><h4 id="1-创建secret-1"><a href="#1-创建secret-1" class="headerlink" title="1 创建secret"></a>1 创建secret</h4><p>由于是直接使用我们的ca.pem、ca-key.pem进行签发的，所以我们是已经拥有ca.pem、peer-key.pem、peer.pem三件套了，这里讲一下将这三个文件传递到pod里面。<br>这里可以视为普通的将文件传递到pod里面，参考 <a href="https://kubernetes.io/zh/docs/concepts/configuration/secret/">https://kubernetes.io/zh/docs/concepts/configuration/secret/</a> 从文件生成secret<br>kubectl apply -k 使用方法参考： <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands">https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands</a></p><p>脚本如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mkdir kustomization &amp;&amp; cp peer.pem peer-key.pem ca.pem kustomization </span><br><span class="line">cd kustomization</span><br><span class="line">cat &lt;&lt;EOF &gt;./kustomization.yaml</span><br><span class="line">secretGenerator:</span><br><span class="line">- name: peer-ca</span><br><span class="line">  files:</span><br><span class="line">  - peer.pem</span><br><span class="line">  - peer-key.pem</span><br><span class="line">  - ca.pem</span><br><span class="line">EOF</span><br><span class="line">cd ..</span><br><span class="line">kubectl apply -k kustomization </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>需要注意的是，这里的secret名字是随机的，需要自己记下来<br><img src="https://img-blog.csdnimg.cn/20200827224758858.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="创建secret"></p><h4 id="2-挂载secret-1"><a href="#2-挂载secret-1" class="headerlink" title="2 挂载secret"></a>2 挂载secret</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tls-secret</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/tmp/tls_secret</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line"> </span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tls-secret</span></span><br><span class="line">    <span class="attr">secret:</span></span><br><span class="line">      <span class="attr">secretName:</span> <span class="string">peer-ca-dg97cgfkgg</span></span><br></pre></td></tr></table></figure><p>综上，pod里有签发证书（peer.pem）及其私钥(peer-key.pem)，和CA根证书(ca.pem)，已可用于业务系统的tls认证。</p><h2 id="五-业务系统使用（HADOOP-JAVA为例）"><a href="#五-业务系统使用（HADOOP-JAVA为例）" class="headerlink" title="五 业务系统使用（HADOOP-JAVA为例）"></a>五 业务系统使用（HADOOP-JAVA为例）</h2><p>该脚本将根据TLS_KEY、TLS_CRT、CA_CRT生成Java用的keystore证书到TARGET_DIR目录下，另外还将<br>脚本解释：</p><ol><li>使用openssl 将pem格式的证书先转成PKCS12再转成JKS 格式<br>TLS_KEY、TLS_CRT =&gt; DEST_KEYSTORE<br>参考自：<a href="https://docs.cloudera.com/documentation/enterprise/5-10-x/topics/cm_sg_openssl_jks.html">https://docs.cloudera.com/documentation/enterprise/5-10-x/topics/cm_sg_openssl_jks.html</a></li><li>CA_CRT =&gt; TRUST_KEYSTORE<br>使用 keytool -import 将CA证书导入自定义JKS 文件和JDK的JKS文件。</li></ol><p>另外需要注意，如果不同JDK版本的 cacerts 位置是不一样的：<br>1.8及以下：<code>$JAVA_HOME/jre/lib/security/cacerts</code><br>9及以上：<code>$JAVA_HOME/lib/security/cacerts </code><br>有空再改一下脚本写成通用的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">   #</span><span class="bash">!/bin/bash</span></span><br><span class="line">   set -e</span><br><span class="line">   </span><br><span class="line"><span class="meta">   #</span><span class="bash"> 使用certificates.k8s.io API方式</span></span><br><span class="line">   TLS_KEY=/tmp/tls_secret/tls.key</span><br><span class="line">   TLS_CRT=/tmp/tls_secret/tls.crt</span><br><span class="line">   CA_CRT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</span><br><span class="line"></span><br><span class="line"><span class="meta">   #</span><span class="bash"> 使用直接签发方式</span></span><br><span class="line"><span class="meta">   #</span><span class="bash"> TLS_KEY=/tmp/tls_secret/peer.pem</span></span><br><span class="line"><span class="meta">   #</span><span class="bash"> TLS_CRT=/tmp/tls_secret/peer-key.pem</span></span><br><span class="line"><span class="meta">   #</span><span class="bash"> CA_CRT=/tmp/tls_secret/ca.pem</span></span><br><span class="line"></span><br><span class="line">   PASSWORD=1qaz@WSX</span><br><span class="line">   TARGET_DIR=/keystore</span><br><span class="line">   PKCS12_OUTPUT=$TARGET_DIR/keystore.pkcs12</span><br><span class="line">   DEST_KEYSTORE=$TARGET_DIR/my.jks</span><br><span class="line">   TRUST_KEYSTORE=$TARGET_DIR/my-truststore.jks</span><br><span class="line">   </span><br><span class="line">   ALIAS_NAME=&quot;service&quot;</span><br><span class="line"></span><br><span class="line">mkdir -p $TARGET_DIR</span><br><span class="line">   openssl &quot;pkcs12&quot; -export -inkey &quot;$&#123;TLS_KEY&#125;&quot; -in &quot;$&#123;TLS_CRT&#125;&quot; -out &quot;$&#123;PKCS12_OUTPUT&#125;&quot; -password &quot;pass:$&#123;PASSWORD&#125;&quot;</span><br><span class="line">   keytool -importkeystore -noprompt -srckeystore &quot;$&#123;PKCS12_OUTPUT&#125;&quot; -srcstoretype &quot;pkcs12&quot; -destkeystore &quot;$&#123;DEST_KEYSTORE&#125;&quot; -storepass &quot;$&#123;PASSWORD&#125;&quot; -srcstorepass &quot;$&#123;PASSWORD&#125;&quot;</span><br><span class="line"></span><br><span class="line">   csplit -z -f crt- $&#123;CA_CRT&#125; &#x27;/-----BEGIN CERTIFICATE-----/&#x27; &#x27;&#123;*&#125;&#x27;</span><br><span class="line"></span><br><span class="line">   for file in crt-*; do</span><br><span class="line">     keytool -import -noprompt -keystore &quot;$&#123;TRUST_KEYSTORE&#125;&quot; -file &quot;$&#123;file&#125;&quot; -storepass &quot;$&#123;PASSWORD&#125;&quot; -alias $&#123;ALIAS_NAME&#125;-$file;</span><br><span class="line">   done</span><br><span class="line">   for file in crt-*; do</span><br><span class="line">     keytool -import -noprompt -keystore $JAVA_HOME/jre/lib/security/cacerts -file &quot;$&#123;file&#125;&quot; -storepass &quot;changeit&quot; -alias $&#123;ALIAS_NAME&#125;-$file;</span><br><span class="line">     </span><br><span class="line">   done</span><br><span class="line">   rm -f crt-*</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Xloggc实践（JVM1.8及之前）</title>
      <link href="//Xloggc/"/>
      <url>//Xloggc/</url>
      
        <content type="html"><![CDATA[<p>Java服务器调优免不了要对gc日志进行分析，我一般是上传gc日志文件到GCEasy进行处理的，上传的文件有大小限制。另外默认的gc日志打印还会存在重启后丢失的问题。综上，我们希望gc日志文件在不能丢失（但允许超过一定时间或大小被清理掉）的情况下控制gc日志的大小或者按时间切割，即像Java日志框架那样的效果。Java9对jvm的日志系统进行了比较大的升级，可以比较好的实现这些需求，但大部分服务端的Java软件还只支持Jdk8，本文记录作者自己的相关配置。</p><span id="more"></span><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>首先推荐看两篇gceasy的博客文章，网上对UseGCLogFileRotation的相关讨论很多都来自于这里：<br><a href="https://blog.gceasy.io/2016/11/15/rotating-gc-log-files/">https://blog.gceasy.io/2016/11/15/rotating-gc-log-files/</a><br><a href="https://blog.gceasy.io/2019/01/29/try-to-avoid-xxusegclogfilerotation/">https://blog.gceasy.io/2019/01/29/try-to-avoid-xxusegclogfilerotation/</a></p><p>简单来说，UseGCLogFileRotation 可以控制gc日志文件大小，且按日期分片切割<br>缺点是：</p><ol><li>记录丢失<br>个人认为不是问题，超过指定文件数量及大小的日志被丢弃是预期操作。</li><li>循环打印<br> 若限制日志文件数共5个，分别为0、1、2、3、4，在文件4达到文件最大值后系统将从1开始覆写，最终的结果就是顺序错乱，不能直接使用（需要人为地修改文件名以修正顺序）</li><li>重启后从编号0开始写入，而非上次的写入位置。结合第2点你就会发现你的日志顺序已经完全不可信了。</li></ol><h3 id="应对方法"><a href="#应对方法" class="headerlink" title="应对方法"></a>应对方法</h3><ol start="0"><li>直接使用-Xloggc:gc-%t.log（<code>推荐</code>）<br>简单粗暴，缺点是文件依然太大，需要用自己切割</li><li>结合-Xloggc:gc-%t.log使用<br>使用 -Xloggc:gc-%t.log可以解决问题3，但仍存在循环打印。<ol><li>设置较大的文件数量和大小限制，尽量使其不产生循环打印<br>上面文章讨论区的一个小伙伴提出的，其实也是个办法<br>以下是我的日志文件输出测试，不同一个批次的文件数量限制可能不同，用蓝框隔离。</li><li>手动调整顺序（其实也不麻烦）。</li></ol></li><li>如果是业务系统的话直接用高版本的jdk吧，不用看这篇文章了</li></ol><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>（使用两个gc日志文件，每个文件最大为1k）：<br>每一个批次都有一个current后缀的日志文件标识当前写入，同一批次的日志文件用从0开始的序号后缀隔开，通常current应该在序号最大的位置，如果不是则说明存在循环打印的情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xmx60M -Xms60M -XX:+PrintGCDetails  -XX:+PrintGCDateStamps  -Xloggc:gc-%t.log  -XX:+UseGCLogFileRotation  -XX:NumberOfGCLogFiles=2  -XX:GCLogFileSize=1K</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200820003502171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="日志示例"></p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jvm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka动态调整副本因子replication.factor及json生成脚本</title>
      <link href="//kafka_replication_factor/"/>
      <url>//kafka_replication_factor/</url>
      
        <content type="html"><![CDATA[<p>kafka默认的副本因子default.replication.factor是1，即无额外副本，如果在创建topic时没有指定副本数，则无高可用性。</p><span id="more"></span><h3 id="1-说明"><a href="#1-说明" class="headerlink" title="1. 说明"></a>1. 说明</h3><p>kafka默认的副本因子default.replication.factor是1，即无额外副本，如果在创建topic时没有指定副本数，则无高可用性。该参数在topic创建时生效，topic创建后无法直接对topic级别的副本数进行修改，但官方提供了在partition级别增加副本数的功能，用于集群添加节点的情况。<br>详情参考官方文档：<a href="https://kafka.apache.org/documentation/#basic_ops_increase_replication_factor">https://kafka.apache.org/documentation/#basic_ops_increase_replication_factor</a></p><p>简单来说就是使用json文件描述该topic每一个partition的情况，每一个partition包含副本分布的描述。然后使用 kafka-reassign-partitions.sh 执行安装json文件完成再分配任务即可。</p><h3 id="2-json生成脚本"><a href="#2-json生成脚本" class="headerlink" title="2. json生成脚本"></a>2. json生成脚本</h3><p>这里提供json的生成脚本，参考自：<a href="https://github.com/dkurzaj/generate-kafka-replication-factor-json/blob/master/generate-kafka-replication-factor-json.sh">https://github.com/dkurzaj/generate-kafka-replication-factor-json/blob/master/generate-kafka-replication-factor-json.sh</a></p><p>其中BROKER_IDS为要分配的brokerId,<br>NUMBER_OF_PARTITIONS为topic分区数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">BROKER_IDS=(1 2 3)</span><br><span class="line">NUMBER_OF_PARTITIONS=5</span><br><span class="line">TOPIC_NAME=__consumer_offsets</span><br><span class="line"></span><br><span class="line">output_file=&quot;increase-rf-json/increase-replication-factor-&quot;$&#123;TOPIC_NAME&#125;&quot;.json&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Beginning of the file</span></span><br><span class="line">echo &#x27;&#123;&quot;version&quot;:1,&#x27; &gt; $output_file</span><br><span class="line">echo &#x27;  &quot;partitions&quot;:[&#x27; &gt;&gt; $output_file</span><br><span class="line"></span><br><span class="line">current_broker_id_index=0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Responsible <span class="keyword">for</span> the circular array over the brokers IDs</span></span><br><span class="line">set_next_broker()&#123;</span><br><span class="line">    current_broker_id_index=$1</span><br><span class="line">    current_broker_id_index=$(($current_broker_id_index + 1))</span><br><span class="line">    current_broker_id_index=$(($current_broker_id_index % $&#123;#BROKER_IDS[@]&#125;))</span><br><span class="line">    return $current_broker_id_index</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Forges the string containing the replicas brokers of a partition</span></span><br><span class="line">get_brokers_string()&#123;</span><br><span class="line">    current_broker_id_index=$1</span><br><span class="line">    brokers_string=&quot;$&#123;BROKER_IDS[$current_broker_id_index]&#125;&quot;</span><br><span class="line">    set_next_broker $current_broker_id_index</span><br><span class="line">    current_broker_id_index=$?</span><br><span class="line">    brokers_string=&quot;$brokers_string,$&#123;BROKER_IDS[$current_broker_id_index]&#125;&quot;</span><br><span class="line">    set_next_broker $current_broker_id_index</span><br><span class="line">    current_broker_id_index=$?</span><br><span class="line">    brokers_string=&quot;$brokers_string,$&#123;BROKER_IDS[$current_broker_id_index]&#125;&quot;</span><br><span class="line">    set_next_broker $current_broker_id_index</span><br><span class="line">    current_broker_id_index=$?</span><br><span class="line">    echo $brokers_string</span><br><span class="line">    return $current_broker_id_index</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Create all the lines</span></span><br><span class="line">partition_number=0</span><br><span class="line">while ((&quot;$partition_number&quot; &lt; &quot;$NUMBER_OF_PARTITIONS-1&quot;)); do</span><br><span class="line">    brokers_string=$(get_brokers_string $current_broker_id_index)</span><br><span class="line">    current_broker_id_index=$?</span><br><span class="line">    echo &quot;    &#123;\&quot;topic\&quot;:\&quot;$TOPIC_NAME\&quot;,\&quot;partition\&quot;:$partition_number,\&quot;replicas\&quot;:[$brokers_string]&#125;,&quot; &gt;&gt; $output_file</span><br><span class="line">    partition_number=$(($partition_number + 1))</span><br><span class="line">done</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Last line without trailing coma</span></span><br><span class="line">brokers_string=$(get_brokers_string $current_broker_id_index)</span><br><span class="line">echo &quot;    &#123;\&quot;topic\&quot;:\&quot;$TOPIC_NAME\&quot;,\&quot;partition\&quot;:$partition_number,\&quot;replicas\&quot;:[$brokers_string]&#125;&quot; &gt;&gt; $output_file</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> End of the file</span></span><br><span class="line">echo &#x27;]&#125;&#x27; &gt;&gt; $output_file</span><br><span class="line"></span><br><span class="line">exit 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>生成json内容如下：<br>可以看到这个文件本质上是对partition而非topic的描述</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;version&quot;</span>:<span class="number">1</span>,</span><br><span class="line">  <span class="attr">&quot;partitions&quot;</span>:[</span><br><span class="line">    &#123;<span class="attr">&quot;topic&quot;</span>:<span class="string">&quot;__consumer_offsets&quot;</span>,<span class="attr">&quot;partition&quot;</span>:<span class="number">0</span>,<span class="attr">&quot;replicas&quot;</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]&#125;,</span><br><span class="line">    &#123;<span class="attr">&quot;topic&quot;</span>:<span class="string">&quot;__consumer_offsets&quot;</span>,<span class="attr">&quot;partition&quot;</span>:<span class="number">1</span>,<span class="attr">&quot;replicas&quot;</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]&#125;,</span><br><span class="line">    &#123;<span class="attr">&quot;topic&quot;</span>:<span class="string">&quot;__consumer_offsets&quot;</span>,<span class="attr">&quot;partition&quot;</span>:<span class="number">2</span>,<span class="attr">&quot;replicas&quot;</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]&#125;,</span><br><span class="line">    &#123;<span class="attr">&quot;topic&quot;</span>:<span class="string">&quot;__consumer_offsets&quot;</span>,<span class="attr">&quot;partition&quot;</span>:<span class="number">3</span>,<span class="attr">&quot;replicas&quot;</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]&#125;,</span><br><span class="line">    &#123;<span class="attr">&quot;topic&quot;</span>:<span class="string">&quot;__consumer_offsets&quot;</span>,<span class="attr">&quot;partition&quot;</span>:<span class="number">4</span>,<span class="attr">&quot;replicas&quot;</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]&#125;</span><br><span class="line">]&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="3-例子"><a href="#3-例子" class="headerlink" title="3. 例子"></a>3. 例子</h3><p>生成后执行命令格式如下（也可使用zookeeper代替bootstrap-server）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br></pre></td></tr></table></figure><p>实例（有50个partition）：<br><img src="https://img-blog.csdnimg.cn/20200819094107721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="execute"></p><p>使用verify代替execute即可查看执行进度<br><img src="https://img-blog.csdnimg.cn/20200819094400579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70#pic_center" alt="verify"></p><h3 id="4-改进方向"><a href="#4-改进方向" class="headerlink" title="4. 改进方向"></a>4. 改进方向</h3><p>目前的shell脚本生成的partition副本分布固定为 BROKER_IDS ，适用于节点数和副本数相同的情况，如果有10节点而只要3副本就不行，生成的json会使副本集中的3个节点。<br>不过一般还是建议修改默认副本数或者创建topic时执行副本数而非使用这个脚本，如果是集群添加节点的情况，建议还是使用专业的带负载平衡功能的kafka管理系统。</p><h3 id="5-建议设置参数"><a href="#5-建议设置参数" class="headerlink" title="5. 建议设置参数"></a>5. 建议设置参数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">offsets.topic.replication.factor=3</span><br><span class="line">transaction.state.log.replication.factor=3</span><br><span class="line">transaction.state.log.min.isr=3</span><br><span class="line">default.replication.factor=3</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>空间索引 S2 学习指南及Java工具类实践</title>
      <link href="//google_spatial_search_s2/"/>
      <url>//google_spatial_search_s2/</url>
      
        <content type="html"><![CDATA[<p>geohash对于大区域查询表现极不良好，经调研测试，改用google的s2。因为涉及的资料、工具较多，特此记录，以备后用。</p><span id="more"></span><h2 id="一-学习指南"><a href="#一-学习指南" class="headerlink" title="一 学习指南"></a>一 学习指南</h2><h3 id="0-介绍说明"><a href="#0-介绍说明" class="headerlink" title="0 介绍说明"></a>0 介绍说明</h3><p>班门不弄斧，这里推荐 halfrost 大神的空间搜索系列文章，推荐先浏览一遍。<br>这一篇是对S2的概念介绍：<a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Go/go_spatial_search.md">高效的多维空间点索引算法 — Geohash 和 Google S2</a><br>这一篇是对S2里面的各个组件的介绍：<a href="https://github.com/halfrost/Halfrost-Field/blob/master/contents/Go/go_s2_regionCoverer.md">Google S2 是如何解决空间覆盖最优解问题的?</a></p><h3 id="1-s2-对比-geohash-的优点"><a href="#1-s2-对比-geohash-的优点" class="headerlink" title="1 s2 对比 geohash 的优点"></a>1 s2 对比 geohash 的优点</h3><ol><li>s2有30级，geohash只有12级。s2的层级变化较平缓，方便选择。</li><li>s2功能强大，解决了向量计算，面积计算，多边形覆盖，距离计算等问题，减少了开发工作量。</li><li><em><strong>s2解决了多边形覆盖问题</strong></em>。个人认为这是其与geohash功能上最本质的不同。给定不规则范围，s2可以计算出一个多边形近似覆盖这个范围。其覆盖用的格子数量根据精确度可控。geohash在这方面十分不友好，划定一个大一点的区域，其格子数可能达到数千，若减少格子数则丢失精度，查询区域过大。<br>如下，在min level和max level不变的情况下，只需设置可接受的max cells数值，即可控制覆盖精度。而且其cell的region大小自动适配。geohash要在如此大范围实现高精度覆盖则会产生极为庞大的网格数。<br>另外需要注意的是,在minLevel,maxLevel,maxCells这3个参数中,不一定能完全满足.一般而言是优先满足maxLevel即最精细的网格大小,再尽可能控制cell数量在maxCells里面.而minLevel由于会合并网格,所以很难满足(在查询大区域的时候可能会出现一个大网格和很多个小网格,导致木桶效应.这个时候可能将大网格划分为指定等级的小网格,即最终效果为,严格遵循minLevel和maxLevel,为此牺牲maxCells,后面有代码)<br><img src="https://img-blog.csdnimg.cn/20200429231333130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="max cells 为10"><br><img src="https://img-blog.csdnimg.cn/20200429231612572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="max cells 为45"><h3 id="2-精度表"><a href="#2-精度表" class="headerlink" title="2 精度表"></a>2 精度表</h3></li></ol><table><thead><tr><th align="center">level</th><th align="center">min area</th><th align="center">max area</th><th align="center">average area</th><th align="center">units</th><th align="center">Random cell 1 (UK) min edge length</th><th align="center">Random cell 1 (UK) max edge length</th><th align="center">Random cell 2 (US) min edge length</th><th align="center">Random cell 2 (US) max edge length</th><th align="center">Number of cells</th></tr></thead><tbody><tr><td align="center">00</td><td align="center">85011012.19</td><td align="center">85011012.19</td><td align="center">85011012.19</td><td align="center">km2</td><td align="center">7842 km</td><td align="center">7842 km</td><td align="center">7842 km</td><td align="center">7842 km</td><td align="center">6</td></tr><tr><td align="center">01</td><td align="center">21252753.05</td><td align="center">21252753.05</td><td align="center">21252753.05</td><td align="center">km2</td><td align="center">3921 km</td><td align="center">5004 km</td><td align="center">3921 km</td><td align="center">5004 km</td><td align="center">24</td></tr><tr><td align="center">02</td><td align="center">4919708.23</td><td align="center">6026521.16</td><td align="center">5313188.26</td><td align="center">km2</td><td align="center">1825 km</td><td align="center">2489 km</td><td align="center">1825 km</td><td align="center">2489 km</td><td align="center">96</td></tr><tr><td align="center">03</td><td align="center">1055377.48</td><td align="center">1646455.50</td><td align="center">1328297.07</td><td align="center">km2</td><td align="center">840 km</td><td align="center">1167 km</td><td align="center">1130 km</td><td align="center">1310 km</td><td align="center">384</td></tr><tr><td align="center">04</td><td align="center">231564.06</td><td align="center">413918.15</td><td align="center">332074.27</td><td align="center">km2</td><td align="center">432 km</td><td align="center">609 km</td><td align="center">579 km</td><td align="center">636 km</td><td align="center">1536</td></tr><tr><td align="center">05</td><td align="center">53798.67</td><td align="center">104297.91</td><td align="center">83018.57</td><td align="center">km2</td><td align="center">210 km</td><td align="center">298 km</td><td align="center">287 km</td><td align="center">315 km</td><td align="center">6K</td></tr><tr><td align="center">06</td><td align="center">12948.81</td><td align="center">26113.30</td><td align="center">20754.64</td><td align="center">km2</td><td align="center">108 km</td><td align="center">151 km</td><td align="center">143 km</td><td align="center">156 km</td><td align="center">24K</td></tr><tr><td align="center">07</td><td align="center">3175.44</td><td align="center">6529.09</td><td align="center">5188.66</td><td align="center">km2</td><td align="center">54 km</td><td align="center">76 km</td><td align="center">72 km</td><td align="center">78 km</td><td align="center">98K</td></tr><tr><td align="center">08</td><td align="center">786.20</td><td align="center">1632.45</td><td align="center">1297.17</td><td align="center">km2</td><td align="center">27 km</td><td align="center">38 km</td><td align="center">36 km</td><td align="center">39 km</td><td align="center">393K</td></tr><tr><td align="center">09</td><td align="center">195.59</td><td align="center">408.12</td><td align="center">324.29</td><td align="center">km2</td><td align="center">14 km</td><td align="center">19 km</td><td align="center">18 km</td><td align="center">20 km</td><td align="center">1573K</td></tr><tr><td align="center">10</td><td align="center">48.78</td><td align="center">102.03</td><td align="center">81.07</td><td align="center">km2</td><td align="center">7 km</td><td align="center">9 km</td><td align="center">9 km</td><td align="center">10 km</td><td align="center">6M</td></tr><tr><td align="center">11</td><td align="center">12.18</td><td align="center">25.51</td><td align="center">20.27</td><td align="center">km2</td><td align="center">3 km</td><td align="center">5 km</td><td align="center">4 km</td><td align="center">5 km</td><td align="center">25M</td></tr><tr><td align="center">12</td><td align="center">3.04</td><td align="center">6.38</td><td align="center">5.07</td><td align="center">km2</td><td align="center">1699 m</td><td align="center">2 km</td><td align="center">2 km</td><td align="center">2 km</td><td align="center">100M</td></tr><tr><td align="center">13</td><td align="center">0.76</td><td align="center">1.59</td><td align="center">1.27</td><td align="center">km2</td><td align="center">850 m</td><td align="center">1185 m</td><td align="center">1123 m</td><td align="center">1225 m</td><td align="center">402M</td></tr><tr><td align="center">14</td><td align="center">0.19</td><td align="center">0.40</td><td align="center">0.32</td><td align="center">km2</td><td align="center">425 m</td><td align="center">593 m</td><td align="center">562 m</td><td align="center">613 m</td><td align="center">1610M</td></tr><tr><td align="center">15</td><td align="center">47520.30</td><td align="center">99638.93</td><td align="center">79172.67</td><td align="center">m2</td><td align="center">212 m</td><td align="center">296 m</td><td align="center">281 m</td><td align="center">306 m</td><td align="center">6B</td></tr><tr><td align="center">16</td><td align="center">11880.08</td><td align="center">24909.73</td><td align="center">19793.17</td><td align="center">m2</td><td align="center">106 m</td><td align="center">148 m</td><td align="center">140 m</td><td align="center">153 m</td><td align="center">25B</td></tr><tr><td align="center">17</td><td align="center">2970.02</td><td align="center">6227.43</td><td align="center">4948.29</td><td align="center">m2</td><td align="center">53 m</td><td align="center">74 m</td><td align="center">70 m</td><td align="center">77 m</td><td align="center">103B</td></tr><tr><td align="center">18</td><td align="center">742.50</td><td align="center">1556.86</td><td align="center">1237.07</td><td align="center">m2</td><td align="center">27 m</td><td align="center">37 m</td><td align="center">35 m</td><td align="center">38 m</td><td align="center">412B</td></tr><tr><td align="center">19</td><td align="center">185.63</td><td align="center">389.21</td><td align="center">309.27</td><td align="center">m2</td><td align="center">13 m</td><td align="center">19 m</td><td align="center">18 m</td><td align="center">19 m</td><td align="center">1649B</td></tr><tr><td align="center">20</td><td align="center">46.41</td><td align="center">97.30</td><td align="center">77.32</td><td align="center">m2</td><td align="center">7 m</td><td align="center">9 m</td><td align="center">9 m</td><td align="center">10 m</td><td align="center">7T</td></tr><tr><td align="center">21</td><td align="center">11.60</td><td align="center">24.33</td><td align="center">19.33</td><td align="center">m2</td><td align="center">3 m</td><td align="center">5 m</td><td align="center">4 m</td><td align="center">5 m</td><td align="center">26T</td></tr><tr><td align="center">22</td><td align="center">2.90</td><td align="center">6.08</td><td align="center">4.83</td><td align="center">m2</td><td align="center">166 cm</td><td align="center">2 m</td><td align="center">2 m</td><td align="center">2 m</td><td align="center">105T</td></tr><tr><td align="center">23</td><td align="center">0.73</td><td align="center">1.52</td><td align="center">1.21</td><td align="center">m2</td><td align="center">83 cm</td><td align="center">116 cm</td><td align="center">110 cm</td><td align="center">120 cm</td><td align="center">422T</td></tr><tr><td align="center">24</td><td align="center">0.18</td><td align="center">0.38</td><td align="center">0.30</td><td align="center">m2</td><td align="center">41 cm</td><td align="center">58 cm</td><td align="center">55 cm</td><td align="center">60 cm</td><td align="center">1689T</td></tr><tr><td align="center">25</td><td align="center">453.19</td><td align="center">950.23</td><td align="center">755.05</td><td align="center">cm2</td><td align="center">21 cm</td><td align="center">29 cm</td><td align="center">27 cm</td><td align="center">30 cm</td><td align="center">7e15</td></tr><tr><td align="center">26</td><td align="center">113.30</td><td align="center">237.56</td><td align="center">188.76</td><td align="center">cm2</td><td align="center">10 cm</td><td align="center">14 cm</td><td align="center">14 cm</td><td align="center">15 cm</td><td align="center">27e15</td></tr><tr><td align="center">27</td><td align="center">28.32</td><td align="center">59.39</td><td align="center">47.19</td><td align="center">cm2</td><td align="center">5 cm</td><td align="center">7 cm</td><td align="center">7 cm</td><td align="center">7 cm</td><td align="center">108e15</td></tr><tr><td align="center">28</td><td align="center">7.08</td><td align="center">14.85</td><td align="center">11.80</td><td align="center">cm2</td><td align="center">2 cm</td><td align="center">4 cm</td><td align="center">3 cm</td><td align="center">4 cm</td><td align="center">432e15</td></tr><tr><td align="center">29</td><td align="center">1.77</td><td align="center">3.71</td><td align="center">2.95</td><td align="center">cm2</td><td align="center">12 mm</td><td align="center">18 mm</td><td align="center">17 mm</td><td align="center">18 mm</td><td align="center">1729e15</td></tr><tr><td align="center">30</td><td align="center">0.44</td><td align="center">0.93</td><td align="center">0.74</td><td align="center">cm2</td><td align="center">6 mm</td><td align="center">9 mm</td><td align="center">8 mm</td><td align="center">9 mm</td><td align="center">7e18</td></tr></tbody></table><h3 id="3-相关资料"><a href="#3-相关资料" class="headerlink" title="3 相关资料"></a>3 相关资料</h3><ol><li>halfrost 的 git 仓库，包含空间搜索系列文章：<a href="https://github.com/halfrost/Halfrost-Field">https://github.com/halfrost/Halfrost-Field</a></li><li>s2 官网：<a href="https://s2geometry.io/">https://s2geometry.io</a></li><li>s2 地图/可视化工具（功能强大，强烈推荐）： <a href="http://s2.sidewalklabs.com/regioncoverer/">http://s2.sidewalklabs.com/regioncoverer/</a></li><li>经纬度画圆/画矩形 地图/可视化工具 ：<a href="https://www.mapdevelopers.com/draw-circle-tool.php">https://www.mapdevelopers.com/draw-circle-tool.php</a></li><li>经纬度画多边形 地图/可视化工具 ：<a href="http://apps.headwallphotonics.com/">http://apps.headwallphotonics.com</a></li><li>csdn参考文章：Google S2 常用操作 ：<a href="https://blog.csdn.net/deng0515001/article/details/88031153">https://blog.csdn.net/deng0515001/article/details/88031153</a></li></ol><h2 id="二-工具类及测试"><a href="#二-工具类及测试" class="headerlink" title="二 工具类及测试"></a>二 工具类及测试</h2><h3 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h3><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>以下是个人使用的Java工具类，持有对象S2RegionCoverer（用于获取给定区域的cellId），用于操作3种常见区域类型（圆，矩形，多边形）。支持多种传参（ch.hsr.geohash的WGS84Point传递经纬度，或者Tuple工具类传递经纬度）<br>主要包含3类方法：</p><ol><li>getS2RegionByXXX<br>获取给定经纬度坐标对应的S2Region,该region可用于获取cellId,或用于判断包含关系</li><li>getCellIdList<br>获取给定region的cellId,并通过childrenCellId方法控制其严格遵守minLevel</li><li>contains<br>对于指定S2Region,判断经纬度或CellToken是否在其范围内</li></ol><p>注意事项：</p><ol><li>该S2RegionCoverer不确定是否线程安全，待测试，不建议动态修改其配置参数</li><li>原生的矩形Rect在某些参数下表现不正常，待确认，这里将其转为多边形对待。<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">S2Util</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 实例</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    INSTANCE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> minLevel = <span class="number">11</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> maxLevel = <span class="number">16</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> maxCells = <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> S2RegionCoverer COVERER = <span class="keyword">new</span> S2RegionCoverer();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        COVERER.setMinLevel(minLevel);</span><br><span class="line">        COVERER.setMaxLevel(maxLevel);</span><br><span class="line">        COVERER.setMaxCells(maxCells);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 将单个cellId转换为多个指定level的cellId</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> s2CellId</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> desLevel</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;S2CellId&gt; <span class="title">childrenCellId</span><span class="params">(S2CellId s2CellId, Integer desLevel)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> childrenCellId(s2CellId, s2CellId.level(), desLevel);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> List&lt;S2CellId&gt; <span class="title">childrenCellId</span><span class="params">(S2CellId s2CellId, Integer curLevel, Integer desLevel)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (curLevel &lt; desLevel) &#123;</span><br><span class="line">            <span class="keyword">long</span> interval = (s2CellId.childEnd().id() - s2CellId.childBegin().id()) / <span class="number">4</span>;</span><br><span class="line">            List&lt;S2CellId&gt; s2CellIds = Lists.newArrayList();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line">                <span class="keyword">long</span> id = s2CellId.childBegin().id() + interval * i;</span><br><span class="line">                s2CellIds.addAll(childrenCellId(<span class="keyword">new</span> S2CellId(id), curLevel + <span class="number">1</span>, desLevel));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> s2CellIds;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> Lists.newArrayList(s2CellId);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 将cellToken转换为经纬度</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> token</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Tuple2&lt;Double, Double&gt; <span class="title">toLatLon</span><span class="params">(String token)</span> </span>&#123;</span><br><span class="line">        S2LatLng latLng = <span class="keyword">new</span> S2LatLng(S2CellId.fromToken(token).toPoint());</span><br><span class="line">        <span class="keyword">return</span> Tuple2.tuple(latLng.latDegrees(), latLng.lngDegrees());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 将经纬度转换为cellId</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> lat</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> lon</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> S2CellId <span class="title">toCellId</span><span class="params">(<span class="keyword">double</span> lat, <span class="keyword">double</span> lon)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> S2CellId.fromLatLng(S2LatLng.fromDegrees(lat, lon));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 判断region是否包含指定cellToken</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> region</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cellToken</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">contains</span><span class="params">(S2Region region, String cellToken)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> region.contains(<span class="keyword">new</span> S2Cell(S2CellId.fromToken(cellToken)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 判断region是否包含指定经纬度坐标</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> region</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> lat</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> lon</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">contains</span><span class="params">(S2Region region, <span class="keyword">double</span> lat, <span class="keyword">double</span> lon)</span> </span>&#123;</span><br><span class="line">        S2LatLng s2LatLng = S2LatLng.fromDegrees(lat, lon);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">boolean</span> contains = region.contains(<span class="keyword">new</span> S2Cell(s2LatLng));</span><br><span class="line">            <span class="keyword">return</span> contains;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (NullPointerException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据region获取cellId列表</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> region</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;S2CellId&gt; <span class="title">getCellIdList</span><span class="params">(S2Region region)</span> </span>&#123;</span><br><span class="line">        List&lt;S2CellId&gt; primeS2CellIdList = COVERER.getCovering(region).cellIds();</span><br><span class="line">        <span class="keyword">return</span> primeS2CellIdList.stream().flatMap(s2CellId -&gt; S2Util.childrenCellId(s2CellId, S2Util.minLevel).stream()).collect(Collectors.toList());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据region获取合并后的cellId列表</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> region</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;S2CellId&gt; <span class="title">getCompactCellIdList</span><span class="params">(S2Region region)</span> </span>&#123;</span><br><span class="line">        List&lt;S2CellId&gt; primeS2CellIdList = COVERER.getCovering(region).cellIds();</span><br><span class="line">        <span class="keyword">return</span> primeS2CellIdList;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/////////////     获取圆形region       ///////////////</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> S2Region <span class="title">getS2RegionByCircle</span><span class="params">(<span class="keyword">double</span> lat, <span class="keyword">double</span> lon, <span class="keyword">double</span> radius)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> capHeight = (<span class="number">2</span> * S2.M_PI) * (radius / <span class="number">40075017</span>);</span><br><span class="line">        S2Cap cap = S2Cap.fromAxisHeight(S2LatLng.fromDegrees(lat, lon).toPoint(), capHeight * capHeight / <span class="number">2</span>);</span><br><span class="line">        S2CellUnion s2CellUnion = COVERER.getCovering(cap);</span><br><span class="line">        <span class="keyword">return</span> cap;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> S2Region <span class="title">getS2RegionByCircle</span><span class="params">(WGS84Point point, <span class="keyword">double</span> radius)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> getS2RegionByCircle(point.getLatitude(), point.getLongitude(), radius);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/////////////     获取矩形region       ///////////////</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> S2Region <span class="title">geS2RegionByRect</span><span class="params">(WGS84Point point1, WGS84Point point2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> getS2RegionByRect(point1.getLatitude(), point1.getLongitude(), point2.getLatitude(), point2.getLongitude());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> S2Region <span class="title">getS2RegionByRect</span><span class="params">(Tuple2&lt;Double, Double&gt; point1, Tuple2&lt;Double, Double&gt; point2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> getS2RegionByRect(point1.getVal1(), point1.getVal2(), point2.getVal1(), point2.getVal2());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> S2Region <span class="title">getS2RegionByRect</span><span class="params">(<span class="keyword">double</span> lat1, <span class="keyword">double</span> lon1, <span class="keyword">double</span> lat2, <span class="keyword">double</span> lon2)</span> </span>&#123;</span><br><span class="line">        List&lt;Tuple2&lt;Double, Double&gt;&gt; latLonTuple2List = Lists.newArrayList(Tuple2.tuple(lat1, lon1), Tuple2.tuple(lat1, lon2), Tuple2.tuple(lat2, lon2), Tuple2.tuple(lat2, lon1));</span><br><span class="line">        <span class="keyword">return</span> getS2RegionByPolygon(latLonTuple2List);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/////////////     获取多边形region       ///////////////</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> S2Region <span class="title">getS2RegionByPolygon</span><span class="params">(WGS84Point[] pointArray)</span> </span>&#123;</span><br><span class="line">        List&lt;Tuple2&lt;Double, Double&gt;&gt; latLonTuple2List = Lists.newArrayListWithExpectedSize(pointArray.length);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; pointArray.length; ++i) &#123;</span><br><span class="line">            latLonTuple2List.add(Tuple2.tuple(pointArray[i].getLatitude(), pointArray[i].getLongitude()));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> getS2RegionByPolygon(latLonTuple2List);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> S2Region <span class="title">getS2RegionByPolygon</span><span class="params">(Tuple2&lt;Double, Double&gt;[] tuple2Array)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> getS2RegionByPolygon(Lists.newArrayList(tuple2Array));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注意需要以逆时针方向添加坐标点</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> S2Region <span class="title">getS2RegionByPolygon</span><span class="params">(List&lt;Tuple2&lt;Double, Double&gt;&gt; latLonTuple2List)</span> </span>&#123;</span><br><span class="line">        List&lt;S2Point&gt; pointList = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;Double, Double&gt; latlonTuple2 : latLonTuple2List) &#123;</span><br><span class="line">            pointList.add(S2LatLng.fromDegrees(latlonTuple2.getVal1(), latlonTuple2.getVal2()).toPoint());</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        S2Loop s2Loop = <span class="keyword">new</span> S2Loop(pointList);</span><br><span class="line">        S2PolygonBuilder builder = <span class="keyword">new</span> S2PolygonBuilder(S2PolygonBuilder.Options.DIRECTED_XOR);</span><br><span class="line">        builder.addLoop(s2Loop);</span><br><span class="line">        <span class="keyword">return</span> builder.assemblePolygon();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/////////////     配置coverer参数       ///////////////</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getMinLevel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> minLevel;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setMinLevel</span><span class="params">(<span class="keyword">int</span> minLevel)</span> </span>&#123;</span><br><span class="line">        S2Util.minLevel = minLevel;</span><br><span class="line">        COVERER.setMinLevel(minLevel);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getMaxLevel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> maxLevel;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setMaxLevel</span><span class="params">(<span class="keyword">int</span> maxLevel)</span> </span>&#123;</span><br><span class="line">        S2Util.maxLevel = maxLevel;</span><br><span class="line">        COVERER.setMaxLevel(maxLevel);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getMaxCells</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> maxCells;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setMaxCells</span><span class="params">(<span class="keyword">int</span> maxCells)</span> </span>&#123;</span><br><span class="line">        S2Util.maxCells = maxCells;</span><br><span class="line">        COVERER.setMaxCells(maxCells);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><h5 id="1-不规则-多边形"><a href="#1-不规则-多边形" class="headerlink" title="1. (不规则)多边形"></a>1. (不规则)多边形</h5><ol><li>去<a href="http://apps.headwallphotonics.com/%E7%94%BB%E4%B8%AA%E5%A4%9A%E8%BE%B9%E5%BD%A2,%E8%BF%99%E9%87%8C%E6%88%91%E7%94%BB%E4%BA%86%E4%B8%AA%E6%9C%89%E6%A3%B1%E6%9C%89%E8%A7%92%E7%9A%84%E7%88%B1%E5%BF%83%E6%A0%87%E5%BF%97,%E5%A6%82%E4%B8%8B">http://apps.headwallphotonics.com/画个多边形,这里我画了个有棱有角的爱心标志,如下</a>.<br><img src="https://img-blog.csdnimg.cn/20200502173431384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="爱心标志"></li><li>将左下角的坐标信息作为参数,调用,测试代码如下<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getCellIdListByPolygon</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Map&lt;Integer,Integer&gt; sizeCountMap= Maps.newHashMap();</span><br><span class="line">    StringBuilder sb3=<span class="keyword">new</span> StringBuilder();</span><br><span class="line">    S2Region s2Region = S2Util.getS2RegionByPolygon(Lists.newArrayList(Tuple2.tuple(<span class="number">23.851458634747043</span>, <span class="number">113.66432546548037</span>),  Tuple2.tuple(<span class="number">21.60205563594303</span>, <span class="number">114.82887624673037</span>),Tuple2.tuple(<span class="number">23.771049234941454</span>, <span class="number">116.18019460610537</span>),Tuple2.tuple(<span class="number">23.16640234327511</span>, <span class="number">114.94423269204286</span>)));</span><br><span class="line">    List&lt;S2CellId&gt; cellIdListByPolygon = S2Util.getCellIdList(s2Region);</span><br><span class="line">    cellIdListByPolygon.forEach(s2CellId -&gt; &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Level:&quot;</span> + s2CellId.level() + <span class="string">&quot;,ID:&quot;</span> + s2CellId.toToken() + <span class="string">&quot;,Min:&quot;</span> + s2CellId.rangeMin().toToken() + <span class="string">&quot;,Max:&quot;</span> + s2CellId.rangeMax().toToken());</span><br><span class="line">        sb3.append(<span class="string">&quot;,&quot;</span>).append(s2CellId.toToken());</span><br><span class="line">        sizeCountMap.put(s2CellId.level(),sizeCountMap.getOrDefault(s2CellId.level(),<span class="number">0</span>)+<span class="number">1</span>);</span><br><span class="line">    &#125;);</span><br><span class="line">    System.out.println(sb3.substring(<span class="number">1</span>));</span><br><span class="line">    System.out.println(<span class="string">&quot;totalSize:&quot;</span>+cellIdListByPolygon.size());</span><br><span class="line">    sizeCountMap.entrySet().forEach(integerIntegerEntry -&gt; &#123;</span><br><span class="line">        System.out.printf(<span class="string">&quot;level:%d,size:%d\n&quot;</span>,integerIntegerEntry.getKey(),integerIntegerEntry.getValue());</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>执行结果如下<br><img src="https://img-blog.csdnimg.cn/20200502173708151.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看到网格数远远超出了设定的100,不过网格大小严格控制在了11到16之间.</li><li>将cellToken列表(逗号分隔)复制到 <a href="http://s2.sidewalklabs.com/regioncoverer/">http://s2.sidewalklabs.com/regioncoverer/</a> ,点击那个网格(data)标志即可,如下<br><img src="https://img-blog.csdnimg.cn/20200502174203269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li><li>如果调用的是getCompactCellIdList,则结果如下,其cell数从1000多压缩到200多.<br><img src="https://img-blog.csdnimg.cn/20200502174644676.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h5 id="2-圆形"><a href="#2-圆形" class="headerlink" title="2 圆形"></a>2 圆形</h5></li><li>这次我们再用 <a href="https://www.mapdevelopers.com/draw-circle-tool.php">https://www.mapdevelopers.com/draw-circle-tool.php</a> 到台湾省上面画个圈圈,如下<br><img src="https://img-blog.csdnimg.cn/20200502175137239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="圈圈"></li><li>然后将其经纬度和坐标信息作为参数,调用,测试方法如下,使用的是getCompactCellIdList<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getCellIdListByCircle</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Map&lt;Integer,Integer&gt; sizeCountMap= Maps.newHashMap();</span><br><span class="line">    StringBuilder sb3=<span class="keyword">new</span> StringBuilder();</span><br><span class="line">    S2Region s2Region = S2Util.getS2RegionByCircle(<span class="number">23.753954</span>,<span class="number">120.749615</span>,<span class="number">193511.10</span>);</span><br><span class="line">    List&lt;S2CellId&gt; cellIdListByPolygon = S2Util.getCompactCellIdList(s2Region);</span><br><span class="line">    cellIdListByPolygon.forEach(s2CellId -&gt; &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Level:&quot;</span> + s2CellId.level() + <span class="string">&quot;,ID:&quot;</span> + s2CellId.toToken() + <span class="string">&quot;,Min:&quot;</span> + s2CellId.rangeMin().toToken() + <span class="string">&quot;,Max:&quot;</span> + s2CellId.rangeMax().toToken());</span><br><span class="line">        sb3.append(<span class="string">&quot;,&quot;</span>).append(s2CellId.toToken());</span><br><span class="line">        sizeCountMap.put(s2CellId.level(),sizeCountMap.getOrDefault(s2CellId.level(),<span class="number">0</span>)+<span class="number">1</span>);</span><br><span class="line">    &#125;);</span><br><span class="line">    System.out.println(sb3.substring(<span class="number">1</span>));</span><br><span class="line">    System.out.println(<span class="string">&quot;totalSize:&quot;</span>+cellIdListByPolygon.size());</span><br><span class="line">    sizeCountMap.entrySet().forEach(integerIntegerEntry -&gt; &#123;</span><br><span class="line">        System.out.printf(<span class="string">&quot;level:%d,size:%d\n&quot;</span>,integerIntegerEntry.getKey(),integerIntegerEntry.getValue());</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>结果如下<br><img src="https://img-blog.csdnimg.cn/20200502175555629.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li></ol>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> s2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBCK2修复RIT实践笔记</title>
      <link href="//hbck2_rit/"/>
      <url>//hbck2_rit/</url>
      
        <content type="html"><![CDATA[<p>本文记录了作者使用HBCK2工具对线上HBase发生RIT状态的处理，仅供参考，若有疵漏，还望指正。<br>网络上关于HBCK2的文章很少，基本都是复制粘贴自田竞云(小米)的这一篇：<a href="https://mp.weixin.qq.com/s/GVMWwB1WsKcdvZGfvX1lcA?spm=a2c4e.11153940.blogcont683107.11.49d762a815MegW">HBase指南 | HBase 2.0之修复工具HBCK2运维指南</a><br>事实上这一篇文章介绍得也已经很详细了。这里只是做一些实践上的补充说明。</p><span id="more"></span><h3 id="1-下载"><a href="#1-下载" class="headerlink" title="1. 下载"></a>1. 下载</h3><p>直接去<a href="https://hbase.apache.org/downloads.html">hbase的官网下载地址</a>里就可以找到。这里直接给最新版本的下载链接（截止至2020年4月）：<a href="https://downloads.apache.org/hbase/hbase-operator-tools-1.0.0/hbase-operator-tools-1.0.0-bin.tar.gz">https://downloads.apache.org/hbase/hbase-operator-tools-1.0.0/hbase-operator-tools-1.0.0-bin.tar.gz</a><br>但还是推荐自己去git clone编译，因为官网提供的编译版本有滞后性。通常来说，使用最新版本的hbase再搭配使用最新编译的HBCK2,可以解决绝大部分莫名其妙的问题。（fixMeta+restart暴力流）<br>新版本的HBCK2有更多更方便的功能，不过一般只能在新版本的hbase中使用。</p><h3 id="2-使用命令"><a href="#2-使用命令" class="headerlink" title="2. 使用命令"></a>2. 使用命令</h3><p>将其解压后得到 hbase-hbck2-1.0.0.jar，再cp到$HBASE_HOME/lib下，执行  <em><strong>hbase org.apache.hbase.HBCK2 &lt;命令&gt;</strong></em>  即可，第一次使用推荐  <em><strong>hbase org.apache.hbase.HBCK2  -h</strong></em> 查看详细介绍</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><h3 id="1-查找问题"><a href="#1-查找问题" class="headerlink" title="1. 查找问题"></a>1. 查找问题</h3><p>参考HBCK2运维指南的思路：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A(canary tool)--&gt; B(Procedures &amp; Locks 页面状态)</span><br><span class="line">B --&gt; C(RIT队列)</span><br><span class="line">C --&gt; D(Master日志)</span><br></pre></td></tr></table></figure><h3 id="2-实践例子"><a href="#2-实践例子" class="headerlink" title="2. 实践例子"></a>2. 实践例子</h3><ol><li><p>处理Procedures &amp; Locks<br>在 Procedures &amp; Locks 页面查找waiting状态的procedure，按顺序进行bypass。按顺序是因为有一些waiting的发生是procedure存在依赖关系，将其bypass后后面的procedure会进入success状态。如果bypass返回false就使用bypass -r，还是不行再使用bypass -or</p></li><li><p>处理RIT队列<br>参考HBCK2运维指南可以以txt格式拿到RIT队列的所有procedure的id，将其复制到任意文件（如pid.txt），再执行以下命令即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat pid.txt | xargs hbase org.apache.hbase.HBCK2 bypass -or </span><br></pre></td></tr></table></figure><p>然后再以txt格式拿到RIT队列的所有region的encodedName，将其复制到任意文件（如region.txt），再执行以下命令即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat region.txt | xargs hbase org.apache.hbase.HBCK2 assigns</span><br></pre></td></tr></table></figure></li><li><p>assign各个表中offline的region<br>检查一下各个表中是否有region的StorefileSize为0，当然也可能是本身没有存储多少数据，要注意辨别。这种一般对其assigns就可以了。<br><img src="https://img-blog.csdnimg.cn/20200416012024639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>对于一些大表可能有上千个region的，一个个甄选未免太浪费时间，可以直接在web界面将其region区域全部拷贝下来，复制到txt，使用下述命令进行筛选</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat regions.txt |grep -v &#x27;GB&#x27; |grep -v &#x27;MB&#x27; &gt; region2.txt</span><br></pre></td></tr></table></figure></li><li><p>高版本的hbase有hbck记录页，去页面查看，根据提醒操作就行，更方便<br><img src="https://img-blog.csdnimg.cn/20201214142116129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>使用以下Java代码即可提取出encodedName</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">printRegionEncodedName</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    List&lt;String&gt; lines = FileUtil.readUtf8Lines(<span class="string">&quot;C:\\tmp\\region2.txt&quot;</span>);</span><br><span class="line">    List&lt;String&gt; regionEncodedNameList= Lists.newArrayListWithExpectedSize(lines.size());</span><br><span class="line">    <span class="keyword">for</span> (String line : lines) &#123;</span><br><span class="line">        <span class="keyword">if</span>(StringUtils.isBlank(line))&#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> point1=line.indexOf(<span class="string">&quot;.&quot;</span>) ;</span><br><span class="line">        <span class="keyword">int</span> point2=line.indexOf(<span class="string">&quot;.&quot;</span>, line.indexOf(<span class="string">&quot;.&quot;</span>)+<span class="number">1</span>);</span><br><span class="line">        String s = line.substring(point1+<span class="number">1</span>, point2);</span><br><span class="line">        System.out.println(s);</span><br><span class="line">        regionEncodedNameList.add(s);</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">&quot;size:&quot;</span>+regionEncodedNameList.size());</span><br><span class="line">    <span class="keyword">try</span> (PrintWriter pw = <span class="keyword">new</span> PrintWriter(<span class="keyword">new</span> FileWriter(<span class="string">&quot;C:\\tmp\\encodedName.txt&quot;</span>));)&#123;</span><br><span class="line">        regionEncodedNameList.forEach(regionEncodedName-&gt;pw.println(regionEncodedName+<span class="string">&quot; &quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后又是 <em><strong>cat encodedName.txt | xargs hbase org.apache.hbase.HBCK2 assigns</strong></em> 就可以了<br>如下，一次性对大量region进行操作<br><img src="https://img-blog.csdnimg.cn/20201214141729730.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></li></ol><h3 id="3-注意事项"><a href="#3-注意事项" class="headerlink" title="3. 注意事项"></a>3. 注意事项</h3><p>对procedure执行bypass后其状态会由waiting转换为success(bypassed)，但不会立即移除出rit队列。可通过重启master解决。        </p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> HBASE </category>
          
          <category> bug </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
            <tag> debug </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入解析Spring使用枚举接收参数和返回值机制并提供自定义最佳实践</title>
      <link href="//spring_enum/"/>
      <url>//spring_enum/</url>
      
        <content type="html"><![CDATA[<p>Spring对应枚举传参/返回值默认是用字面量实现的（实际情况更复杂），而《阿里巴巴Java开发手册》规定接口返回值不可以使用枚举类型（包括含枚举类型的POJO对象），为此，本文探究了Spring内部对枚举参数的传递和处理机制，并提供了一套自定义方案。</p><span id="more"></span><h2 id="一-目标与思路"><a href="#一-目标与思路" class="headerlink" title="一 目标与思路"></a>一 目标与思路</h2><h3 id="0-起因"><a href="#0-起因" class="headerlink" title="0 起因"></a>0 起因</h3><p>《阿里巴巴Java开发手册》将接口中枚举的使用分为两类，即 接口参数和接口返回值，并规定：<br><strong>接口参数可以使用枚举类型，但接口返回值不可以使用枚举类型（包括含枚举类型的POJO对象）</strong>。</p><p>知乎有相关讨论和作者亲答，详情可见：<a href="https://www.zhihu.com/question/52760637">Java枚举什么不好，《阿里巴巴JAVA开发手册》对于枚举规定的考量是什么？</a></p><p>现摘录一部分作者回答如下：</p><blockquote><blockquote><p><strong>由于升级原因，导致双方的枚举类不尽相同，在接口解析，类反序列化时出现异常</strong>。</p></blockquote><blockquote><p>Java中出现的任何元素，在Gosling的角度都会有背后的思考和逻辑（尽管并非绝对完美，但Java的顶层抽象已经是天才级了），比如：接口、抽象类、注解、和本文提到的枚举。枚举有好处，类型安全，清晰直接，还可以使用等号来判断，也可以用在switch中。<code>它的劣势也是明显的，就是不要扩展</code>。可是为什么在返回值和参数进行了区分呢，如果不兼容，那么两个都有问题，怎么允许参数可以有枚举。当时的考虑，如果参数也不能用，那么枚举几乎无用武之地了。参数输出，毕竟是本地决定的，你本地有的，传送过去，向前兼容是不会有问题的。但如果是接口返回，就比较恶心了，因为解析回来的这个枚举值，可能本地还没有，这时就会抛出序列化异常。</p></blockquote><blockquote><p>比如：你的本地枚举类，有一个天气Enum：SUNNY, RAINY, CLOUDY，如果根据天气计算心情的方法：guess(WeatcherEnum xx)，传入这三个值都是可以的。返回值：Weather guess(参数)，那么对方运算后，返回一个SNOWY，本地枚举里没有这个值，傻眼了。</p></blockquote></blockquote><p>当然，使用 code 照样不能处理，对此，开发手册作者的回答如下</p><blockquote><p>主要是从防止这种序列化异常角度来考虑，使用code至少不会出大乱子。而catch序列化异常，有点像catch(NullPointerException e)一样代码过度，因为它是可预检异常。</p></blockquote><h3 id="1-统一称谓"><a href="#1-统一称谓" class="headerlink" title="1 统一称谓"></a>1 统一称谓</h3><p>假如有一枚举类如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">ReturnCodeEnum</span> </span>&#123;</span><br><span class="line">    OK(<span class="number">200</span>),</span><br><span class="line">    ERROR(<span class="number">500</span>)</span><br><span class="line">    ;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> code;</span><br><span class="line">    ReturnCodeEnum(<span class="keyword">int</span> code)&#123;</span><br><span class="line">        <span class="keyword">this</span>.code=code;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> code;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>枚举实例有两个默认属性，<code>name</code> 和 <code>ordinal</code>，可通过 name()和ordinal()方法分别获得。其中 name 为枚举字面量（如 OK），ordinal 为枚举实例默认次序（从0开始）<br>需要注意的是，不建议使用枚举的 ordinal，因为枚举实例应该是无序的，ordinal 提供的顺序是不可靠的，所以我们应该使用自定义的枚举字段 code。</p></blockquote><p>后文为方便阐述，以 字面量（name）、默认次序（ordinal）和 code来展开阐述。如 OK 的 字面量为 OK，ordinal 为 0 ，code为 200。</p><h3 id="2-目标"><a href="#2-目标" class="headerlink" title="2 目标"></a>2 目标</h3><h4 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h4><ol><li>直接使用 枚举类型 <strong>接收参数</strong>和<strong>返回值</strong></li><li>系统自动将 参数中的 code 转换为 枚举类型，自动将 返回值中的枚举类型转换为 code</li></ol><h4 id="实现效果"><a href="#实现效果" class="headerlink" title="实现效果"></a>实现效果</h4><p>对于实现通用code枚举接口的枚举类型，有如下效果：</p><ol><li>使用 bean（<strong>application/x-www-form-urlencoded</strong>）接收时，支持 code 自动转换为 枚举类型，同时兼容 字面量转换为枚举类型。注意：表单接收的参数都视为 String，即是将String转为 枚举类型</li><li>使用 @RequestBody （<strong>application/json</strong>）接收时，默认只支持 code 自动转换为枚举类型。如果需要同时支持 code 和 字面量（或者只支持字面量），可以在具体的枚举类里添加@JsonCreator注解的方法，下文会给出参考实现。</li><li>可以使用 @RequestParam 和 @PathVariable 接收枚举类型参数</li><li>使用 @ResponseBody / @RestController（返回 Json）时，默认将 枚举类型转换为 code。</li><li>在接收参数/返回值都不允许使用 ordinal ，这只会导致混乱。</li></ol><h3 id="3-SpringMVC-对-枚举参数的处理"><a href="#3-SpringMVC-对-枚举参数的处理" class="headerlink" title="3 SpringMVC 对 枚举参数的处理"></a>3 SpringMVC 对 枚举参数的处理</h3><p>此处只对 restful 接口进行讨论。对于 restful 接口，Spring MVC 的返回值是使用 @ResponseBody 进行处理的。<br>而参数的接收方式则较多，对于非简单类型，如 Enum ，一般的接收方法为 Bean 接收或 @ResponseBody 接收。</p><h4 id="Spring使用Bean接收枚举参数"><a href="#Spring使用Bean接收枚举参数" class="headerlink" title="Spring使用Bean接收枚举参数"></a>Spring使用Bean接收枚举参数</h4><p>简单来说 Spring 默认使用Bean接收枚举参数时支持 <code>字面量</code>，这也是我们常见的做法。</p><blockquote><p>参考自：<a href="http://note4code.com/2018/03/12/spring%E4%B8%8E%E6%9E%9A%E4%B8%BE%E5%8F%82%E6%95%B0/">Spring与枚举参数</a></p><blockquote><p>GET 请求和 POST Form 请求中的字符串到枚举的转化是通过 org.springframework.core.convert.support.StringToEnumConverterFactory 来实现的.<br>该类实现了接口 ConverterFactory ，通过调用 Enum.valueOf(Class, String) 实现了这个功能。<br>向下追溯源码可以发现该方法实际上是从一个 Map&lt;String, Enum&gt; 的字典中获取了转换后的实际值，着这个 String 类型的 Key 的获取方式就是 Enum.name() 返回的结果，即<code>枚举的字面值</code>。</p></blockquote></blockquote><h4 id="Spring使用-RequestBody-接收枚举参数"><a href="#Spring使用-RequestBody-接收枚举参数" class="headerlink" title="Spring使用@RequestBody 接收枚举参数"></a>Spring使用@RequestBody 接收枚举参数</h4><p>简单来说 Spring使用@RequeseBody 接收枚举参数时支持 <code>字面量和 ordinal</code></p><p>对于@RequestBody，Spring会将其内容视为一段 Json，所做工作为使用 Jackson 完成反序列化。其实现会经过Jackson的EnumDeserializer的deserialize方法。感兴趣的可以去看看源码，这里不贴出来，讲一下思路：</p><ol><li><p>使用字面量（String）进行反序列化</p></li><li><p>判断是否是 int 类型，如果是使用 ordinal 进行反序列化，如果数字不在 ordinal 里面，则抛异常</p></li><li><p>判断是否是数组，是的话交由数组处理，否则抛异常</p><h4 id="Spring使用-ResponseBody-返回值"><a href="#Spring使用-ResponseBody-返回值" class="headerlink" title="Spring使用@ResponseBody 返回值"></a>Spring使用@ResponseBody 返回值</h4><p>如我们平常使用所见，返回的是字面量</p><h3 id="4-思路"><a href="#4-思路" class="headerlink" title="4 思路"></a>4 思路</h3><p>参照Spring对枚举参数的处理，我们可以提供覆盖/替换Spring的处理来达到我们的效果，<br>经本人测试，比较好的实现方案有（不考虑反射）：</p></li><li><p>自定义Bean接收枚举参数规则：</p><ol><li>可行方案<br>通过Spring MVC注入特定类型自定义转换器实现从code到 枚举的自动转换</li><li>做法<br>使用 WebMvcConfigurer的<code>addFormatters</code>注入自定义ConverterFactory，该工厂负责生成 通用code枚举接口的实现类对应的转换器<br>详见第二部分–代码实现。</li><li>参考资料<br><a href="https://blog.csdn.net/u014527058/article/details/62883573">Spring Boot绑定枚举类型参数</a></li></ol></li><li><p>自定义@RequestBody 和@ResponseBody处理枚举参数</p><ol><li><p>可行方案<br>使用<code>@JsonValue</code>自定义特定枚举类的Jackson序列化/反序列化方式</p><ol><li>具体做法<br>使用 <code>@JsonValue</code>注解标记  获取code值的枚举实例方法。</li><li>注意事项<br>该code值是使用jackson<code>序列化</code>/<code>反序列化</code>时枚举对应的值，会覆盖原来从字面量反序列化回枚举的默认实现。<br>如果想要保留原来从字面量反序列化回枚举类的功能，需要自定义一个<code> @JsonCreator</code> 的构造/静态工厂方法。</li><li>相关代码<br>代码如下：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@JsonValue</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> code;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@JsonCreator</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ReturnCodeEnum <span class="title">create</span><span class="params">(String name)</span></span>&#123;</span><br><span class="line">    <span class="keyword">try</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ReturnCodeEnum.valueOf(name);</span><br><span class="line">    &#125;<span class="keyword">catch</span> (IllegalArgumentException e)&#123;</span><br><span class="line">        <span class="keyword">int</span> code=Integer.parseInt(name);</span><br><span class="line">        <span class="keyword">for</span> (ReturnCodeEnum value : ReturnCodeEnum.values()) &#123;</span><br><span class="line">            <span class="keyword">if</span>(value.code==code)&#123;</span><br><span class="line">                <span class="keyword">return</span> value;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;No element matches &quot;</span>+name);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></li><li><p>不可行方案</p><ol><li><p>替换@RequestBody和@ResponseBody或相关处理器  / 自定义HttpMessageConverter</p><ul><li><p>例子<br>如使用自定义的 @ResponseBody 注解及对应 HandlerMethodReturnValueHandler<br>使用自定义 HttpMessageConverter 实现对 json 返回资源的完全控制</p></li><li><p>不可行原因<br>我们平时使用 @ResponseBody 是交给 <code>RequestResponseBodyMethodProcessor </code>这个类处理，所以我们也可以弃用@ResponseBody并自己写一个注解和对负责处理该注解的 HandlerMethodReturnValueHandler。这样我们就可以完全控制返回值的处理了。这样也就相对于放弃了 @ResponseBody。<br>自己实现 HttpMessageConverter 则是在更高的层次进行处理，代价太大。</p></li><li><p>相关资料<br><a href="https://www.baeldung.com/spring-httpmessageconverter-rest">baeldung：Http Message Converters with the Spring Framework</a><br><a href="https://www.baeldung.com/spring-type-conversions">baeldung：Guide to Spring Type Conversions</a><br>自定义注解和HandlerMethodReturnValueHandler可以参考：<br><a href="https://www.ctolib.com/topics-109580.html">Spring MVC 更灵活的控制 json 返回（自定义过滤字段）</a><br><a href="https://www.jianshu.com/p/4fa3006c066f">深入Spring:自定义ResponseBody</a><br><a href="http://www.xiaojiezhu.com/wordpress/?p=17#i">spring自定义返回值解析器</a><br><a href="http://www.cnblogs.com/fangjian0423/p/springMVC-request-param-analysis.html#interface_demo">详解SpringMVC中Controller的方法中参数的工作原理[附带源码分析]</a><br><a href="https://blog.csdn.net/a67474506/article/details/46364159">SpringMVC 学习笔记(七) JSON返回:HttpMessageConverter作用</a><br><a href="https://www.scienjus.com/custom-http-message-converter/">使用自定义HttpMessageConverter对返回内容进行加密</a><br><a href="https://www.jianshu.com/p/e25ecc0c5762">自定义枚举 — Gson转换</a></p></li></ul></li><li><p>使用@JsonCreator在接口层面定义反序列化规则</p><ul><li>不可行原因</li></ul><p>   <strong>@JsonCreator只适用于枚举类不适用于接口。</strong><br>   @JsonCreator本质上是要在没有类实例的时候使用的，所以只能标记在 构造方法或者静态工厂方法上，接口的话不可行，传统的接口方法属实例方法，新增的 default 方法也属实例方法，另外的 static 方法又不可继承。所以这个思路只限于具体类型，不适用于接口。</p><ul><li>相关资料<br>相关Jackson资料参考：<br><a href="https://github.com/FasterXML/jackson-annotations/wiki/Jackson-Annotations">Github：Jackson注解官方文档</a><br><a href="https://cloud.tencent.com/developer/article/1147127">Jackson常用注解详解1 初级2 中级</a></li></ul></li><li><p>适用@JsonDeserialize在接口层面定义反序列化规则</p><ul><li>不可行原因<br><strong>注解自定义从 json字符串 转换为 实体类的方法也只适用于枚举类不适用于接口。</strong><br>使用@JsonDeserialize(using = 自定义反序列化类.class)，在自定义Jackson反序列化类实现deserialize(JsonParser p, DeserializationContext ctxt)方法。<br>可以获取 json字符串（即 code），但没办法通过接口使用code获取枚举对象，理由同上，接口没有可用的同时可继承的方法。</li><li>相关资料<br>自定义Jackson序列化/反序列化类参考：<a href="https://www.ibm.com/developerworks/cn/java/jackson-advanced-application/index.html">IBM:Jackson 框架的高阶应用</a></li></ul></li></ol></li></ol></li></ol><h2 id="二-代码实现"><a href="#二-代码实现" class="headerlink" title="二 代码实现"></a>二 代码实现</h2><h3 id="1-通用code枚举接口"><a href="#1-通用code枚举接口" class="headerlink" title="1 通用code枚举接口"></a>1 通用code枚举接口</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> V1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: linshenkx</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span>: 2019/1/12</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 带编号的枚举接口</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CodedEnum</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 使用jackson序列化/反序列化时枚举对应的值</span></span><br><span class="line"><span class="comment">     * 如果想要保留原来从字面量反序列化回枚举类的功能，</span></span><br><span class="line"><span class="comment">     * 需要自定义一个 <span class="doctag">@JsonCreator</span> 的构造/静态工厂方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 自定义枚举code</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@JsonValue</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getCode</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-转换器工厂类"><a href="#2-转换器工厂类" class="headerlink" title="2 转换器工厂类"></a>2 转换器工厂类</h3><p>代码实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> V1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: linshenkx</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span>: 2019/1/12</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 带编号的枚举转换器 工厂</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CodedEnumConverterFactory</span> <span class="keyword">implements</span> <span class="title">ConverterFactory</span>&lt;<span class="title">String</span>, <span class="title">CodedEnum</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 目标类型与对应转换器的Map</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;Class,Converter&gt; CONVERTER_MAP=<span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据目标类型获取相应的转换器</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> targetType 目标类型</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> &lt;T&gt; CodedEnum的实现类</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> &lt;T extends CodedEnum&gt; <span class="function">Converter&lt;String, T&gt; <span class="title">getConverter</span><span class="params">(Class&lt;T&gt; targetType)</span> </span>&#123;</span><br><span class="line">        Converter converter=CONVERTER_MAP.get(targetType);</span><br><span class="line">        <span class="keyword">if</span>(converter==<span class="keyword">null</span>)&#123;</span><br><span class="line">            converter=<span class="keyword">new</span> IntegerStrToEnumConverter&lt;&gt;(targetType);</span><br><span class="line">            CONVERTER_MAP.put(targetType,converter);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> converter;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 将int对应的字符串转换为目标类型的转换器</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> &lt;T&gt; 目标类型（CodedEnum的实现类）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">IntegerStrToEnumConverter</span>&lt;<span class="title">T</span> <span class="keyword">extends</span> <span class="title">CodedEnum</span>&gt; <span class="keyword">implements</span> <span class="title">Converter</span>&lt;<span class="title">String</span>,<span class="title">T</span>&gt;</span>&#123;</span><br><span class="line">        <span class="keyword">private</span> Map&lt;String,T&gt; enumMap=<span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="title">IntegerStrToEnumConverter</span><span class="params">(Class&lt;T&gt; enumType)</span></span>&#123;</span><br><span class="line">            T[] enums=enumType.getEnumConstants();</span><br><span class="line">            <span class="keyword">for</span> (T e:enums)&#123;</span><br><span class="line">                <span class="comment">//从 code 反序列化回枚举</span></span><br><span class="line">                enumMap.put(e.getCode()+<span class="string">&quot;&quot;</span>,e);</span><br><span class="line">                <span class="comment">//从枚举字面量反序列回枚</span></span><br><span class="line">                <span class="comment">//是Spring默认的方案</span></span><br><span class="line">                <span class="comment">//此处添加可避免下面convert方法抛出IllegalArgumentException异常后被系统捕获再次调用默认方案</span></span><br><span class="line">                enumMap.put(((Enum)e).name()+<span class="string">&quot;&quot;</span>,e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> T <span class="title">convert</span><span class="params">(String source)</span> </span>&#123;</span><br><span class="line">            T result=enumMap.get(source);</span><br><span class="line">            <span class="keyword">if</span>(result==<span class="keyword">null</span>)&#123;</span><br><span class="line">                <span class="comment">//抛出该异常后，会调用 spring 的默认转换方案，即使用 枚举字面量进行映射</span></span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;No element matches &quot;</span>+source);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-Spring-MVC-配置类"><a href="#3-Spring-MVC-配置类" class="headerlink" title="3 Spring MVC 配置类"></a>3 Spring MVC 配置类</h3><h4 id="1-相关知识"><a href="#1-相关知识" class="headerlink" title="1 相关知识"></a>1 相关知识</h4><ol><li>Spring Boot 默认提供Spring MVC 自动配置，不需要使用@EnableWebMvc注解</li><li>如果需要配置MVC（拦截器、格式化、视图等） 请使用添加@Configuration并实现WebMvcConfigurer接口.不要添加@EnableWebMvc注解。</li><li>@EnableWebMvc 只能添加到一个@Configuration配置类上，用于导入Spring Web MVC configuration</li><li>如果Spring Boot在classpath里看到有 spring webmvc 也会自动添加@EnableWebMvc</li></ol><p>简单来说就是在SpringBoot中不要使用@EnableWebMvc，使用@Configuration标记自定义@WebMvcConfigurer类就行，而且该类允许多个同时存在。</p><p>相关资料：<br><a href="https://blog.csdn.net/zxc123e/article/details/84636521">Spring注解@EnableWebMvc使用坑点解析</a><br><a href="https://blog.csdn.net/pinebud55/article/details/53420481">解析@EnableWebMvc 、WebMvcConfigurationSupport和WebMvcConfigurationAdapter</a><br><a href="https://www.jianshu.com/p/d47a09532de7">WebMvcConfigurationSupport与WebMvcConfigurer的关系</a><br><a href="https://blog.csdn.net/lqadam/article/details/80637335">@EnableWebMvc如何禁止@EnableAutoConfiguration</a></p><h4 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2 代码实现"></a>2 代码实现</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> V1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: linshenkx</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span>: 2019/1/12</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 将转换器工厂添加到Spring</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CodedEnumWebAppConfigurer</span> <span class="keyword">implements</span> <span class="title">WebMvcConfigurer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addFormatters</span><span class="params">(FormatterRegistry registry)</span> </span>&#123;</span><br><span class="line">        registry.addConverterFactory(<span class="keyword">new</span> CodedEnumConverterFactory());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="三-测试及分析"><a href="#三-测试及分析" class="headerlink" title="三 测试及分析"></a>三 测试及分析</h2><h3 id="1-枚举类"><a href="#1-枚举类" class="headerlink" title="1 枚举类"></a>1 枚举类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> V1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: linshenkx</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span>: 2019/1/13</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 枚举类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">ReturnCodeEnum</span> <span class="keyword">implements</span> <span class="title">CodedEnum</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 正常</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    OK(<span class="number">200</span>),</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 出错</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    ERROR(<span class="number">500</span>)</span><br><span class="line">    ;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> code;</span><br><span class="line"></span><br><span class="line">    ReturnCodeEnum(<span class="keyword">int</span> code)&#123;</span><br><span class="line">        <span class="keyword">this</span>.code=code;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> code;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@JsonCreator</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ReturnCodeEnum <span class="title">create</span><span class="params">(String name)</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> ReturnCodeEnum.valueOf(name);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (IllegalArgumentException e)&#123;</span><br><span class="line">            <span class="keyword">int</span> code=Integer.parseInt(name);</span><br><span class="line">            <span class="keyword">for</span> (ReturnCodeEnum value : ReturnCodeEnum.values()) &#123;</span><br><span class="line">                <span class="keyword">if</span>(value.code==code)&#123;</span><br><span class="line">                    <span class="keyword">return</span> value;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;No element matches &quot;</span>+name);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-包含枚举类的POJO"><a href="#2-包含枚举类的POJO" class="headerlink" title="2 包含枚举类的POJO"></a>2 包含枚举类的POJO</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> V1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: linshenkx</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span>: 2019/1/12</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 枚举包装类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyResult</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> ReturnCodeEnum returnCode;</span><br><span class="line">    <span class="keyword">private</span> String message;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="3-测试类"><a href="#3-测试类" class="headerlink" title="3 测试类"></a>3 测试类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@version</span> V1.0</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: linshenkx</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span>: 2019/1/12</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 测试类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping(&quot;/test&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@PostMapping(value = &quot;/enumForm&quot;)</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> MyResult <span class="title">testEnumForm</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">         <span class="meta">@RequestBody</span> MyResult myResult)</span> </span>&#123;</span><br><span class="line">    ReturnCodeEnum status = myResult.getReturnCode();</span><br><span class="line">    System.out.println(<span class="string">&quot;name():&quot;</span>+status.name());</span><br><span class="line">    System.out.println(<span class="string">&quot;ordinal():&quot;</span>+status.ordinal());</span><br><span class="line">    System.out.println(<span class="string">&quot;getCode():&quot;</span>+status.getCode());</span><br><span class="line">    <span class="keyword">return</span> myResult;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@PostMapping(value = &quot;/enumJson&quot;)</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> MyResult <span class="title">testEnumJson</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">          <span class="meta">@RequestBody</span> MyResult myResult)</span> </span>&#123;</span><br><span class="line">    ReturnCodeEnum status = myResult.getReturnCode();</span><br><span class="line">    System.out.println(<span class="string">&quot;name():&quot;</span>+status.name());</span><br><span class="line">    System.out.println(<span class="string">&quot;ordinal():&quot;</span>+status.ordinal());</span><br><span class="line">    System.out.println(<span class="string">&quot;getCode():&quot;</span>+status.getCode());</span><br><span class="line">    <span class="keyword">return</span> myResult;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@PostMapping(value = &quot;/enumPath/&#123;status&#125;&quot;)</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> ReturnCodeEnum <span class="title">testEnumPath</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">          <span class="meta">@PathVariable</span> ReturnCodeEnum status)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> status;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@PostMapping(value = &quot;/enumParam&quot;)</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> ReturnCodeEnum <span class="title">testEnumParam</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">          <span class="meta">@RequestParam</span> ReturnCodeEnum status)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> status;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外还需注入上面的转换器工厂，这里不再重复贴出。</p><h3 id="4-测试结果"><a href="#4-测试结果" class="headerlink" title="4 测试结果"></a>4 测试结果</h3><h4 id="预测分析"><a href="#预测分析" class="headerlink" title="预测分析"></a>预测分析</h4><p>如上，因为ReturnCodeEnum 实现了 CodedEnum 接口，并注入对应转换器工厂，所以可以在 表单提交的时候适用code和字面量接收枚举参数。<br>ReturnCodeEnum还写了@JsonValue注解的方法，所以使用Json传参/返回值时使用@JsonValue对应的返回值。<br>因为我们还想实现Json传参的时候支持字面量，所以我们在@JsonCreator注解的方法里写了支持 code 和字面量，该方法会使@JsonValue 对反序列化的支持失效，所以写的时候不仅要支持字面量还要支持原本的目的—–code。<br>由于我们已经覆盖了原来的序列化/反序列化方式，所以 ordinal 的支持已经失效。<br>另外，由于我们可以将参数中的String转化为枚举，所以我们也可以直接使用 @PathVariable 和 @RequestParam（Content-Type: multipart/form-data）来传递枚举参数(相关资料：<a href="https://www.baeldung.com/spring-type-conversions">Baeldung：Guide to Spring Type Conversions</a>)，但是注意这个时候不能使用 包含枚举类型的POJO类，除非你再定义一个从简单类型到复合类型的转换器。</p><h4 id="1-bean接收（application-x-www-form-urlencoded）"><a href="#1-bean接收（application-x-www-form-urlencoded）" class="headerlink" title="1 bean接收（application/x-www-form-urlencoded）"></a>1 bean接收（application/x-www-form-urlencoded）</h4><ol><li><p>字面量<br><img src="https://img-blog.csdnimg.cn/2019011409281053.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="OK"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name():<span class="function">OK</span></span><br><span class="line"><span class="function"><span class="title">ordinal</span><span class="params">()</span>:0</span></span><br><span class="line"><span class="function"><span class="title">getCode</span><span class="params">()</span>:200</span></span><br></pre></td></tr></table></figure></li><li><p>code<br><img src="https://img-blog.csdnimg.cn/20190114092919865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="code"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name():<span class="function">OK</span></span><br><span class="line"><span class="function"><span class="title">ordinal</span><span class="params">()</span>:0</span></span><br><span class="line"><span class="function"><span class="title">getCode</span><span class="params">()</span>:200</span></span><br></pre></td></tr></table></figure></li><li><p>ordinal<br><img src="https://img-blog.csdnimg.cn/20190114093114217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="ordinal"><br>抛出异常</p><blockquote><p>Field error in object ‘myResult’ on field ‘returnCode’: rejected value [0]; codes [typeMismatch.myResult.returnCode,typeMismatch.returnCode,typeMismatch.com.dx.hbdt.system.manager.hongbao.controller.ReturnCodeEnum,typeMismatch]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [myResult.returnCode,returnCode]; arguments []; default message [returnCode]]; default message [Failed to convert property value of type ‘java.lang.String’ to required type ‘com.dx.hbdt.system.manager.hongbao.controller.ReturnCodeEnum’ for property ‘returnCode’; nested exception is org.springframework.core.convert.ConversionFailedException: Failed to convert from type [java.lang.String] to type [com.dx.hbdt.system.manager.hongbao.controller.ReturnCodeEnum] for value ‘0’; nested exception is java.lang.IllegalArgumentException: No element matches 0]]</p></blockquote></li></ol><h4 id="2-RequestBody-接收（application-json）"><a href="#2-RequestBody-接收（application-json）" class="headerlink" title="2 @RequestBody 接收（application/json）"></a>2 @RequestBody 接收（application/json）</h4><ol><li><p>字面量<br><img src="https://img-blog.csdnimg.cn/20190114093238352.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="OK"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name():<span class="function">OK</span></span><br><span class="line"><span class="function"><span class="title">ordinal</span><span class="params">()</span>:0</span></span><br><span class="line"><span class="function"><span class="title">getCode</span><span class="params">()</span>:200</span></span><br></pre></td></tr></table></figure></li><li><p>code<br><img src="https://img-blog.csdnimg.cn/20190114093333135.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="200"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name():<span class="function">OK</span></span><br><span class="line"><span class="function"><span class="title">ordinal</span><span class="params">()</span>:0</span></span><br><span class="line"><span class="function"><span class="title">getCode</span><span class="params">()</span>:200</span></span><br></pre></td></tr></table></figure></li><li><p>ordinal<br><img src="https://img-blog.csdnimg.cn/2019011409350597.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="异常"><br>调用@JsonCreator的create方法的时候抛出 IllegalArgumentException 异常</p></li><li><p>其他<br>字符串是要携带引号的，如果不携带引号会解析错误，如{    “returnCode”: OK }<br>数字可不携带引号，在反序列化成枚举时仍会看成 String 对待，并获得与上面相同的效果。</p></li></ol><h4 id="3-PathVariable"><a href="#3-PathVariable" class="headerlink" title="3 @PathVariable"></a>3 @PathVariable</h4><p><img src="https://img-blog.csdnimg.cn/20190114101616935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="路径"></p><h4 id="4-RequestParam（multipart-form-data）"><a href="#4-RequestParam（multipart-form-data）" class="headerlink" title="4 @RequestParam（multipart/form-data）"></a>4 @RequestParam（multipart/form-data）</h4><p><img src="https://img-blog.csdnimg.cn/20190114101654469.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaW55dWE=,size_16,color_FFFFFF,t_70" alt="参数"></p>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringSecurity5.0的DelegatingPasswordEncoder详解</title>
      <link href="//DelegatingPasswordEncoder/"/>
      <url>//DelegatingPasswordEncoder/</url>
      
        <content type="html"><![CDATA[<p>本文参考自Spring Security 5.0.4.RELEASE  的官方文档,结合源码介绍了DelegatingPasswordEncoder,对其工作过程进行分析并解决其中遇到的问题.包括<code>There is no PasswordEncoder mapped for the id &quot;null&quot;</code>的非法参数异常的正确处理方法</p><span id="more"></span><p>##PasswordEncoder<br>首先要理解<strong>DelegatingPasswordEncoder</strong>的作用和存在意义,明白官方为什么要使用它来取代原先的<strong>NoOpPasswordEncoder</strong></p><p><strong>DelegatingPasswordEncoder</strong>和<strong>NoOpPasswordEncoder</strong>都是<code>PasswordEncoder</code>接口的实现类,根据官方的定义,Spring Security的<strong>PasswordEncoder</strong>接口用于执行密码的单向转换，以便安全地存储密码。</p><p>关于密码存储的演变历史这里我不多做介绍,简单来说就是现在数据库存储的密码基本都是经过编码的,而<strong>决定如何编码</strong>以及<strong>判断未编码的字符序列和编码后的字符串是否匹配</strong>就是PassswordEncoder的责任.</p><p>这里我们可以看一下PasswordEncoder接口的源码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">PasswordEncoder</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Encode the raw password. Generally, a good encoding algorithm applies a SHA-1 or</span></span><br><span class="line"><span class="comment"> * greater hash combined with an 8-byte or greater randomly generated salt.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">String <span class="title">encode</span><span class="params">(CharSequence rawPassword)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Verify the encoded password obtained from storage matches the submitted raw</span></span><br><span class="line"><span class="comment"> * password after it too is encoded. Returns true if the passwords match, false if</span></span><br><span class="line"><span class="comment"> * they do not. The stored password itself is never decoded.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> rawPassword the raw password to encode and match</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> encodedPassword the encoded password from storage to compare with</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true if the raw password, after encoding, matches the encoded password from</span></span><br><span class="line"><span class="comment"> * storage</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">matches</span><span class="params">(CharSequence rawPassword, String encodedPassword)</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>根据源码,我们可以直观地看到,PassswordEncoder接口只有两个方法,一个是<code>String encode(CharSequence rawPassword);</code>,用于将字符序列(即原密码)进行编码,另一个方法是<code>boolean matches(CharSequence rawPassword, String encodedPassword);</code>,用于比较字符序列和编码后的密码是否匹配.</p><p>理解了PasswordEncoder的作用后我们来Spring Security 5.0之前默认的PasswordEncoder实现类<code>NoOpPasswordEncoder</code>,这个类已经被标记为过时了,因为不安全,下面就让我们来看看它是如何地不安全的</p><h3 id="1-NoOpPasswordEncoder"><a href="#1-NoOpPasswordEncoder" class="headerlink" title="1 NoOpPasswordEncoder"></a>1 NoOpPasswordEncoder</h3><p>事实上,NoOpPasswordEncoder就是没有编码的编码器,源码如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Deprecated</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">NoOpPasswordEncoder</span> <span class="keyword">implements</span> <span class="title">PasswordEncoder</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">encode</span><span class="params">(CharSequence rawPassword)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> rawPassword.toString();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">matches</span><span class="params">(CharSequence rawPassword, String encodedPassword)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> rawPassword.toString().equals(encodedPassword);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Get the singleton &#123;<span class="doctag">@link</span> NoOpPasswordEncoder&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> PasswordEncoder <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> INSTANCE;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> PasswordEncoder INSTANCE = <span class="keyword">new</span> NoOpPasswordEncoder();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">NoOpPasswordEncoder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到,<strong>NoOpPasswordEncoder</strong>的<strong>encode</strong>方法就只是简单地把字符序列转成字符串,也就是说,你输入的密码”123456”存储在数据库里仍然是”123456”,这样如果数据库被攻破的话,密码就直接泄露了,十分不安全.而且,<strong>NoOpPasswordEncoder</strong>也就失去了所谓密码编码器的意义了.</p><p>不过,正因其十分简单,所以在Spring Security 5.0 之前<strong>NoOpPasswordEncoder</strong>是作为默认的密码编码器而存在到,它可以是你没有主动加密时的一个默认选择.</p><p>另外,<strong>NoOpPasswordEncoder</strong>的实现是一个标准的饿汉单例模式,关于单例模式可以看这一篇文章,<a href="https://blog.csdn.net/alinyua/article/details/79776613">单例模式及其4种推荐写法和3类保护手段</a></p><h3 id="2-DelegatingPasswordEncoder"><a href="#2-DelegatingPasswordEncoder" class="headerlink" title="2 DelegatingPasswordEncoder"></a>2 DelegatingPasswordEncoder</h3><p>通过上面的学习,我们可以知道随着安全要求的提高,之前的默认密码编码器<strong>NoOpPasswordEncoder</strong>已经被”不推荐”了,那我们有理由推测现在的默认密码编码器换成了使用某一特定算法的编码器.可是这样便会带来三个问题:</p><ul><li>有许多使用旧密码编码的应用程序无法轻松迁移</li><li>密码存储的最佳做法(算法)可能会再次发生变化</li><li>作为一个框架，Spring Security不能经常发生突变</li></ul><p>简单来说,就是新的密码编码器和旧密码的兼容性,以及自身的稳健性以及需要一定的可变性(切换到更好的算法).听起来是不是十分矛盾?那我们就来看看<strong>DelegatingPasswordEncoder</strong>是怎么解决这个问题的,在看解决方法之前先看使用<strong>DelegatingPasswordEncoder</strong>所能达到的效果:</p><ul><li>确保使用当前密码存储建议对密码进行编码</li><li>允许验证现代和传统格式的密码</li><li>允许将来升级编码算法</li></ul><p>是不是很神奇?事实上<strong>DelegatingPasswordEncoder</strong>并不是传统意义上的编码器,它并不使用某一特定算法进行编码,顾名思义,它是一个<strong>委派密码编码器</strong>,它将具体编码的实现根据要求委派给不同的算法,以此来实现不同编码算法之间的兼容和变化协调.</p><h4 id="1-构造方法"><a href="#1-构造方法" class="headerlink" title="1 构造方法"></a>1 构造方法</h4><p>下面我们来看看<strong>DelegatingPasswordEncoder</strong>的构造方法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public DelegatingPasswordEncoder(String idForEncode,</span><br><span class="line">Map&lt;String, PasswordEncoder&gt; idToPasswordEncoder) &#123;</span><br><span class="line">if(idForEncode == null) &#123;</span><br><span class="line">throw new IllegalArgumentException(&quot;idForEncode cannot be null&quot;);</span><br><span class="line">&#125;</span><br><span class="line">if(!idToPasswordEncoder.containsKey(idForEncode)) &#123;</span><br><span class="line">throw new IllegalArgumentException(&quot;idForEncode &quot; + idForEncode + &quot;is not found in idToPasswordEncoder &quot; + idToPasswordEncoder);</span><br><span class="line">&#125;</span><br><span class="line">for(String id : idToPasswordEncoder.keySet()) &#123;</span><br><span class="line">if(id == null) &#123;</span><br><span class="line">continue;</span><br><span class="line">&#125;</span><br><span class="line">if(id.contains(PREFIX)) &#123;</span><br><span class="line">throw new IllegalArgumentException(&quot;id &quot; + id + &quot; cannot contain &quot; + PREFIX);</span><br><span class="line">&#125;</span><br><span class="line">if(id.contains(SUFFIX)) &#123;</span><br><span class="line">throw new IllegalArgumentException(&quot;id &quot; + id + &quot; cannot contain &quot; + SUFFIX);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">this.idForEncode = idForEncode;</span><br><span class="line">this.passwordEncoderForEncode = idToPasswordEncoder.get(idForEncode);</span><br><span class="line">this.idToPasswordEncoder = new HashMap&lt;&gt;(idToPasswordEncoder);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>idForEncode</strong>决定密码编码器的类型,<strong>idToPasswordEncoder</strong>决定判断匹配时兼容的类型<br>而且<strong>idToPasswordEncoder</strong>必须包含<strong>idForEncode</strong>(不然加密后就无法匹配了)</p><p>围绕这个构造方法通常有两种创建思路,如下:</p><h5 id="工厂构造"><a href="#工厂构造" class="headerlink" title="工厂构造"></a>工厂构造</h5><p>首先是工厂构造</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PasswordEncoder passwordEncoder =</span><br><span class="line">    PasswordEncoderFactories.createDelegatingPasswordEncoder();</span><br></pre></td></tr></table></figure><p>其具体实现如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> PasswordEncoder <span class="title">createDelegatingPasswordEncoder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String encodingId = <span class="string">&quot;bcrypt&quot;</span>;</span><br><span class="line">Map&lt;String, PasswordEncoder&gt; encoders = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">encoders.put(encodingId, <span class="keyword">new</span> BCryptPasswordEncoder());</span><br><span class="line">encoders.put(<span class="string">&quot;ldap&quot;</span>, <span class="keyword">new</span> LdapShaPasswordEncoder());</span><br><span class="line">encoders.put(<span class="string">&quot;MD4&quot;</span>, <span class="keyword">new</span> Md4PasswordEncoder());</span><br><span class="line">encoders.put(<span class="string">&quot;MD5&quot;</span>, <span class="keyword">new</span> MessageDigestPasswordEncoder(<span class="string">&quot;MD5&quot;</span>));</span><br><span class="line">encoders.put(<span class="string">&quot;noop&quot;</span>, NoOpPasswordEncoder.getInstance());</span><br><span class="line">encoders.put(<span class="string">&quot;pbkdf2&quot;</span>, <span class="keyword">new</span> Pbkdf2PasswordEncoder());</span><br><span class="line">encoders.put(<span class="string">&quot;scrypt&quot;</span>, <span class="keyword">new</span> SCryptPasswordEncoder());</span><br><span class="line">encoders.put(<span class="string">&quot;SHA-1&quot;</span>, <span class="keyword">new</span> MessageDigestPasswordEncoder(<span class="string">&quot;SHA-1&quot;</span>));</span><br><span class="line">encoders.put(<span class="string">&quot;SHA-256&quot;</span>, <span class="keyword">new</span> MessageDigestPasswordEncoder(<span class="string">&quot;SHA-256&quot;</span>));</span><br><span class="line">encoders.put(<span class="string">&quot;sha256&quot;</span>, <span class="keyword">new</span> StandardPasswordEncoder());</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> DelegatingPasswordEncoder(encodingId, encoders);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个可以简单地理解为,遇到新密码,<strong>DelegatingPasswordEncoder</strong>会委托给<strong>BCryptPasswordEncoder</strong>(encodingId为<em>bcryp</em><em>)进行加密,同时,对历史上使用*<em>ldap</em></em>,<strong>MD4</strong>,<strong>MD5</strong>等等加密算法的密码认证保持兼容(如果数据库里的密码使用的是<strong>MD5</strong>算法,那使用matches方法认证仍可以通过,但新密码会使<strong>bcrypt</strong>进行储存),十分神奇,原理后面会讲</p><h5 id="定制构造"><a href="#定制构造" class="headerlink" title="定制构造"></a>定制构造</h5><p>接下来是定制构造,其实和工厂方法是一样的,一般情况下推荐直接使用工厂方法,这里给一个小例子</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">String idForEncode = <span class="string">&quot;bcrypt&quot;</span>;</span><br><span class="line">Map encoders = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">encoders.put(idForEncode, <span class="keyword">new</span> BCryptPasswordEncoder());</span><br><span class="line">encoders.put(<span class="string">&quot;noop&quot;</span>, NoOpPasswordEncoder.getInstance());</span><br><span class="line">encoders.put(<span class="string">&quot;pbkdf2&quot;</span>, <span class="keyword">new</span> Pbkdf2PasswordEncoder());</span><br><span class="line">encoders.put(<span class="string">&quot;scrypt&quot;</span>, <span class="keyword">new</span> SCryptPasswordEncoder());</span><br><span class="line">encoders.put(<span class="string">&quot;sha256&quot;</span>, <span class="keyword">new</span> StandardPasswordEncoder());</span><br><span class="line"></span><br><span class="line">PasswordEncoder passwordEncoder =</span><br><span class="line">    <span class="keyword">new</span> DelegatingPasswordEncoder(idForEncode, encoders);</span><br></pre></td></tr></table></figure><h4 id="2-密码存储格式"><a href="#2-密码存储格式" class="headerlink" title="2 密码存储格式"></a>2 密码存储格式</h4><p>密码的标准存储格式是:</p><blockquote><p>{id}encodedPassword</p></blockquote><p>其中,<strong>id</strong>标识使用<strong>PaswordEncoder</strong>的种类<br><strong>encodedPassword</strong>是原密码被编码后的密码</p><blockquote><p>注意:<br><strong>rawPassword</strong>,<strong>encodedPassword</strong>,<strong>密码存储格式</strong>(<strong>prefixEncodedPassword</strong>),这三者是不同的概念!<br><code>rawPassword</code>相当于字符序列”<code>123456</code>“<br><code>encodedPassword</code>是使用id为”mycrypt”对应的密码编码器”123456”编码后的字符串,假设为”<code>qwertyuiop</code>“<br><code>存储的密码 prefixEncodedPassword</code>是在数据库中,我们所能见到的形式,如”<code>&#123;mycrypt&#125;qwertyuiop</code>“<br>这个概念在后面讲matches方法的源码时会用到,请留意</p></blockquote><p>例如<strong>rawPassword</strong>为<strong>password</strong>在使用不同编码算法的情况下在数据库的存储如下:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;bcrypt&#125;$2a$10$dXJ3SW6G7P50lGmMkkmwe.20cQQubK3.HZWzG3YB1tlRy.fqvM/BG </span><br><span class="line">&#123;noop&#125;password </span><br><span class="line">&#123;pbkdf2&#125;5d923b44a6d129f3ddf3e3c8d29412723dcbde72445e8ef6bf3b508fbf17fa4ed4d6b99ca763d8dc </span><br><span class="line">&#123;scrypt&#125;$e0801$8bWJaSu2IKSn9Z9kM+TPXfOc/9bdYSrN1oD9qfVThWEwdRTnO7re7Ei+fUZRJ68k9lTyuTeUp4of4g24hHnazw==$OAOec05+bXxvuu/1qZ6NUR+xQYvYv7BeL1QxwRpY5Pc=  </span><br><span class="line">&#123;sha256&#125;97cde38028ad898ebc02e690819fa220e88c62e0699403e94fff291cfffaf8410849f27605abcbc0 </span><br></pre></td></tr></table></figure><blockquote><p>这里需要指明,密码的可靠性并不依赖于加密算法的保密,即密码的可靠在于就算你知道我使用的是什么算法你也无法还原出原密码(当然,对于本身就可逆的编码算法来说就不是这样了,但这样的算法我们通常不会认为是可靠的),而且,即使没有标明使用的是什么算法,攻击者也很容易根据一些规律从编码后的密码字符串中推测出编码算法,如bcrypt算法通常是以<code>$2a$</code>开头的</p></blockquote><h4 id="3-密码编码与匹配"><a href="#3-密码编码与匹配" class="headerlink" title="3 密码编码与匹配"></a>3 密码编码与匹配</h4><p>从上文可知,<strong>idForEncode</strong>这个构造参数决定使用哪个PasswordEncoder进行密码的编码,编码的方法如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String PREFIX = <span class="string">&quot;&#123;&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String SUFFIX = <span class="string">&quot;&#125;&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">encode</span><span class="params">(CharSequence rawPassword)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> PREFIX + <span class="keyword">this</span>.idForEncode + SUFFIX + <span class="keyword">this</span>.passwordEncoderForEncode.encode(rawPassword);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以用上文构造的<strong>DelegatingPasswordEncoder</strong>默认使用<strong>BCryptPasswordEncoder</strong>,结果格式如</p><blockquote><p>{bcrypt}$2a$10$dXJ3SW6G7P50lGmMkkmwe.20cQQubK3.HZWzG3YB1tlRy.fqvM/BG</p></blockquote><p>密码编码方法比较简单,重点在于匹配.匹配方法源码如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">matches</span><span class="params">(CharSequence rawPassword, String prefixEncodedPassword)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span>(rawPassword == <span class="keyword">null</span> &amp;&amp; prefixEncodedPassword == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//取出编码算法的id</span></span><br><span class="line">String id = extractId(prefixEncodedPassword);</span><br><span class="line"><span class="comment">//根据编码算法的id从支持的密码编码器Map(构造时传入)中取出对应编码器</span></span><br><span class="line">PasswordEncoder delegate = <span class="keyword">this</span>.idToPasswordEncoder.get(id);</span><br><span class="line"><span class="keyword">if</span>(delegate == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">//如果找不到对应的密码编码器则使用默认密码编码器进行匹配判断,此时比较的密码字符串是 prefixEncodedPassword</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>.defaultPasswordEncoderForMatches</span><br><span class="line">.matches(rawPassword, prefixEncodedPassword);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//从 prefixEncodedPassword 中提取获得 encodedPassword </span></span><br><span class="line">String encodedPassword = extractEncodedPassword(prefixEncodedPassword);</span><br><span class="line"><span class="comment">//使用对应编码器进行匹配判断,此时比较的密码字符串是 encodedPassword ,不携带编码算法id头</span></span><br><span class="line"><span class="keyword">return</span> delegate.matches(rawPassword, encodedPassword);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个匹配方法其实也挺好理解的,唯一需要特别注意的就是找不到对应密码编码器时使用的默认密码编码器,</p><p>我们来看看<strong>defaultPasswordEncoderForMatches</strong>是一个什么东西</p><h4 id="4-defaultPasswordEncoderForMatches及-id为null异常"><a href="#4-defaultPasswordEncoderForMatches及-id为null异常" class="headerlink" title="4 defaultPasswordEncoderForMatches及 id为null异常"></a>4 defaultPasswordEncoderForMatches及 id为null异常</h4><p>在<strong>DelegatingPasswordEncoder</strong>的源码里对应内容如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String PREFIX = <span class="string">&quot;&#123;&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String SUFFIX = <span class="string">&quot;&#125;&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> String idForEncode;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> PasswordEncoder passwordEncoderForEncode;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;String, PasswordEncoder&gt; idToPasswordEncoder;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> PasswordEncoder defaultPasswordEncoderForMatches = <span class="keyword">new</span> UnmappedIdPasswordEncoder();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDefaultPasswordEncoderForMatches</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">PasswordEncoder defaultPasswordEncoderForMatches)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span>(defaultPasswordEncoderForMatches == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;defaultPasswordEncoderForMatches cannot be null&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">this</span>.defaultPasswordEncoderForMatches = defaultPasswordEncoderForMatches;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">UnmappedIdPasswordEncoder</span> <span class="keyword">implements</span> <span class="title">PasswordEncoder</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">encode</span><span class="params">(CharSequence rawPassword)</span> </span>&#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;encode is not supported&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">matches</span><span class="params">(CharSequence rawPassword,</span></span></span><br><span class="line"><span class="params"><span class="function">String prefixEncodedPassword)</span> </span>&#123;</span><br><span class="line">String id = extractId(prefixEncodedPassword);</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;There is no PasswordEncoder mapped for the id \&quot;&quot;</span> + id + <span class="string">&quot;\&quot;&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到,<strong>DelegatingPasswordEncoder</strong>里面,<br><strong>PREFIX <strong>和</strong>SUFFIX <strong>是常量,<br><strong>idForEncode</strong>,<strong>passwordEncoderForEncode</strong>和</strong>idToPasswordEncoder</strong>是在构造方法中传入决定并不可修改的,<br>只有<strong>defaultPasswordEncoderForMatches <strong>是有一个</strong>setDefaultPasswordEncoderForMatches</strong>方法进行设置的可变对象.</p><p>而且,它有一个私有的默认实现<code>UnmappedIdPasswordEncoder </code>,这个所谓的默认实现的唯一作用就是<strong>抛出异常提醒你要自己选择一个默认密码编码器来取代它</strong>,通常我们只会可能用到它的matches方法,这个时候就会报抛出如下异常</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java.lang.IllegalArgumentException: There is no PasswordEncoder mapped <span class="keyword">for</span> the id <span class="string">&quot;null&quot;</span></span><br><span class="line">at org.springframework.security.crypto.password.DelegatingPasswordEncoder$UnmappedIdPasswordEncoder.matches(DelegatingPasswordEncoder.java:<span class="number">233</span>)</span><br><span class="line">at org.springframework.security.crypto.password.DelegatingPasswordEncoder.matches(DelegatingPasswordEncoder.java:<span class="number">196</span>)</span><br></pre></td></tr></table></figure><h4 id="5-解决方法"><a href="#5-解决方法" class="headerlink" title="5 解决方法"></a>5 解决方法</h4><p>遇到这个异常,最简单的做法就是明确提供一个<strong>PasswordEncoder</strong>对密码进行编码,如果是从Spring Security 5.0 之前迁移而来的,由于之前默认使用的是<strong>NoOpPasswordEncoder</strong>并且数据库的密码保存格式不带有加密算法id头,会报id为null异常,所以应该明确提供一个<strong>NoOpPasswordEncoder</strong>密码编码器.</p><p>这里有两种思路,其一就是使用<strong>NoOpPasswordEncoder</strong>取代<strong>DelegatingPasswordEncoder</strong>,以恢复到之前版本的状态,这也是笔者在其他博客上看得比较多的一种解决方法.另外就是使用<strong>DelegatingPasswordEncoder</strong>的<code>setDefaultPasswordEncoderForMatches</code>方法指定默认的密码编码器为<strong>NoOpPasswordEncoder</strong>,这两种方法孰优孰劣自然不言而喻,官方文档是这么说的</p><blockquote><p>Reverting to NoOpPasswordEncoder is not considered to be secure. You should instead migrate to using DelegatingPasswordEncoder to support secure password encoding.<br>恢复到NoOpPasswordEncoder不被认为是安全的。您应该转而使用DelegatingPasswordEncoder支持安全密码编码</p></blockquote><p>当然,你也可以将数据库保存的密码都加上一个<code>&#123;noop&#125;</code>前缀,这样<strong>DelegatingPasswordEncoder</strong>就知道要使用<strong>NoOpPasswordEncoder</strong>了,这确实是一种方法,但没必要,这里我们来看一下前面的两种解决方法的实现</p><h5 id="1-使用NoOpPasswordEncoder取代DelegatingPasswordEncoder"><a href="#1-使用NoOpPasswordEncoder取代DelegatingPasswordEncoder" class="headerlink" title="1 使用NoOpPasswordEncoder取代DelegatingPasswordEncoder"></a>1 使用NoOpPasswordEncoder取代DelegatingPasswordEncoder</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Bean</span></span><br><span class="line"> <span class="keyword">public</span>  <span class="keyword">static</span> NoOpPasswordEncoder passwordEncoder（）&#123;</span><br><span class="line">     <span class="keyword">return</span> NoOpPasswordEncoder.getInstance（）;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="2-使用DelegatingPasswordEncoder指定defaultPasswordEncoderForMatches"><a href="#2-使用DelegatingPasswordEncoder指定defaultPasswordEncoderForMatches" class="headerlink" title="2 使用DelegatingPasswordEncoder指定defaultPasswordEncoderForMatches"></a>2 使用DelegatingPasswordEncoder指定defaultPasswordEncoderForMatches</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    @Bean</span><br><span class="line">    public  static PasswordEncoder passwordEncoder( )&#123;</span><br><span class="line">        DelegatingPasswordEncoder delegatingPasswordEncoder =</span><br><span class="line">                (DelegatingPasswordEncoder) PasswordEncoderFactories.createDelegatingPasswordEncoder();</span><br><span class="line">//设置defaultPasswordEncoderForMatches为NoOpPasswordEncoder</span><br><span class="line">        delegatingPasswordEncoder.setDefaultPasswordEncoderForMatches(NoOpPasswordEncoder.getInstance());</span><br><span class="line">        return  delegatingPasswordEncoder;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 后端开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
